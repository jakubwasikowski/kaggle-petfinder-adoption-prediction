{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "0a7e025d428361b9e1c7b0fda4cd762df32a5009"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS_LOCAL = True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from abc import ABC, abstractmethod\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import colorsys\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from math import sqrt\n",
    "from itertools import chain\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "\n",
    "from IPython.display import display, Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import PIL as pil\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "from sklearn.pipeline import make_pipeline, make_union, Pipeline, FeatureUnion\n",
    "from sklearn.metrics import confusion_matrix as sk_cmatrix\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder\n",
    "from skimage.restoration import estimate_sigma\n",
    "from ml_metrics import quadratic_weighted_kappa, rmse\n",
    "\n",
    "DATA_HOME = Path('../input')\n",
    "PF_HOME = DATA_HOME / 'petfinder-adoption-prediction'\n",
    "\n",
    "IS_LOCAL = Path('IS_LOCAL').exists()\n",
    "LABEL_NUM = 5\n",
    "FOLDS_NUM = 5\n",
    "\n",
    "DEFAULT = 999999\n",
    "pd.set_option('display.max_rows', 30)\n",
    "\n",
    "print(f'IS_LOCAL = {IS_LOCAL}')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "SEED = 7\n",
    "np.random.seed(SEED)\n",
    "CATEGORY_SUFFIX = '@c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "76a0fcf653021b0b5440c41d47590a787861d928"
   },
   "outputs": [],
   "source": [
    "def greedy_group_k_fold_stratified(X, y, groups, k):\n",
    "    y_counts_per_group = defaultdict(lambda: np.zeros(LABEL_NUM))\n",
    "    y_distr = Counter()\n",
    "    for label, g in zip(y, groups):\n",
    "        y_counts_per_group[g][label] += 1\n",
    "        y_distr[label] += 1\n",
    "\n",
    "    y_counts_per_fold = defaultdict(lambda: np.zeros(LABEL_NUM))\n",
    "    groups_per_fold = defaultdict(set)\n",
    "\n",
    "    def eval_y_counts_per_fold(y_counts, fold):\n",
    "        y_counts_per_fold[fold] += y_counts\n",
    "        std_per_label = []\n",
    "        for label in range(LABEL_NUM):\n",
    "            label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(k)])\n",
    "            std_per_label.append(label_std)\n",
    "        y_counts_per_fold[fold] -= y_counts\n",
    "        return np.mean(std_per_label)\n",
    "    \n",
    "    groups_and_y_counts = list(y_counts_per_group.items())\n",
    "    random.Random(SEED).shuffle(groups_and_y_counts)\n",
    "\n",
    "    for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\n",
    "        best_fold = None\n",
    "        min_eval = None\n",
    "        for i in range(k):\n",
    "            fold_eval = eval_y_counts_per_fold(y_counts, i)\n",
    "            if min_eval is None or fold_eval < min_eval:\n",
    "                min_eval = fold_eval\n",
    "                best_fold = i\n",
    "        y_counts_per_fold[best_fold] += y_counts\n",
    "        groups_per_fold[best_fold].add(g)\n",
    "\n",
    "    all_groups = set(groups)\n",
    "    for i in range(k):\n",
    "        train_groups = all_groups - groups_per_fold[i]\n",
    "        test_groups = groups_per_fold[i]\n",
    "\n",
    "        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n",
    "        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n",
    "\n",
    "        yield train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "29bc8ed7b95e70e591a85150d8a147eb07cf15bd"
   },
   "outputs": [],
   "source": [
    "def by_indices(data, indices):\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            return data.iloc[indices]\n",
    "        if isinstance(data, list):\n",
    "            return [data[i] for i in indices]\n",
    "        return data[indices]\n",
    "\n",
    "def explode(df, col, fillna=None):\n",
    "    ret = None\n",
    "    \n",
    "    col_name = col if isinstance(col, str) else '_'.join(col)\n",
    "    \n",
    "    new_cols = pd.DataFrame(({f'{col_name}_{k}': v for k, v in d.items()} \n",
    "                             for idx, d in df[col].iteritems()))\n",
    "    if fillna is not None:\n",
    "        new_cols = new_cols.fillna(fillna)\n",
    "    new_cols.index = df.index\n",
    "    \n",
    "    ret = pd.concat([df, new_cols], axis=1)\n",
    "    del ret[col]\n",
    "    return ret\n",
    "\n",
    "def run_cv_model(\n",
    "    name, \n",
    "    train_y,\n",
    "    train_x, train_ids,\n",
    "    test_x, test_ids, \n",
    "    reg_model_factory, qwk_model_factory\n",
    "):\n",
    "    def format_stats(mean: float, std: float) -> str:\n",
    "        return f'{mean:.3f} Â± {std:.3f}'\n",
    "\n",
    "    def save_predictions(pred_y):\n",
    "        if IS_LOCAL:\n",
    "            preds_dir = Path('predictions')\n",
    "            preds_path = preds_dir / f'{name}.csv'\n",
    "            checks_num = 0\n",
    "            while preds_path.exists():\n",
    "                checks_num += 1\n",
    "                new_file_name = f'{name}-{checks_num}.csv'\n",
    "                preds_path = preds_dir / new_file_name\n",
    "        else:\n",
    "            preds_path = 'submission.csv'\n",
    "        \n",
    "        print(f'Saving to path `{preds_path}`')\n",
    "\n",
    "        submission = pd.DataFrame({'PetID': test_ids, 'AdoptionSpeed': pred_y.astype(np.int32)})\n",
    "        submission.to_csv(preds_path, index=False)\n",
    "        print(f'Saving successfull')\n",
    "    \n",
    "    def get_formatted_cv_evals(dev_rmse_scores, val_rmse_scores, dev_qwk_scores, val_qwk_scores):\n",
    "        data = []\n",
    "        indices = []\n",
    "        for fold_ind, scores in enumerate(zip(\n",
    "            dev_rmse_scores, val_rmse_scores, dev_qwk_scores, val_qwk_scores\n",
    "        )):\n",
    "            data.append([f'{s:.3f}' for s in scores])\n",
    "            indices.append(f'{name} - fold {fold_ind}')\n",
    "        return pd.DataFrame(data, index=indices, columns=[\n",
    "            'dev cv rmse', 'val cv rmse', 'dev cv qwk', 'val cv qwk'])\n",
    "    \n",
    "    def get_distr(y_vals):\n",
    "        y_distr = Counter(y_vals)\n",
    "        y_vals_sum = sum(y_distr.values())\n",
    "        return [f'{y_distr[i] / y_vals_sum:.2%}' for i in range(LABEL_NUM)]\n",
    "\n",
    "    dev_rmse_scores = []\n",
    "    dev_qwk_scores = []\n",
    "    \n",
    "    val_rmse_scores = []\n",
    "    val_qwk_scores = []\n",
    "\n",
    "    pred_train_y_reg = np.zeros(len(train_y))\n",
    "    pred_test_y_reg = 0\n",
    "    \n",
    "    models = []\n",
    "    qwk_models = []\n",
    "    \n",
    "#     splits = StratifiedKFold(n_splits=k, random_state=7, shuffle=True).split(train_x, train_y)\n",
    "#     splits = GroupKFold(n_splits=k).split(train_x, train_y, groups=train_rescue_ids)\n",
    "#     splits = group_k_fold_stratified(train_x, train_y, groups=train_rescue_ids, FOLDS_NUM)\n",
    "#     splits = greedy_group_k_fold_stratified(train_x, train_y, groups=train_rescue_ids, FOLDS_NUM)\n",
    "\n",
    "    for fold, (dev_indices, val_indices) in enumerate(splits):\n",
    "        g1 = set(by_indices(train_rescue_ids, dev_indices))\n",
    "        g2 = set(by_indices(train_rescue_ids, val_indices))\n",
    "        \n",
    "        assert len(g1 & g2) == 0\n",
    "        \n",
    "        dev_x, val_x = by_indices(train_x, dev_indices), by_indices(train_x, val_indices)\n",
    "        dev_y, val_y = by_indices(train_y, dev_indices), by_indices(train_y, val_indices)\n",
    "        dev_ids, val_ids = by_indices(train_ids, dev_indices), by_indices(train_ids, val_indices)\n",
    "        \n",
    "        display(pd.DataFrame([\n",
    "            [f'{len(dev_y) / len(train_y):.2%}'] + get_distr(dev_y),\n",
    "            [f'{len(val_y) / len(train_y):.2%}'] + get_distr(val_y)\n",
    "        ], index=[f'fold {fold}: dev_y', f'fold {fold}: val_y'], \n",
    "           columns=['# of examples'] + list(range(LABEL_NUM))\n",
    "        ))\n",
    "\n",
    "        model = reg_model_factory()\n",
    "        model.fit(dev_x, dev_ids, dev_y, val_x, val_ids, val_y)\n",
    "        \n",
    "        pred_dev_y_reg = model.predict(dev_x, dev_ids, is_train=True)\n",
    "        dev_rmse_scores.append(rmse(dev_y, pred_dev_y_reg))\n",
    "        \n",
    "        pred_val_y_reg = model.predict(val_x, val_ids, is_train=True)\n",
    "        val_rmse_scores.append(rmse(val_y, pred_val_y_reg))\n",
    "\n",
    "        qwk_model = qwk_model_factory()\n",
    "        qwk_model.fit(pred_dev_y_reg, dev_ids, dev_y, pred_val_y_reg, val_ids, val_y)\n",
    "        \n",
    "        pred_dev_y = qwk_model.predict(pred_dev_y_reg, dev_ids, is_train=True)\n",
    "        dev_qwk_scores.append(quadratic_weighted_kappa(dev_y, pred_dev_y))\n",
    "        \n",
    "        pred_val_y = qwk_model.predict(pred_val_y_reg, val_ids, is_train=True)\n",
    "        val_qwk_scores.append(quadratic_weighted_kappa(val_y, pred_val_y))\n",
    "\n",
    "        pred_train_y_reg[val_indices] = pred_val_y_reg\n",
    "        pred_test_y_reg += model.predict(test_x, test_ids, is_train=False)\n",
    "        \n",
    "        models.append(model)\n",
    "        qwk_models.append(qwk_model)\n",
    "\n",
    "    pred_test_y_reg /= FOLDS_NUM\n",
    "\n",
    "    qwk_model = qwk_model_factory()\n",
    "    qwk_model = qwk_model.from_models(qwk_models)\n",
    "\n",
    "    pred_train_y = qwk_model.predict(pred_train_y_reg, train_ids, is_train=True)\n",
    "    train_qwk = quadratic_weighted_kappa(train_y, pred_train_y)\n",
    "    \n",
    "    display(get_formatted_cv_evals(dev_rmse_scores, val_rmse_scores, dev_qwk_scores, val_qwk_scores))\n",
    "\n",
    "    display(pd.DataFrame([[\n",
    "        '',\n",
    "        f'{np.mean(dev_rmse_scores):.3f}',\n",
    "        f'{1.96 * np.std(dev_rmse_scores):.3f}',\n",
    "        f'{np.mean(val_rmse_scores):.3f}',\n",
    "        f'{1.96 * np.std(val_rmse_scores):.3f}',\n",
    "        f'{np.mean(dev_qwk_scores):.3f}',\n",
    "        f'{1.96 * np.std(dev_qwk_scores):.3f}',\n",
    "        f'{np.mean(val_qwk_scores):.3f}',\n",
    "        f'{1.96 * np.std(val_qwk_scores):.3f}',\n",
    "        f'{train_qwk:.3f}'\n",
    "    ]], index=[name], columns=[\n",
    "        'params', \n",
    "        'dev cv rmse (avg)', 'dev cv rmse (conf int)', 'val cv rmse (avg)', 'val cv rmse (conf int)',\n",
    "        'dev cv qwk (avg)', 'dev cv qwk (conf int)', 'val cv qwk (avg)', 'val cv qwk (conf int)', \n",
    "        'train set qwk']))\n",
    "    \n",
    "    pred_test_y = qwk_model.predict(pred_test_y_reg, test_ids, is_train=False)\n",
    "    \n",
    "    display(pd.DataFrame([\n",
    "        get_distr(train_y),\n",
    "        get_distr(pred_train_y),\n",
    "        get_distr(pred_test_y),\n",
    "    ], index=['train_y', 'pred_train_y', 'pred_test_y'], columns=list(range(LABEL_NUM))))\n",
    "    \n",
    "    save_predictions(pred_test_y)\n",
    "    \n",
    "    conf_matrix = pd.DataFrame(\n",
    "        sk_cmatrix(train_y, pred_train_y), index=list(range(LABEL_NUM)), columns=list(range(LABEL_NUM)))\n",
    "    ax = sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
    "    ax.set_xlabel('y_pred')\n",
    "    ax.set_ylabel('y_true')\n",
    "    display(ax)\n",
    "    \n",
    "    return OrderedDict([\n",
    "        ('models',            models),\n",
    "        ('qwk_models',        qwk_models),\n",
    "        ('pred_train_y_reg',  pred_train_y_reg),\n",
    "        ('pred_train_y',      pred_train_y),\n",
    "        ('pred_test_y_reg',   pred_test_y_reg),\n",
    "        ('pred_test_y',       pred_test_y),\n",
    "    ])\n",
    "\n",
    "\n",
    "def display_train_example(idx):\n",
    "    data_id = train_ids[idx]\n",
    "    data = train_x.iloc[idx].to_dict()\n",
    "    df = pd.DataFrame([\n",
    "        ('Type', {1: 'dog', 2: 'cat'}[data['Type']]),\n",
    "        ('Name', data['Name']),\n",
    "        ('Age', data['Age']),\n",
    "        ('Breed1', breed_labels_map.get(data['Breed1'], \"MISSING\")),\n",
    "        ('Breed2', breed_labels_map.get(data['Breed2'], \"MISSING\")),\n",
    "        ('Gender', {1: 'Male', 2: 'Female', 3: 'Mixed'}[data['Gender']]),\n",
    "        ('Color1', color_labels_map.get(data['Color1'], \"MISSING\")),\n",
    "        ('Color2', color_labels_map.get(data['Color2'], \"MISSING\")),\n",
    "        ('MaturitySize', {1: 'small', 2: 'medium', 3: 'large', 4: 'extra large', 0: 'NOT SPECIFIED'}[data['MaturitySize']]),\n",
    "        ('FurLength', {1: 'short', 2: 'medium', 3: 'Long', 0: 'NOT SPECIFIED'}[data['FurLength']]),\n",
    "        ('Vaccinated', {1: 'Yes', 2: 'No', 3: 'Not sure'}[data['Vaccinated']]),\n",
    "        ('Dewormed', {1: 'Yes', 2: 'No', 3: 'Not sure'}[data['Dewormed']]),\n",
    "        ('Sterilized', {1: 'Yes', 2: 'No', 3: 'Not sure'}[data['Sterilized']]),\n",
    "        ('Health', {1: 'Healthy', 2: 'Minor', 3: 'Serious', 0: 'NOT SPECIFIED'}[data['Health']]),\n",
    "        ('Quantity', data['Quantity']),\n",
    "        ('Fee', data['Fee']),\n",
    "        ('State', state_labels_map[data['State']]),\n",
    "        ('VideoAmt', data['VideoAmt']),\n",
    "        ('PhotoAmt', data['PhotoAmt']),\n",
    "    ], columns=['attribute', 'value'])\n",
    "    df = df.set_index('attribute')\n",
    "    display(df)\n",
    "    display(data['Description'])\n",
    "    for i in range(1, int(data['PhotoAmt']) + 1):\n",
    "        display(Image(filename=str(PF_HOME / 'train_images' / f'{data_id}-{i}.jpg')))\n",
    "\n",
    "def show_random_example(pred_train_y, actual_label, pred_label, n=10):\n",
    "    indices = [idx for idx, (y, pred_y) in enumerate(zip(train_y, pred_train_y)) \n",
    "               if y == actual_label and pred_y == pred_label]\n",
    "    display_train_example(random.choice(indices))\n",
    "\n",
    "    \n",
    "def derive_feature_names(transformer):\n",
    "    if isinstance(transformer, Pipeline):\n",
    "        _, t = transformer.steps[-1]\n",
    "        return derive_feature_names(t)\n",
    "    elif isinstance(transformer, FeatureUnion):\n",
    "        return [f'{n}_{f}' for n, t in transformer.transformer_list for f in derive_feature_names(t)]\n",
    "    try:\n",
    "        return transformer.get_feature_names()\n",
    "    except AttributeError:\n",
    "        raise AttributeError(f'Transformer {transformer} does not have `get_feature_names` function')\n",
    "\n",
    "\n",
    "def apply_pipeline(pipeline, train_x, test_x):\n",
    "    train_f = pipeline.fit_transform(train_x)\n",
    "    test_f = pipeline.transform(test_x)\n",
    "    feature_names = derive_feature_names(pipeline)\n",
    "    return train_f, test_f, pipeline, feature_names\n",
    "\n",
    "\n",
    "def get_categorical_indices(feature_names):\n",
    "    return [i for i, name in enumerate(feature_names) if name.endswith(CATEGORY_SUFFIX)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "91a899437403e586b54b7652a4d6471c5e1fc0e4"
   },
   "outputs": [],
   "source": [
    "class ColumnSelector(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self, cat_cols, num_cols):\n",
    "        self._cat_cols = cat_cols\n",
    "        self._num_cols = num_cols\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, *_):\n",
    "        res_X = X[self._cat_cols + self._num_cols]\n",
    "        res_X.loc[:, self._cat_cols] = X[self._cat_cols].astype('category')\n",
    "        return res_X\n",
    "\n",
    "\n",
    "class Mapper(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self, func):\n",
    "        self._func = func\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, *_):\n",
    "        return self._func(X)\n",
    "\n",
    "\n",
    "class Features(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self, transformer, prefix='', categorical=False, feature_names=None):\n",
    "        self._transformer = transformer\n",
    "        self._prefix = prefix\n",
    "        self._categorical_suffix = CATEGORY_SUFFIX if categorical else ''\n",
    "        if feature_names is not None:\n",
    "            self._feature_names = self._format_feature_names(feature_names)\n",
    "            self._features_num = len(feature_names)\n",
    "        else:\n",
    "            self._feature_names = None\n",
    "            self._features_num = None\n",
    "    \n",
    "    def fit(self, *args, **kwds):\n",
    "        self._transformer.fit(*args, **kwds)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, *args, **kwds):\n",
    "        result = self._transformer.transform(*args, **kwds)\n",
    "        \n",
    "        if self._features_num is None:\n",
    "            self._features_num = self._get_size(result)\n",
    "        else:\n",
    "            assert self._features_num == self._get_size(result)\n",
    "        return result\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        if not self._feature_names:\n",
    "            try:\n",
    "                fns = self._transformer.get_feature_names()\n",
    "            except AttributeError:\n",
    "                if self._features_num is None:\n",
    "                    raise ValueError('Feature names cannot be taken before calling transform')\n",
    "                fns = range(self._features_num)\n",
    "            self._feature_names = self._format_feature_names(fns)\n",
    "        return self._feature_names\n",
    "    \n",
    "    def _format_feature_names(self, feature_names):\n",
    "        return [f'{self._prefix}{i}{self._categorical_suffix}' for i in feature_names]\n",
    "    \n",
    "    def _get_size(self, x):\n",
    "        if isinstance(x, list) or isinstance(x, pd.Series):\n",
    "            return len(x[0])\n",
    "        return x.shape[1]\n",
    "\n",
    "\n",
    "def Cols(cols, categorical=False):\n",
    "    return Features(\n",
    "        Mapper(lambda x: list(zip(*[x[c] for c in cols]))),\n",
    "        categorical=categorical,\n",
    "        feature_names=cols\n",
    "    )\n",
    "\n",
    "# def CatCols(cols):\n",
    "#     return Cols(cols, True)\n",
    "\n",
    "\n",
    "class CatCols(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self, cols, use_label_encoder=False):\n",
    "        self._cols = cols\n",
    "        self._feature_names = [f'{c}{CATEGORY_SUFFIX}' for c in cols]\n",
    "        \n",
    "        self._encoders = None\n",
    "        if use_label_encoder:\n",
    "            self._encoders = defaultdict(dict)\n",
    "            \n",
    "    def fit(self, X, *args, **kwds):        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, *args, **kwds):\n",
    "        col_values = []\n",
    "        for c in self._cols:\n",
    "            vals = X[c]\n",
    "            if self._encoders is not None:\n",
    "                vals = [self._encode_value(c, v) for v in vals]\n",
    "            col_values.append(vals)\n",
    "        return list(zip(*col_values))\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self._feature_names\n",
    "    \n",
    "    def _encode_value(self, col, value):\n",
    "        try:\n",
    "            result = self._encoders[col][value]\n",
    "        except KeyError:\n",
    "            result = len(self._encoders[col])\n",
    "            self._encoders[col][value] = result\n",
    "        return result\n",
    "        \n",
    "\n",
    "class Predictor(ABC):\n",
    "    @abstractmethod\n",
    "    def fit(self, train_x, train_ids, train_y, valid_x, valid_ids, valid_y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, x, ids, is_train):\n",
    "        pass\n",
    "\n",
    "\n",
    "class QwkPredictor(Predictor):\n",
    "    @abstractmethod\n",
    "    def from_models(self, models):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "3ca917ef29448f56f1fee2b9f58d282ef3a4ed52"
   },
   "outputs": [],
   "source": [
    "class LgbmPredictor(Predictor):\n",
    "    def __init__(self, params, f_names):\n",
    "        params = params.copy()\n",
    "        self.num_rounds = params.pop('num_rounds')\n",
    "        self.verbose_eval = params.pop('verbose_eval')\n",
    "        self.early_stop = params.pop('early_stop', None)\n",
    "\n",
    "        self.params = dict(params)\n",
    "        self.params['seed'] = SEED\n",
    "        \n",
    "        self.model = None\n",
    "        self._cat_indices = get_categorical_indices(f_names)\n",
    "\n",
    "    def fit(self, train_x, train_ids, train_y, valid_x, valid_ids, valid_y):\n",
    "        print(train_x.shape)\n",
    "        print(valid_x.shape)\n",
    "        d_train = lgb.Dataset(train_x, label=train_y, categorical_feature=self._cat_indices)\n",
    "        d_valid = lgb.Dataset(valid_x, label=valid_y, categorical_feature=self._cat_indices)\n",
    "\n",
    "        watch_list = [d_train, d_valid]\n",
    "\n",
    "        self.model = lgb.train(\n",
    "            self.params,\n",
    "            train_set=d_train,\n",
    "            num_boost_round=self.num_rounds,\n",
    "            valid_sets=watch_list,\n",
    "            verbose_eval=self.verbose_eval,\n",
    "            early_stopping_rounds=self.early_stop\n",
    "        )\n",
    "\n",
    "    def predict(self, x, ids, is_train):\n",
    "        return self.model.predict(x, num_iteration=self.model.best_iteration)\n",
    "\n",
    "    \n",
    "def display_lgbm_importance(r, f_names, n=100):\n",
    "    with pd.option_context('display.max_rows', None):\n",
    "        display(pd.DataFrame(\n",
    "            Counter(\n",
    "                dict(zip(f_names, r['models'][0].model.feature_importance('gain')))\n",
    "            ).most_common(n)\n",
    "        ))\n",
    "        \n",
    "\n",
    "def apply_coeffs(coeffs, x):\n",
    "    x_p = np.copy(x)\n",
    "    for i, pred in enumerate(x_p):\n",
    "        if pred < coeffs[0]:\n",
    "            x_p[i] = 0\n",
    "        elif coeffs[0] <= pred < coeffs[1]:\n",
    "            x_p[i] = 1\n",
    "        elif coeffs[1] <= pred < coeffs[2]:\n",
    "            x_p[i] = 2\n",
    "        elif coeffs[2] <= pred < coeffs[3]:\n",
    "            x_p[i] = 3\n",
    "        else:\n",
    "            x_p[i] = 4\n",
    "    return x_p\n",
    "\n",
    "\n",
    "class QwkOptimizer(QwkPredictor):\n",
    "    def __init__(self):\n",
    "        self.coefficient = None\n",
    "\n",
    "    def fit(self, train_x, train_ids, train_y, valid_x, valid_ids, valid_y):\n",
    "        assert len(train_x) == len(train_y)\n",
    "        assert len(valid_x) == len(valid_y)\n",
    "        loss_partial = partial(self._kappa_loss, x=train_x, y=train_y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coefficient = scipy.optimize.minimize(\n",
    "            loss_partial, initial_coef, method='nelder-mead')['x']\n",
    "        print(f'Coefficients after fitting are {self.coefficient}')\n",
    "\n",
    "    def predict(self, x, ids, is_train):\n",
    "        return apply_coeffs(self.coefficient, x)\n",
    "\n",
    "    def from_models(self, models):\n",
    "        self.coefficient = np.mean([m.coefficient for m in models], axis=0)\n",
    "        print(f'Average coefficients taken from models are {self.coefficient}')\n",
    "        return self\n",
    "\n",
    "    def _kappa_loss(self, coeffs, x, y):\n",
    "        x_p = apply_coeffs(coeffs, x)\n",
    "        return -quadratic_weighted_kappa(y, x_p)\n",
    "\n",
    "    \n",
    "class DistributionQwkPredictor(QwkPredictor):\n",
    "    def __init__(self):\n",
    "        self.coefficients = None\n",
    "    \n",
    "    def fit(self, train_reg, train_ids, train_y, valid_reg, valid_ids, valid_y):\n",
    "        y_freq = pd.Series(list(valid_y)).value_counts()\n",
    "        cum_class_distr = [int(y_freq.loc[list(range(i))].sum()) for i in range(1, LABEL_NUM)]\n",
    "        \n",
    "        regs = sorted(valid_reg)\n",
    "        coefficients = []\n",
    "        for cls_count in cum_class_distr:\n",
    "            left_reg = regs[cls_count - 1] if cls_count > 0 else regs[0]\n",
    "            right_reg = regs[cls_count] if cls_count < len(regs) else regs[-1]\n",
    "            coefficients.append(\n",
    "                np.mean([left_reg, right_reg])\n",
    "            )\n",
    "            \n",
    "        print(f'Trained coefficients are {coefficients}')\n",
    "        self.coefficients = np.array(coefficients)\n",
    "\n",
    "    def predict(self, y_pred_reg, ids, is_train):\n",
    "        return apply_coeffs(self.coefficients, y_pred_reg)\n",
    "    \n",
    "    def from_models(self, models):\n",
    "        self.coefficients = np.mean([m.coefficients for m in models], axis=0)\n",
    "        print(f'Average coefficients taken from models are {self.coefficients}')\n",
    "        return self\n",
    "\n",
    "    \n",
    "class TrainingSetDistributionPredictor(QwkPredictor):\n",
    "    def __init__(self):\n",
    "        self.coefficient = None\n",
    "        self.y_freq = None\n",
    "\n",
    "    def fit(self, train_x, train_ids, train_y, valid_x, valid_ids, valid_y):\n",
    "        self.y_freq = pd.Series(list(train_y)).value_counts()\n",
    "\n",
    "    def predict(self, y_pred_reg, ids, is_train):\n",
    "        return self._values_to_classes(\n",
    "            y_pred_reg, self.y_freq\n",
    "        )\n",
    "        \n",
    "    def _values_to_classes(self, ys: np.ndarray, y_freq: pd.Series, verbose=False) -> np.ndarray:\n",
    "        y_freq = y_freq / y_freq.sum()\n",
    "        cum_freqs = [y_freq.loc[list(range(i))].sum() for i in range(LABEL_NUM)]\n",
    "        thresholds = []\n",
    "        # TODO faster\n",
    "        for y in sorted(set(ys)):\n",
    "            fraction = (ys < y).mean()\n",
    "            if fraction > cum_freqs[len(thresholds) + 1]:\n",
    "                thresholds.append(y)\n",
    "                if len(thresholds) == LABEL_NUM - 1:\n",
    "                    break\n",
    "        ys_out = np.array([LABEL_NUM - 1] * len(ys), dtype=int)\n",
    "        for i in reversed(range(LABEL_NUM - 1)):\n",
    "            ys_out[ys < thresholds[i]] = i\n",
    "        if verbose:\n",
    "            logging.info('thresholds: ' +  ' '.join(f'{th:.3f}' for th in thresholds))\n",
    "        return ys_out\n",
    "    \n",
    "    def from_models(self, models):\n",
    "        self.y_freq = pd.Series(list(np.mean([\n",
    "            m.y_freq.sort_index() for m in models\n",
    "        ], axis=0)))\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "f5dfd5d1a821501738b3e53d473665d34f6cfb69"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "class PetPyTorchDataset(Dataset):\n",
    "    def __init__(self, pet_ids, train, y=None, transform=None):\n",
    "        self.transform = transforms.Compose(transform or [])\n",
    "        \n",
    "        image_dir = PF_HOME / ('train_images' if train else 'test_images')\n",
    "        self.image_paths, data_indices = self._get_image_paths(image_dir, pet_ids)\n",
    "        \n",
    "        if y is not None:\n",
    "            self.y = np.array(by_indices(y, data_indices), dtype=np.float32)\n",
    "        else: \n",
    "            self.y = np.full(len(data_indices), -1, dtype=np.float32)\n",
    "            \n",
    "        self.pet_ids = by_indices(pet_ids, data_indices)\n",
    "        \n",
    "    def _get_image_paths(self, image_dir, pet_ids, only_first_image=True):\n",
    "        def get_image_idx(p):\n",
    "            return int(str(p).rsplit('-', 1)[1].split('.')[0])\n",
    "        def get_pet_id(p):\n",
    "            return str(p).rsplit('/', 1)[1].split('-')[0]\n",
    "        \n",
    "        image_paths_per_pet_id = defaultdict(list)\n",
    "        for p in image_dir.iterdir():\n",
    "            image_paths_per_pet_id[get_pet_id(p)].append(p)\n",
    "        \n",
    "        image_paths = []\n",
    "        data_indices = []\n",
    "        for p_index, p_id in enumerate(pet_ids):\n",
    "            pet_image_paths = sorted(image_paths_per_pet_id[p_id], key=get_image_idx)\n",
    "            if len(pet_image_paths) > 0 and only_first_image:\n",
    "                pet_image_paths = [pet_image_paths[0]]\n",
    "            image_paths.extend(pet_image_paths)\n",
    "            data_indices.extend([p_index] * len(pet_image_paths))\n",
    "        \n",
    "        return image_paths, data_indices\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = pil.Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        return self.transform(image), torch.Tensor([self.y[idx]]), self.pet_ids[idx]\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base = models.resnet34(pretrained=False)\n",
    "        self.base.load_state_dict(self._load_net_state())\n",
    "        \n",
    "        last_resnet_size = self.base.fc.in_features\n",
    "        print(f'Last resnet layer is: {last_resnet_size}')\n",
    "        \n",
    "        self.fc = nn.Linear(last_resnet_size * 4, 1)\n",
    "    \n",
    "    def _load_net_state(self):\n",
    "        net_state_path = DATA_HOME / 'resnet34pytorch/resnet34-333f7ec4.pth'\n",
    "        with net_state_path.open('rb') as in_file:\n",
    "            return torch.load(in_file)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base.conv1(x)\n",
    "        x = self.base.bn1(x)\n",
    "        x = self.base.relu(x)\n",
    "        x = self.base.maxpool(x)\n",
    "\n",
    "        x = self.base.layer1(x)\n",
    "        x = self.base.layer2(x)\n",
    "        x = self.base.layer3(x)\n",
    "        x = self.base.layer4(x)\n",
    "\n",
    "        x = self.base.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        loss = torch.sqrt(self.mse(yhat,y) + self.eps)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "class PyTorchModel(Predictor):\n",
    "    def __init__(self, net, lr, epochs, batch_size, verbose=True):\n",
    "        self.net = net\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.transforms = [\n",
    "            transforms.Resize((256, 256)),\n",
    "#             transforms.CenterCrop(224),\n",
    "#             transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "        \n",
    "        device_type = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f'Device `{device_type}` will be used')\n",
    "        self.device = torch.device(device_type)\n",
    "        self.net.to(self.device)\n",
    "    \n",
    "    def fit(self, train_x, train_ids, train_y, valid_x, valid_ids, valid_y):\n",
    "        print(f'Loading train images for {len(train_ids)} pet ids')\n",
    "        train_ds = PetPyTorchDataset(train_ids, train=True, y=train_y, transform=self.transforms)\n",
    "        train_loader = DataLoader(train_ds, shuffle=True, batch_size=self.batch_size)\n",
    "        \n",
    "        print(f'Loading valid images for {len(valid_ids)} pet ids')\n",
    "        valid_ds = PetPyTorchDataset(valid_ids, train=True, y=valid_y, transform=self.transforms)\n",
    "        valid_loader = DataLoader(valid_ds, shuffle=True, batch_size=self.batch_size)\n",
    "        \n",
    "        loss_func = RMSELoss()\n",
    "        optimizer = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        \n",
    "        best_valid_loss = float('inf')\n",
    "        best_net_state = None\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            for x, y, pet_ids in tqdm(train_loader, desc=f'Training (epoch={epoch})', leave=False):\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                y_pred = self.net(x)\n",
    "\n",
    "                loss = loss_func(y_pred, y)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                valid_y_pred, valid_y, _ = self._interference(valid_loader)\n",
    "                valid_loss = loss_func(valid_y_pred, valid_y)\n",
    "\n",
    "                loss_val = loss.item()\n",
    "                valid_loss_val = valid_loss.item()\n",
    "\n",
    "                if self.verbose:\n",
    "                    print(f'Epoch={epoch}, '\n",
    "                          f'Batch loss={loss_val}, '\n",
    "                          f'Valid loss={valid_loss_val}')\n",
    "\n",
    "                if valid_loss_val < best_valid_loss:\n",
    "                    best_valid_loss = valid_loss_val\n",
    "                    best_net_state = self.net.state_dict()\n",
    "\n",
    "        self.net.load_state_dict(best_net_state)\n",
    "    \n",
    "    def _interference(self, loader):\n",
    "        with torch.no_grad():\n",
    "            preds_per_batch = []\n",
    "            y_per_batch = []\n",
    "            pet_ids = []\n",
    "            for batch_x, batch_y, batch_pet_ids in tqdm(loader, desc=f'Interference...', leave=False):\n",
    "                batch_x = batch_x.to(self.device)\n",
    "                batch_y = batch_y.to(self.device)\n",
    "                preds = self.net(batch_x)\n",
    "                preds_per_batch.append(preds)\n",
    "                y_per_batch.append(batch_y)\n",
    "                pet_ids.extend(batch_pet_ids)\n",
    "            return torch.cat(preds_per_batch), torch.cat(y_per_batch), pet_ids\n",
    "\n",
    "    def predict(self, x, ids, is_train):\n",
    "        ds = PetPyTorchDataset(ids, train=is_train, transform=self.transforms)\n",
    "        loader = DataLoader(ds, shuffle=False, batch_size=self.batch_size)\n",
    "        y_pred, _, inf_ids = self._interference(loader)\n",
    "        y_pred = y_pred.squeeze(1).cpu().numpy()\n",
    "        \n",
    "        preds_per_pet_id = {}\n",
    "        for pred, pet_id in zip(y_pred, inf_ids):\n",
    "            assert pet_id not in preds_per_pet_id\n",
    "            preds_per_pet_id[pet_id] = pred\n",
    "        \n",
    "        def random_class():\n",
    "            return float(np.random.randint(0, LABEL_NUM, dtype='int'))\n",
    "        \n",
    "        return np.array([preds_per_pet_id.get(pet_id, random_class()) for pet_id in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "a1729b04b73f84570380a94aaec5e2b9e6cb4df5"
   },
   "outputs": [],
   "source": [
    "def get_idx(p):\n",
    "    return int(str(p).rsplit('-', 1)[1].split('.')[0])\n",
    "\n",
    "def replace_missing_with_nan(x):\n",
    "    if isinstance(x, scipy.sparse.csr_matrix):\n",
    "        x = x.toarray()\n",
    "    np.place(x, x == DEFAULT, np.nan)\n",
    "    return x\n",
    "\n",
    "def read_data(data_path, shuffle=False):\n",
    "    X = pd.read_csv(data_path)\n",
    "    if shuffle:\n",
    "        X = X.sample(frac=1, random_seed=SEED).reset_index(drop=True)\n",
    "    try:\n",
    "        y = list(X.AdoptionSpeed)\n",
    "    except AttributeError:\n",
    "        y = None\n",
    "    ids = list(X.PetID)\n",
    "#     X.drop(['AdoptionSpeed'], axis=1, inplace=True, errors='ignore')\n",
    "    return X, y, ids\n",
    "\n",
    "\n",
    "def read_metadata(pet_ids_list, metadata_dir):\n",
    "    def extract_label_annotations(metadata_dict):\n",
    "        def by_score(xs): return xs[1]\n",
    "        label_annotations = []\n",
    "        for annotation in metadata_dict.get('labelAnnotations', []):\n",
    "            desc = annotation['description']\n",
    "            score = annotation['score']\n",
    "            label_annotations.append((desc, score))\n",
    "        return sorted(label_annotations, key=by_score, reverse=True)\n",
    "    \n",
    "    def extract_dominant_colors(metadata_dict):\n",
    "        def by_score(xs): return xs[3]\n",
    "        dominant_colors = []\n",
    "        colors = metadata_dict.get('imagePropertiesAnnotation', {}).get('dominantColors', {}).get('colors', [])\n",
    "        for color_obj in colors:\n",
    "            rgb_obj = color_obj['color']\n",
    "            red = rgb_obj.get('red', 0) / 255\n",
    "            green = rgb_obj.get('green', 0) / 255\n",
    "            blue = rgb_obj.get('blue', 0) / 255\n",
    "            score = color_obj['score']\n",
    "            pixel_fraction = color_obj['pixelFraction']\n",
    "            dominant_colors.append(\n",
    "                (red, green, blue, score, pixel_fraction)\n",
    "            )\n",
    "        return dominant_colors\n",
    "    \n",
    "    def _get_image_paths(image_dir):\n",
    "        def get_pet_id(p):\n",
    "            return str(p).rsplit('/', 1)[1].split('-')[0]\n",
    "        def get_sorted_photos_by_idx(paths):\n",
    "            return sorted((get_idx(p), p) for p in paths)\n",
    "        \n",
    "        meta_paths_per_pet_id = defaultdict(list)\n",
    "        for p in image_dir.iterdir():\n",
    "            pet_id = get_pet_id(p)\n",
    "            meta_paths_per_pet_id[pet_id].append(p)\n",
    "        \n",
    "        return {\n",
    "            pet_id: get_sorted_photos_by_idx(meta_paths) \n",
    "            for pet_id, meta_paths in meta_paths_per_pet_id.items()\n",
    "        }\n",
    "    \n",
    "    image_path_per_pet_id = _get_image_paths(Path(metadata_dir))\n",
    "    for pet_id in tqdm(pet_ids_list, 'Reading pet ids'):\n",
    "        pet_metadata_paths = image_path_per_pet_id.get(pet_id, [])\n",
    "        \n",
    "        pet_label_annotations = []\n",
    "        pet_dominant_colors = []\n",
    "        \n",
    "        for idx, metadata_path in pet_metadata_paths:\n",
    "            with metadata_path.open() as in_file:\n",
    "                metadata_dict = json.load(in_file)\n",
    "                \n",
    "                pet_label_annotations.append(extract_label_annotations(metadata_dict))\n",
    "                pet_dominant_colors.append(extract_dominant_colors(metadata_dict))\n",
    "        \n",
    "        yield pet_label_annotations, pet_dominant_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "557cc2919af6c5a6ba3e8ef6e8f3b0a452ca4add"
   },
   "outputs": [],
   "source": [
    "def read_sentiment_data(pet_ids_list, sentiment_dir):\n",
    "    sentiment_dir = Path(sentiment_dir)\n",
    "    missed_sentiments = 0\n",
    "    for pet_id in pet_ids_list:\n",
    "        sentiment_data_path = sentiment_dir / f'{pet_id}.json'\n",
    "        \n",
    "        if not sentiment_data_path.exists():\n",
    "            yield [], {}, DEFAULT, DEFAULT\n",
    "            missed_sentiments += 1\n",
    "        else:\n",
    "            with sentiment_data_path.open() as in_file:\n",
    "                sentiment_data = json.load(in_file)\n",
    "\n",
    "                sentence_sents = [(obj['sentiment']['magnitude'], obj['sentiment']['score']) \n",
    "                                       for obj in sentiment_data['sentences']]\n",
    "                entity_sents = {obj['name'].lower().strip(): obj['salience']\n",
    "                            for obj in sentiment_data['entities']}\n",
    "                doc_sent_magnitude = sentiment_data['documentSentiment']['magnitude']\n",
    "                doc_sent_score = sentiment_data['documentSentiment']['score']\n",
    "\n",
    "                yield sentence_sents, entity_sents, (doc_sent_magnitude, doc_sent_score)\n",
    "    print(f'Missed sentiment files for {missed_sentiments} pet ids')\n",
    "\n",
    "def pet_data_assigner(X, data_reader, **mappers):\n",
    "    extacted_data = defaultdict(list)\n",
    "    data_num = 0\n",
    "    errors_per_mapper = Counter()\n",
    "    for data in data_reader:\n",
    "        for k, (map_func, default_val) in mappers.items():\n",
    "            try:\n",
    "                map_result = map_func(*data)\n",
    "            except Exception as e:\n",
    "                map_result = default_val\n",
    "                errors_per_mapper[k] += 1\n",
    "            extacted_data[k].append(map_result)\n",
    "        data_num += 1 \n",
    "    \n",
    "    for k, errors_num in errors_per_mapper.items():\n",
    "        if errors_num > 0: \n",
    "            print(f'There were {errors_num} errors ({int(errors_num * 100 / data_num)}%) for mapper {k}')\n",
    "    \n",
    "    for k, data in extacted_data.items():\n",
    "        X[k] = data\n",
    "\n",
    "        \n",
    "def train_test_data_assigner(train_data_reader, test_data_reader, **mapper_info):\n",
    "    mappers = {}\n",
    "    col_assignments = []\n",
    "    for k, (mapper_func, default, add_to) in mapper_info.items():\n",
    "        mappers[k] = (mapper_func, default)\n",
    "        if add_to is not None:\n",
    "            col_assignments.append((add_to, k))\n",
    "    \n",
    "    pet_data_assigner(\n",
    "        train_x, train_data_reader, **mappers\n",
    "    )\n",
    "    pet_data_assigner(\n",
    "        test_x, test_data_reader, **mappers\n",
    "    )\n",
    "    \n",
    "    for cols_list, col in col_assignments:\n",
    "        if col not in cols_list:\n",
    "            cols_list.append(col)\n",
    "        \n",
    "def m(mapper_func, *, default, add_to=None):\n",
    "    return mapper_func, default, add_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "c30ce82221a96a93bfcdf43982f6ea6967d6ab48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14993, 24)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(3948, 23)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RescuerID', 'PetID', 'AdoptionSpeed']\n",
      "['Name', 'Description']\n",
      "['Age', 'Quantity', 'Fee', 'VideoAmt', 'PhotoAmt', 'MaturitySize', 'FurLength', 'Health']\n",
      "['Gender', 'Sterilized', 'Vaccinated', 'Breed2', 'Type', 'Dewormed', 'Color3', 'Color2', 'Breed1', 'State', 'Color1']\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, train_ids = read_data(PF_HOME / 'train' / 'train.csv')\n",
    "test_x, test_y, test_ids = read_data(PF_HOME / 'test' / 'test.csv')\n",
    "\n",
    "train_rescue_ids = list(train_x['RescuerID'])\n",
    "splits = list(greedy_group_k_fold_stratified(train_x, train_y, groups=train_rescue_ids, k=FOLDS_NUM))\n",
    "\n",
    "display(train_x.shape)\n",
    "display(test_x.shape)\n",
    "\n",
    "special_cols = ['RescuerID', 'PetID', 'AdoptionSpeed']\n",
    "text_cols = ['Name', 'Description']\n",
    "num_cols = ['Age', 'Quantity', 'Fee', 'VideoAmt', 'PhotoAmt', 'MaturitySize', 'FurLength', 'Health']\n",
    "cat_cols = list(set(train_x.columns) - set(text_cols) - set(num_cols) - set(special_cols))\n",
    "\n",
    "breed_labels_map = {row.BreedID: row.BreedName \n",
    "                   for _, row in pd.read_csv(PF_HOME / 'breed_labels.csv').iterrows()}\n",
    "\n",
    "state_labels_map = {row.StateID: row.StateName \n",
    "                   for _, row in pd.read_csv(PF_HOME / 'state_labels.csv').iterrows()}\n",
    "\n",
    "color_labels_map = {row.ColorID: row.ColorName \n",
    "                   for _, row in pd.read_csv(PF_HOME / 'color_labels.csv').iterrows()}\n",
    "\n",
    "# get_breed_rating_map(PF_HOME / 'breed_rating.json')\n",
    "\n",
    "# breed_rating_map = {k: obj for d in breed_rating.values() for k, obj in d.items()}\n",
    "\n",
    "# display(len(breed_labels_map))\n",
    "\n",
    "print(special_cols)\n",
    "print(text_cols)\n",
    "print(num_cols)\n",
    "print(cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map_for_single_type(breed_rating, breed_labels):\n",
    "    labels = list(breed_rating.keys()) + list(breed_labels.values())\n",
    "    print(labels)\n",
    "    \n",
    "    tfidf = TfidfTransformer()\n",
    "    tfidf.fit_transform(labels)\n",
    "    word_to_idf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))\n",
    "    \n",
    "    print(word_to_idf)\n",
    "    \n",
    "    breed_rating_map = {}\n",
    "    for breed_id, breed_name in breed_labels.items():\n",
    "        if breed_name in breed_rating:\n",
    "            breed_rating_map[breed_id] = breed_rating[breed_name]\n",
    "        else:\n",
    "            breed_name_idf = {token: word_to_idf[token] for token in breed_name.split()}\n",
    "            best_idf = None\n",
    "            best_data = None\n",
    "            for key, breed_data in breed_rating.items():\n",
    "                key_idf = {token: word_to_idf[token] for token in key.split()}\n",
    "                common_tokens = set(breed_name_idf) & set(key_idf)\n",
    "                \n",
    "\n",
    "def get_breed_rating_map(breed_rating_path, breed_labels_path):\n",
    "    breed_labels = pd.read_csv(breed_labels_path)\n",
    "    \n",
    "    dog_breed_labels = {row.BreedID: row.BreedName.lower()\n",
    "                        for _, row in breed_labels.iterrows() if row.Type == 1}\n",
    "    cat_breed_labels = {row.BreedID: row.BreedName.lower()\n",
    "                        for _, row in breed_labels.iterrows() if row.Type == 2}\n",
    "    \n",
    "    breed_rating = json.loads(breed_rating_path.read_text())\n",
    "    \n",
    "    get_map_for_single_type(\n",
    "        {k.lower(): v for k, v in breed_rating['dog_breeds'].items()}, \n",
    "        dog_breed_labels\n",
    "    )\n",
    "    \n",
    "    breed_rating_raw = {k: obj for d in breed_rating.values() for k, obj in d.items()}\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_breed_rating_map(\n",
    "#     breed_rating_path=PF_HOME/'breed_rating.json', \n",
    "#     breed_labels_path=PF_HOME/'breed_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract datasets\n",
    "# https://www.kaggle.com/christofhenkel/extract-image-features-from-pretrained-nn\n",
    "train_img_features = pd.read_csv('../input/extract-image-features-from-pretrained-nn/train_img_features.csv')\n",
    "test_img_features = pd.read_csv('../input/extract-image-features-from-pretrained-nn/test_img_features.csv')\n",
    "\n",
    "# img_features columns set names\n",
    "col_names =[\"PetID\"] + [\"{}_img_feature\".format(_) for _ in range(256)]\n",
    "train_img_features.columns = col_names\n",
    "test_img_features.columns = col_names\n",
    "\n",
    "train_x = pd.merge(train_x, train_img_features, on=\"PetID\")\n",
    "test_x = pd.merge(test_x, test_img_features, on=\"PetID\")\n",
    "\n",
    "num_cols = ['Age', 'Quantity', 'Fee', 'VideoAmt', 'PhotoAmt', 'MaturitySize', 'FurLength', 'Health'] + [\n",
    "    \"{}_img_feature\".format(_) for _ in range(256)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_la(l_annotations):\n",
    "    values_per_label = defaultdict(list)\n",
    "    for annots in l_annotations:\n",
    "        for label, score in annots:\n",
    "            values_per_label[label].append(score)\n",
    "    return {label: np.mean(values) for label, values in values_per_label.items()}\n",
    "\n",
    "def neg_skew(vals):\n",
    "    return np.abs(np.max([scipy.stats.skew(vals), 0]))\n",
    "\n",
    "def pos_skew(vals):\n",
    "    return np.min([scipy.stats.skew(vals), 0])\n",
    "\n",
    "def la_vals(la):\n",
    "    return [val for label, val in la]\n",
    "\n",
    "def dict_vals(dictionary):\n",
    "    return list(dictionary.values())\n",
    "\n",
    "def rms(vals):\n",
    "    return np.sqrt(np.mean(np.array(vals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "cf85b8c1c77eddef6abd99481741845f6a713c44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missed sentiment files for 551 pet ids\n",
      "There were 5074 errors (33%) for mapper third_sentence_magnitude\n",
      "There were 5074 errors (33%) for mapper third_sentence_score\n",
      "There were 3161 errors (21%) for mapper second_sentence_magnitude\n",
      "There were 3161 errors (21%) for mapper second_sentence_score\n",
      "There were 551 errors (3%) for mapper first_sentence_magnitude\n",
      "There were 551 errors (3%) for mapper first_sentence_score\n",
      "There were 551 errors (3%) for mapper sentence_magnitude_MIN\n",
      "There were 551 errors (3%) for mapper sentence_magnitude_MAX\n",
      "There were 551 errors (3%) for mapper sentence_magnitude_MEDIAN\n",
      "There were 551 errors (3%) for mapper sentence_magnitude_MEAN\n",
      "There were 551 errors (3%) for mapper sentence_magnitude_STD\n",
      "There were 551 errors (3%) for mapper sentence_magnitude_RMS\n",
      "There were 551 errors (3%) for mapper sentence_magnitude_KURTOSIS\n",
      "There were 551 errors (3%) for mapper sentence_magnitude_NEG_SKEW\n",
      "There were 551 errors (3%) for mapper sentence_magnitude_POS_SKEW\n",
      "There were 551 errors (3%) for mapper sentence_score_MIN\n",
      "There were 551 errors (3%) for mapper sentence_score_MAX\n",
      "There were 551 errors (3%) for mapper sentence_score_MEDIAN\n",
      "There were 551 errors (3%) for mapper sentence_score_MEAN\n",
      "There were 551 errors (3%) for mapper sentence_score_STD\n",
      "There were 551 errors (3%) for mapper sentence_score_RMS\n",
      "There were 551 errors (3%) for mapper sentence_score_KURTOSIS\n",
      "There were 551 errors (3%) for mapper sentence_score_NEG_SKEW\n",
      "There were 551 errors (3%) for mapper sentence_score_POS_SKEW\n",
      "There were 551 errors (3%) for mapper sentence_count\n",
      "There were 551 errors (3%) for mapper entity_sents\n",
      "There were 551 errors (3%) for mapper doc_sent_magnitude\n",
      "There were 551 errors (3%) for mapper doc_sent_score\n",
      "Missed sentiment files for 133 pet ids\n",
      "There were 1233 errors (31%) for mapper third_sentence_magnitude\n",
      "There were 1233 errors (31%) for mapper third_sentence_score\n",
      "There were 793 errors (20%) for mapper second_sentence_magnitude\n",
      "There were 793 errors (20%) for mapper second_sentence_score\n",
      "There were 133 errors (3%) for mapper first_sentence_magnitude\n",
      "There were 133 errors (3%) for mapper first_sentence_score\n",
      "There were 133 errors (3%) for mapper sentence_magnitude_MIN\n",
      "There were 133 errors (3%) for mapper sentence_magnitude_MAX\n",
      "There were 133 errors (3%) for mapper sentence_magnitude_MEDIAN\n",
      "There were 133 errors (3%) for mapper sentence_magnitude_MEAN\n",
      "There were 133 errors (3%) for mapper sentence_magnitude_STD\n",
      "There were 133 errors (3%) for mapper sentence_magnitude_RMS\n",
      "There were 133 errors (3%) for mapper sentence_magnitude_KURTOSIS\n",
      "There were 133 errors (3%) for mapper sentence_magnitude_NEG_SKEW\n",
      "There were 133 errors (3%) for mapper sentence_magnitude_POS_SKEW\n",
      "There were 133 errors (3%) for mapper sentence_score_MIN\n",
      "There were 133 errors (3%) for mapper sentence_score_MAX\n",
      "There were 133 errors (3%) for mapper sentence_score_MEDIAN\n",
      "There were 133 errors (3%) for mapper sentence_score_MEAN\n",
      "There were 133 errors (3%) for mapper sentence_score_STD\n",
      "There were 133 errors (3%) for mapper sentence_score_RMS\n",
      "There were 133 errors (3%) for mapper sentence_score_KURTOSIS\n",
      "There were 133 errors (3%) for mapper sentence_score_NEG_SKEW\n",
      "There were 133 errors (3%) for mapper sentence_score_POS_SKEW\n",
      "There were 133 errors (3%) for mapper sentence_count\n",
      "There were 133 errors (3%) for mapper entity_sents\n",
      "There were 133 errors (3%) for mapper doc_sent_magnitude\n",
      "There were 133 errors (3%) for mapper doc_sent_score\n"
     ]
    }
   ],
   "source": [
    "train_test_data_assigner(\n",
    "    read_sentiment_data(train_ids, PF_HOME / 'train_sentiment'),\n",
    "    read_sentiment_data(test_ids, PF_HOME / 'test_sentiment'),\n",
    "    first_sentence_magnitude=     m(lambda ss, es, ds: ss[0][0], default=DEFAULT, add_to=num_cols),\n",
    "    first_sentence_score=         m(lambda ss, es, ds: ss[0][1], default=DEFAULT, add_to=num_cols),\n",
    "    second_sentence_magnitude=    m(lambda ss, es, ds: ss[1][0], default=DEFAULT, add_to=num_cols),\n",
    "    second_sentence_score=        m(lambda ss, es, ds: ss[1][1], default=DEFAULT, add_to=num_cols),\n",
    "    third_sentence_magnitude=     m(lambda ss, es, ds: ss[2][0], default=DEFAULT, add_to=num_cols),\n",
    "    third_sentence_score=         m(lambda ss, es, ds: ss[2][1], default=DEFAULT, add_to=num_cols),\n",
    "    sentence_magnitude_MIN=       m(lambda ss, es, ds: np.min([m for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    sentence_magnitude_MAX=       m(lambda ss, es, ds: np.max([m for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    sentence_magnitude_MEDIAN=    m(lambda ss, es, ds: np.median([m for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    sentence_magnitude_MEAN=      m(lambda ss, es, ds: np.mean([m for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    sentence_magnitude_STD=       m(lambda ss, es, ds: np.std([m for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    sentence_magnitude_RMS=       m(lambda ss, es, ds: rms([m for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    sentence_magnitude_KURTOSIS=  m(lambda ss, es, ds: scipy.stats.kurtosis([m for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    sentence_magnitude_NEG_SKEW=  m(lambda ss, es, ds: neg_skew([m for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    sentence_magnitude_POS_SKEW=  m(lambda ss, es, ds: pos_skew([m for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    sentence_score_MIN=       m(lambda ss, es, ds: np.min([s for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    sentence_score_MAX=       m(lambda ss, es, ds: np.max([s for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    sentence_score_MEDIAN=    m(lambda ss, es, ds: np.median([s for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    sentence_score_MEAN=      m(lambda ss, es, ds: np.mean([s for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    sentence_score_STD=       m(lambda ss, es, ds: np.std([s for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    sentence_score_RMS=       m(lambda ss, es, ds: rms([s for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    sentence_score_KURTOSIS=  m(lambda ss, es, ds: scipy.stats.kurtosis([s for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    sentence_score_NEG_SKEW=  m(lambda ss, es, ds: neg_skew([s for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    sentence_score_POS_SKEW=  m(lambda ss, es, ds: pos_skew([s for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    sentence_count=               m(lambda ss, es, ds: len(ss), default=DEFAULT, add_to=num_cols),\n",
    "    entity_sents=                 m(lambda ss, es, ds: es, default={}),\n",
    "    doc_sent_magnitude=           m(lambda ss, es, ds: ds[0], default=DEFAULT, add_to=num_cols),\n",
    "    doc_sent_score=               m(lambda ss, es, ds: ds[1], default=DEFAULT, add_to=num_cols),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "2915cf92ebfaf1bddb29b8c439df39d4ac912f0a",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d35dafc7e8946279a5037bdac64f53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Reading pet ids', max=14993, style=ProgressStyle(description_â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages/numpy/core/_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There were 341 errors (2%) for mapper first_img_la\n",
      "There were 343 errors (2%) for mapper first_img_la_score_MIN\n",
      "There were 343 errors (2%) for mapper first_img_la_score_MAX\n",
      "There were 341 errors (2%) for mapper first_img_la_score_MEDIAN\n",
      "There were 341 errors (2%) for mapper first_img_la_score_MEAN\n",
      "There were 341 errors (2%) for mapper first_img_la_score_STD\n",
      "There were 341 errors (2%) for mapper first_img_la_score_RMS\n",
      "There were 341 errors (2%) for mapper first_img_la_score_KURTOSIS\n",
      "There were 341 errors (2%) for mapper first_img_la_score_NEG_SKEW\n",
      "There were 341 errors (2%) for mapper first_img_la_score_POS_SKEW\n",
      "There were 341 errors (2%) for mapper all_imgs_la_score_MIN\n",
      "There were 341 errors (2%) for mapper all_imgs_la_score_MAX\n",
      "There were 341 errors (2%) for mapper first_img_dm_score_MIN\n",
      "There were 341 errors (2%) for mapper first_img_dm_score_MAX\n",
      "There were 341 errors (2%) for mapper first_img_dm_score_MEDIAN\n",
      "There were 341 errors (2%) for mapper first_img_dm_score_MEAN\n",
      "There were 341 errors (2%) for mapper first_img_dm_score_STD\n",
      "There were 341 errors (2%) for mapper first_img_dm_score_RMS\n",
      "There were 341 errors (2%) for mapper first_img_dm_score_KURTOSIS\n",
      "There were 341 errors (2%) for mapper first_img_dm_score_NEG_SKEW\n",
      "There were 341 errors (2%) for mapper first_img_dm_score_POS_SKEW\n",
      "There were 341 errors (2%) for mapper all_imgs_dm_score_MIN\n",
      "There were 341 errors (2%) for mapper all_imgs_dm_score_MAX\n",
      "There were 341 errors (2%) for mapper first_img_dm_pxl_frac_MIN\n",
      "There were 341 errors (2%) for mapper first_img_dm_pxl_frac_MAX\n",
      "There were 341 errors (2%) for mapper first_img_dm_pxl_frac_MEDIAN\n",
      "There were 341 errors (2%) for mapper first_img_dm_pxl_frac_MEAN\n",
      "There were 341 errors (2%) for mapper first_img_dm_pxl_frac_STD\n",
      "There were 341 errors (2%) for mapper first_img_dm_pxl_frac_RMS\n",
      "There were 341 errors (2%) for mapper first_img_dm_pxl_frac_KURTOSIS\n",
      "There were 341 errors (2%) for mapper first_img_dm_pxl_frac_NEG_SKEW\n",
      "There were 341 errors (2%) for mapper first_img_dm_pxl_frac_POS_SKEW\n",
      "There were 341 errors (2%) for mapper all_imgs_dm_pxl_frac_MIN\n",
      "There were 341 errors (2%) for mapper all_imgs_dm_pxl_frac_MAX\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34331c7604fc4855a542f7583ec48fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Reading pet ids', max=3948, style=ProgressStyle(description_wâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There were 127 errors (3%) for mapper first_img_la\n",
      "There were 127 errors (3%) for mapper first_img_la_score_MIN\n",
      "There were 127 errors (3%) for mapper first_img_la_score_MAX\n",
      "There were 127 errors (3%) for mapper first_img_la_score_MEDIAN\n",
      "There were 127 errors (3%) for mapper first_img_la_score_MEAN\n",
      "There were 127 errors (3%) for mapper first_img_la_score_STD\n",
      "There were 127 errors (3%) for mapper first_img_la_score_RMS\n",
      "There were 127 errors (3%) for mapper first_img_la_score_KURTOSIS\n",
      "There were 127 errors (3%) for mapper first_img_la_score_NEG_SKEW\n",
      "There were 127 errors (3%) for mapper first_img_la_score_POS_SKEW\n",
      "There were 127 errors (3%) for mapper all_imgs_la_score_MIN\n",
      "There were 127 errors (3%) for mapper all_imgs_la_score_MAX\n",
      "There were 127 errors (3%) for mapper first_img_dm_score_MIN\n",
      "There were 127 errors (3%) for mapper first_img_dm_score_MAX\n",
      "There were 127 errors (3%) for mapper first_img_dm_score_MEDIAN\n",
      "There were 127 errors (3%) for mapper first_img_dm_score_MEAN\n",
      "There were 127 errors (3%) for mapper first_img_dm_score_STD\n",
      "There were 127 errors (3%) for mapper first_img_dm_score_RMS\n",
      "There were 127 errors (3%) for mapper first_img_dm_score_KURTOSIS\n",
      "There were 127 errors (3%) for mapper first_img_dm_score_NEG_SKEW\n",
      "There were 127 errors (3%) for mapper first_img_dm_score_POS_SKEW\n",
      "There were 127 errors (3%) for mapper all_imgs_dm_score_MIN\n",
      "There were 127 errors (3%) for mapper all_imgs_dm_score_MAX\n",
      "There were 127 errors (3%) for mapper first_img_dm_pxl_frac_MIN\n",
      "There were 127 errors (3%) for mapper first_img_dm_pxl_frac_MAX\n",
      "There were 127 errors (3%) for mapper first_img_dm_pxl_frac_MEDIAN\n",
      "There were 127 errors (3%) for mapper first_img_dm_pxl_frac_MEAN\n",
      "There were 127 errors (3%) for mapper first_img_dm_pxl_frac_STD\n",
      "There were 127 errors (3%) for mapper first_img_dm_pxl_frac_RMS\n",
      "There were 127 errors (3%) for mapper first_img_dm_pxl_frac_KURTOSIS\n",
      "There were 127 errors (3%) for mapper first_img_dm_pxl_frac_NEG_SKEW\n",
      "There were 127 errors (3%) for mapper first_img_dm_pxl_frac_POS_SKEW\n",
      "There were 127 errors (3%) for mapper all_imgs_dm_pxl_frac_MIN\n",
      "There were 127 errors (3%) for mapper all_imgs_dm_pxl_frac_MAX\n"
     ]
    }
   ],
   "source": [
    "train_test_data_assigner(\n",
    "    read_metadata(train_ids, PF_HOME / 'train_metadata'),\n",
    "    read_metadata(test_ids, PF_HOME / 'test_metadata'),\n",
    "    first_img_la=     m(lambda la, dm: dict(la[0]), default={}),\n",
    "    first_img_la_score_MIN=    m(lambda la, dm: np.min(la_vals(la[0])), default=DEFAULT, add_to=num_cols),\n",
    "    first_img_la_score_MAX=    m(lambda la, dm: np.max(la_vals(la[0])), default=DEFAULT, add_to=num_cols),\n",
    "    first_img_la_score_MEDIAN= m(lambda la, dm: np.median(la_vals(la[0])), default=DEFAULT, add_to=num_cols),\n",
    "    first_img_la_score_MEAN=   m(lambda la, dm: np.mean(la_vals(la[0])), default=DEFAULT, add_to=num_cols),\n",
    "    first_img_la_score_STD=    m(lambda la, dm: np.std(la_vals(la[0])), default=DEFAULT, add_to=num_cols),\n",
    "    first_img_la_score_RMS=    m(lambda la, dm: rms(la_vals(la[0])), default=DEFAULT, add_to=num_cols),\n",
    "    first_img_la_score_KURTOSIS=    m(lambda la, dm: scipy.stats.kurtosis(la_vals(la[0])), default=DEFAULT, add_to=num_cols),\n",
    "    first_img_la_score_NEG_SKEW=    m(lambda la, dm: neg_skew(la_vals(la[0])), default=DEFAULT, add_to=num_cols),\n",
    "    first_img_la_score_POS_SKEW=    m(lambda la, dm: pos_skew(la_vals(la[0])), default=DEFAULT, add_to=num_cols),\n",
    "    all_imgs_la=    m(lambda la, dm: merge_la(la), default={}),\n",
    "    all_imgs_la_score_MIN=    m(lambda la, dm: np.min(dict_vals(merge_la(la))), default=DEFAULT, add_to=num_cols),\n",
    "    all_imgs_la_score_MAX=    m(lambda la, dm: np.max(dict_vals(merge_la(la))), default=DEFAULT, add_to=num_cols),\n",
    "    all_imgs_la_score_MEDIAN= m(lambda la, dm: np.median(dict_vals(merge_la(la))), default=DEFAULT, add_to=num_cols),\n",
    "    all_imgs_la_score_MEAN=   m(lambda la, dm: np.mean(dict_vals(merge_la(la))), default=DEFAULT, add_to=num_cols),\n",
    "    all_imgs_la_score_STD=    m(lambda la, dm: np.std(dict_vals(merge_la(la))), default=DEFAULT, add_to=num_cols),\n",
    "    all_imgs_la_score_RMS=    m(lambda la, dm: rms(dict_vals(merge_la(la))), default=DEFAULT, add_to=num_cols),\n",
    "    all_imgs_la_score_KURTOSIS=    m(lambda la, dm: scipy.stats.kurtosis(dict_vals(merge_la(la))), default=DEFAULT, add_to=num_cols),\n",
    "    all_imgs_la_score_NEG_SKEW=    m(lambda la, dm: neg_skew(dict_vals(merge_la(la))), default=DEFAULT, add_to=num_cols),\n",
    "    all_imgs_la_score_POS_SKEW=    m(lambda la, dm: pos_skew(dict_vals(merge_la(la))), default=DEFAULT, add_to=num_cols),\n",
    "    first_img_dm_score_MIN=    m(lambda la, dm: np.min([i[3] for i in dm[0]]), default=DEFAULT, add_to=num_cols),\n",
    "    first_img_dm_score_MAX=    m(lambda la, dm: np.max([i[3] for i in dm[0]]), default=DEFAULT, add_to=num_cols),\n",
    "    first_img_dm_score_MEDIAN= m(lambda la, dm: np.median([i[3] for i in dm[0]]), default=DEFAULT, add_to=num_cols),\n",
    "    first_img_dm_score_MEAN=    m(lambda la, dm: np.mean([i[3] for i in dm[0]]), default=DEFAULT, add_to=num_cols),\n",
    "    first_img_dm_score_STD=    m(lambda la, dm: np.std([i[3] for i in dm[0]]), default=DEFAULT, add_to=num_cols),\n",
    "    first_img_dm_score_RMS=    m(lambda la, dm: rms([i[3] for i in dm[0]]), default=DEFAULT, add_to=num_cols),\n",
    "    first_img_dm_score_KURTOSIS=    m(lambda la, dm: scipy.stats.kurtosis([i[3] for i in dm[0]]), default=DEFAULT, add_to=num_cols),\n",
    "    first_img_dm_score_NEG_SKEW=    m(lambda la, dm: neg_skew([i[3] for i in dm[0]]), default=DEFAULT, add_to=num_cols),\n",
    "    first_img_dm_score_POS_SKEW=    m(lambda la, dm: pos_skew([i[3] for i in dm[0]]), default=DEFAULT, add_to=num_cols),\n",
    "    all_imgs_dm_score_MIN=    m(lambda la, dm: np.min([x[3] for xs in dm for x in xs]), default=DEFAULT, add_to=num_cols),\n",
    "    all_imgs_dm_score_MAX=    m(lambda la, dm: np.max([x[3] for xs in dm for x in xs]), default=DEFAULT, add_to=num_cols),\n",
    "    all_imgs_dm_score_MEDIAN=    m(lambda la, dm: np.median([x[3] for xs in dm for x in xs]), default=DEFAULT, add_to=num_cols),\n",
    "    all_imgs_dm_score_MEAN=    m(lambda la, dm: np.mean([x[3] for xs in dm for x in xs]), default=DEFAULT, add_to=num_cols),\n",
    "    all_imgs_dm_score_STD=    m(lambda la, dm: np.std([x[3] for xs in dm for x in xs]), default=DEFAULT, add_to=num_cols),\n",
    "    all_imgs_dm_score_RMS=    m(lambda la, dm: rms([x[3] for xs in dm for x in xs]), default=DEFAULT, add_to=num_cols),\n",
    "    all_imgs_dm_score_KURTOSIS=    m(lambda la, dm: scipy.stats.kurtosis([x[3] for xs in dm for x in xs]), default=DEFAULT, add_to=num_cols),\n",
    "    all_imgs_dm_score_NEG_SKEW=    m(lambda la, dm: neg_skew([x[3] for xs in dm for x in xs]), default=DEFAULT, add_to=num_cols),\n",
    "    all_imgs_dm_score_POS_SKEW=    m(lambda la, dm: pos_skew([x[3] for xs in dm for x in xs]), default=DEFAULT, add_to=num_cols),\n",
    "    first_img_dm_pxl_frac_MIN=    m(lambda la, dm: np.min([i[4] for i in dm[0]]), default=DEFAULT, add_to=num_cols),\n",
    "    first_img_dm_pxl_frac_MAX=    m(lambda la, dm: np.max([i[4] for i in dm[0]]), default=DEFAULT, add_to=num_cols),\n",
    "    first_img_dm_pxl_frac_MEDIAN=    m(lambda la, dm: np.median([i[4] for i in dm[0]]), default=DEFAULT, add_to=num_cols),\n",
    "    first_img_dm_pxl_frac_MEAN=    m(lambda la, dm: np.mean([i[4] for i in dm[0]]), default=DEFAULT, add_to=num_cols),\n",
    "    first_img_dm_pxl_frac_STD=    m(lambda la, dm: np.std([i[4] for i in dm[0]]), default=DEFAULT, add_to=num_cols),\n",
    "    first_img_dm_pxl_frac_RMS=    m(lambda la, dm: rms([i[4] for i in dm[0]]), default=DEFAULT, add_to=num_cols),\n",
    "    first_img_dm_pxl_frac_KURTOSIS=   m(lambda la, dm: scipy.stats.kurtosis([i[4] for i in dm[0]]), default=DEFAULT, add_to=num_cols),\n",
    "    first_img_dm_pxl_frac_NEG_SKEW=   m(lambda la, dm: neg_skew([i[4] for i in dm[0]]), default=DEFAULT, add_to=num_cols),\n",
    "    first_img_dm_pxl_frac_POS_SKEW=   m(lambda la, dm: pos_skew([i[4] for i in dm[0]]), default=DEFAULT, add_to=num_cols),\n",
    "    all_imgs_dm_pxl_frac_MIN=         m(lambda la, dm: np.min([x[4] for xs in dm for x in xs]), default=DEFAULT, add_to=num_cols),\n",
    "    all_imgs_dm_pxl_frac_MAX=         m(lambda la, dm: np.max([x[4] for xs in dm for x in xs]), default=DEFAULT, add_to=num_cols),\n",
    "    all_imgs_dm_pxl_frac_MEDIAN=      m(lambda la, dm: np.median([x[4] for xs in dm for x in xs]), default=DEFAULT, add_to=num_cols),\n",
    "    all_imgs_dm_pxl_frac_MEAN=        m(lambda la, dm: np.mean([x[4] for xs in dm for x in xs]), default=DEFAULT, add_to=num_cols),\n",
    "    all_imgs_dm_pxl_frac_STD=         m(lambda la, dm: np.std([x[4] for xs in dm for x in xs]), default=DEFAULT, add_to=num_cols),\n",
    "    all_imgs_dm_pxl_frac_RMS=         m(lambda la, dm: rms([x[4] for xs in dm for x in xs]), default=DEFAULT, add_to=num_cols),\n",
    "    all_imgs_dm_pxl_frac_KURTOSIS=    m(lambda la, dm: scipy.stats.kurtosis([x[4] for xs in dm for x in xs]), default=DEFAULT, add_to=num_cols),\n",
    "    all_imgs_dm_pxl_frac_NEG_SKEW=    m(lambda la, dm: neg_skew([x[4] for xs in dm for x in xs]), default=DEFAULT, add_to=num_cols),\n",
    "    all_imgs_dm_pxl_frac_POS_SKEW=    m(lambda la, dm: pos_skew([x[4] for xs in dm for x in xs]), default=DEFAULT, add_to=num_cols),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "ecbdc6dd2ce620495b0034cc8bc491caece90c96"
   },
   "outputs": [],
   "source": [
    "class TextFeatures(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self, prefix=''):\n",
    "        self._prefix = prefix\n",
    "    \n",
    "    def transform(self, texts, *_):\n",
    "        return [self._text_features(t) for t in texts]\n",
    "    \n",
    "    def _text_features(self, text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        token_sizes = list(map(len, tokens))\n",
    "        \n",
    "        return [\n",
    "            len(text),\n",
    "            len(tokens),\n",
    "            np.min(token_sizes) if token_sizes else DEFAULT,\n",
    "            np.max(token_sizes) if token_sizes else DEFAULT,\n",
    "            np.median(token_sizes) if token_sizes else DEFAULT,\n",
    "            np.mean(token_sizes) if token_sizes else DEFAULT,\n",
    "            np.std(token_sizes) if token_sizes else DEFAULT,\n",
    "            rms(token_sizes) if token_sizes else DEFAULT,\n",
    "            scipy.stats.kurtosis(token_sizes) if token_sizes else DEFAULT,\n",
    "            neg_skew(token_sizes) if token_sizes else DEFAULT,\n",
    "            pos_skew(token_sizes) if token_sizes else DEFAULT,\n",
    "        ]\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return [f'{self._prefix}{n}' for n in [\n",
    "            'text_len', 'tokens_num', 'token_size_MIN', 'token_size_MAX', 'token_size_MEDIAN', \n",
    "            'token_size_MEAN', 'token_size_STD', 'token_size_RMS', 'token_size_KURTOSIS',\n",
    "            'token_size_NEG_SKEW', 'token_size_POS_SKEW',\n",
    "        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "b5afd42cddcd90fe23e4d6c7e1c333112e6d51d8"
   },
   "outputs": [],
   "source": [
    "img_converters = [\n",
    "    lambda img: img.convert('RGB'),\n",
    "    lambda img: img.convert('HSV'),\n",
    "    lambda img: img.convert('L'),\n",
    "]\n",
    "\n",
    "stats_funcs = [\n",
    "    np.min,\n",
    "    np.max,\n",
    "    np.median,\n",
    "    np.mean,\n",
    "    np.std,\n",
    "    lambda xs, axis: np.sqrt(np.mean(xs**2, axis=axis)),\n",
    "    lambda xs, axis: scipy.stats.kurtosis(xs, axis=axis, fisher=False),\n",
    "    lambda xs, axis: np.abs(scipy.stats.skew(xs, axis=axis).clip(max=0)),\n",
    "    lambda xs, axis: scipy.stats.skew(xs, axis=axis).clip(min=0),\n",
    "]\n",
    "\n",
    "class ImgFeatures(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self, img_no, *img_directories):\n",
    "        self.image_path_per_pet_id = {}\n",
    "        for img_dir in tqdm(img_directories, desc='Reading image paths'):\n",
    "            self.image_path_per_pet_id.update(\n",
    "                self._get_image_paths(img_dir, img_no)\n",
    "            )\n",
    "        self.feature_prefix = f'image{img_no}_'\n",
    "        self.channels_num = 7\n",
    "    \n",
    "    def transform(self, pet_ids, *_):\n",
    "        with Pool(processes=None) as pool:\n",
    "            return [img_f for img_f in pool.imap(\n",
    "                self._extract_img_features,\n",
    "                tqdm(pet_ids, desc='Transforming images...'),\n",
    "                chunksize=2**8\n",
    "            )]\n",
    "    \n",
    "    def _extract_img_features(self, pet_id):\n",
    "        img_path = self.image_path_per_pet_id.get(pet_id, None)\n",
    "        img = pil.Image.open(img_path) if img_path else None\n",
    "        \n",
    "        img_features = []\n",
    "        img_features.extend(self._compute_img_size_features(img))\n",
    "        img_features.extend(self._compute_img_color_features(img))\n",
    "        img_features.extend(self._compute_noisiness_estimate(img))\n",
    "        \n",
    "        return img_features\n",
    "    \n",
    "    def _compute_img_size_features(self, img):\n",
    "        return [\n",
    "            img.width if img is not None else DEFAULT,\n",
    "            img.height if img is not None else DEFAULT,\n",
    "            img.width / img.height if img is not None else DEFAULT,\n",
    "            img.width * img.height if img is not None else DEFAULT,\n",
    "        ]\n",
    "    \n",
    "    def _compute_img_color_features(self, img):\n",
    "        def get_default():\n",
    "            return [DEFAULT] * self.channels_num * len(stats_funcs)\n",
    "        \n",
    "        if img is None:\n",
    "            return get_default()\n",
    "        \n",
    "        try:\n",
    "            converted_imgs = [np.array(conv(img)).reshape(np.product(img.size), -1)\n",
    "                              for conv in img_converters]\n",
    "            img_colors = np.hstack(converted_imgs)\n",
    "\n",
    "            img_color_features = []\n",
    "            for func in stats_funcs:\n",
    "                img_color_features.extend(\n",
    "                    func(img_colors, axis=0)\n",
    "                )\n",
    "            return img_color_features\n",
    "        except ValueError:\n",
    "            return get_default()\n",
    "    \n",
    "    def _compute_noisiness_estimate(self, img):\n",
    "        return [\n",
    "            estimate_sigma(np.array(img.convert('L'))) if img is not None else DEFAULT\n",
    "        ]\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        f_names = [\n",
    "            'width', 'height', 'aspect_ratio', 'resolution'\n",
    "        ]\n",
    "        for s in ['min', 'max', 'median', 'mean', 'std', 'rms', 'kurtosis', 'neg_skew', 'pos_skew']:\n",
    "            for c in ['r', 'g', 'b', 'h', 's', 'v', 'grey']:\n",
    "                f_names.append(f'color_{s}_{c}')\n",
    "        f_names.append('noisiness')\n",
    "        \n",
    "        return [f'{self.feature_prefix}{n}' for n in f_names]\n",
    "    \n",
    "    def _get_image_paths(self, image_dir, img_no):\n",
    "        def get_image_idx(p):\n",
    "            return int(str(p).rsplit('-', 1)[1].split('.')[0])\n",
    "        def get_pet_id(p):\n",
    "            return str(p).rsplit('/', 1)[1].split('-')[0]\n",
    "        \n",
    "        return {get_pet_id(p): p for p in image_dir.iterdir() if get_image_idx(p) == img_no}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "f66049672bb340e3e08f857e683670824005a245"
   },
   "outputs": [],
   "source": [
    "class RescuerFeatures(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self, num_cols, cat_cols, rescued_id_col='RescuerID'):\n",
    "        self._num_cols = num_cols\n",
    "        self._cat_cols = cat_cols\n",
    "        self._rescued_id_col = rescued_id_col\n",
    "        \n",
    "        self.columns = None\n",
    "    \n",
    "    def transform(self, df, *_):\n",
    "        result = self._extract_rescuer_features(df)\n",
    "        for c in self.columns:\n",
    "            if c not in result.columns:\n",
    "                result[c] = np.full(df.shape[0], DEFAULT)\n",
    "        return result[self.columns]\n",
    "    \n",
    "    def fit(self, df, *_):\n",
    "        df = self._extract_rescuer_features(df)\n",
    "        self.columns = df.columns\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, df, *_):\n",
    "        df = self._extract_rescuer_features(df)\n",
    "        self.columns = df.columns\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        if self.columns is None:\n",
    "            raise ValueError('RescuerFeatures was not fitted')\n",
    "        return self.columns\n",
    "    \n",
    "    def _extract_rescuer_features(self, df):\n",
    "        def num_col_features(values):\n",
    "            values = list(values)\n",
    "            return dict(\n",
    "                min=np.min(values) if values else DEFAULT,\n",
    "                max=np.max(values) if values else DEFAULT,\n",
    "                median=np.median(values) if values else DEFAULT,\n",
    "                mean=np.mean(values) if values else DEFAULT,\n",
    "                std=np.std(values) if values else DEFAULT,\n",
    "                rms=rms(values) if values else DEFAULT,\n",
    "                kurtosis=scipy.stats.kurtosis(values) if values else DEFAULT,\n",
    "                neg_skew=neg_skew(values) if values else DEFAULT,\n",
    "                pos_skew=pos_skew(values) if values else DEFAULT,\n",
    "            )\n",
    "        \n",
    "        def cat_col_features(values):\n",
    "            return Counter(values)\n",
    "        \n",
    "        agg_operations = defaultdict(list)\n",
    "        for c in self._num_cols:\n",
    "            agg_operations[c].append(num_col_features)\n",
    "        for c in self._cat_cols:\n",
    "            agg_operations[c].append(cat_col_features)\n",
    "        agg_operations[self._rescued_id_col].append('size')\n",
    "            \n",
    "        by_rescuer_id = df.groupby(self._rescued_id_col).agg(agg_operations)\n",
    "        for outer_c, inner_c in by_rescuer_id.columns:\n",
    "            if inner_c in {'num_col_features', 'cat_col_features'}:\n",
    "                by_rescuer_id = explode(by_rescuer_id, (outer_c, inner_c), DEFAULT)\n",
    "        by_rescuer_id.columns = [c if isinstance(c, str) else '_'.join(c) \n",
    "                                 for c in by_rescuer_id.columns]\n",
    "        \n",
    "        result = df[[self._rescued_id_col]].merge(\n",
    "            by_rescuer_id, how='outer', left_on=self._rescued_id_col, right_index=True\n",
    "        ).sort_index()\n",
    "        result.drop(self._rescued_id_col, axis=1, inplace=True)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_functions = [\n",
    "#     np.min,\n",
    "#     np.max,\n",
    "    pd.DataFrame.median,\n",
    "    pd.DataFrame.mean,\n",
    "    pd.DataFrame.std,\n",
    "    pd.DataFrame.kurt,\n",
    "    pd.DataFrame.skew,\n",
    "]\n",
    "\n",
    "\n",
    "class TargetEncoding(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self, target_col, feature_cols):\n",
    "        self._target_col = target_col\n",
    "        self._feature_cols = feature_cols\n",
    "        \n",
    "        self._out_f_names = None\n",
    "        \n",
    "        self.target_coding_map = None\n",
    "    \n",
    "    def transform(self, x, *_):\n",
    "        result = pd.DataFrame(index=x.index)\n",
    "        for f_col in tqdm(self._feature_cols, desc='Transform in target encoding'):\n",
    "            mapping, default_vals = self.target_coding_map[f_col]\n",
    "            \n",
    "            df_to_append = mapping.reindex(x[f_col])\n",
    "            df_to_append.fillna(value=default_vals, inplace=True)\n",
    "            df_to_append.columns = [f'{f_col}_{c.upper()}' for c in df_to_append.columns]\n",
    "            df_to_append.index = x.index\n",
    "            \n",
    "            result = pd.concat([result, df_to_append], axis=1)\n",
    "        return result\n",
    "    \n",
    "    def fit(self, x, y, *_):\n",
    "        raise ValueError('Fit cannot be used')\n",
    "    \n",
    "    def fit_transform(self, x, y, *_):\n",
    "        target_coding_map = {}\n",
    "        result = pd.DataFrame(index=x.index)\n",
    "        for f_col in tqdm(self._feature_cols, desc='Fit-transform in target encoding'):\n",
    "            df_to_append, mapping, default_vals = self._target_coding(x, f_col, self._target_col)\n",
    "            df_to_append.columns = [f'{f_col}_{c.upper()}' for c in df_to_append.columns]\n",
    "            result = pd.concat([result, df_to_append], axis=1)\n",
    "            target_coding_map[f_col] = (mapping, default_vals)\n",
    "        self.target_coding_map = target_coding_map\n",
    "        \n",
    "        self._out_f_names = result.columns\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self._out_f_names\n",
    "        \n",
    "    def _target_coding(self, data, feature, target='y'):\n",
    "        def concat_dfs(dfs):\n",
    "            out_df = pd.concat(dfs)\n",
    "            return out_df.groupby(out_df.index).mean()\n",
    "        \n",
    "        uniq_vals = list(data[feature].unique())\n",
    "\n",
    "        oof_default_vals = data[target].agg(agg_functions)\n",
    "        target_coded = pd.DataFrame(dict(oof_default_vals), index=data.index)\n",
    "        \n",
    "        oof_mean_cv_to_concat = []\n",
    "        for infold, oof in splits:\n",
    "            kf_inner = KFold(n_splits=10, shuffle=True)\n",
    "            inner_oof_mean_cv_to_concat = []\n",
    "            \n",
    "            try:\n",
    "                inner_splits = list(kf_inner.split(data.iloc[infold]))\n",
    "            except ValueError:\n",
    "                inner_oof_mean_cv = pd.DataFrame(\n",
    "                    dict(oof_default_vals),\n",
    "                    index=uniq_vals\n",
    "                )\n",
    "            else:\n",
    "                oof_default_infold_mean = data.iloc[infold][target].agg(agg_functions)\n",
    "                for infold_inner, oof_inner in inner_splits:\n",
    "                    oof_mean = pd.DataFrame(\n",
    "                        data.iloc[infold_inner].groupby(by=feature)[target].agg(agg_functions),\n",
    "                        index=uniq_vals\n",
    "                    )\n",
    "                    oof_mean.fillna(value=oof_default_infold_mean, inplace=True)\n",
    "                    inner_oof_mean_cv_to_concat.append(oof_mean)\n",
    "\n",
    "                inner_oof_mean_cv = concat_dfs(inner_oof_mean_cv_to_concat)\n",
    "                inner_oof_mean_cv.fillna(value=oof_default_vals, inplace=True)\n",
    "\n",
    "            oof_mean_cv_to_concat.append(inner_oof_mean_cv)\n",
    "\n",
    "            to_assign = inner_oof_mean_cv.loc[data.iloc[oof][feature]]\n",
    "            to_assign.index = oof\n",
    "            target_coded.iloc[oof] = to_assign\n",
    "        \n",
    "        oof_mean_cv = concat_dfs(oof_mean_cv_to_concat)\n",
    "        oof_mean_cv.fillna(value=oof_default_vals, inplace=True)\n",
    "\n",
    "        return target_coded, oof_mean_cv, oof_default_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "8d8a59337b52eda2608556d1b43440235cdb20ca",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e28d7cdfa843459bc36a1e49895516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Reading image paths', max=2, style=ProgressStyle(description_â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfbd54f5243844c0b1273e63017305c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Reading image paths', max=2, style=ProgressStyle(description_â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9292134e92b5496aba0747f8a1521aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Reading image paths', max=2, style=ProgressStyle(description_â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0728cba972164bc3b48dd4a0844f65b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fit-transform in target encoding', max=11, style=ProgressStylâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in sqrt\n",
      "/home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages/numpy/lib/function_base.py:3400: RuntimeWarning: Invalid value encountered in median\n",
      "  r = func(a, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f42424277e4e99a7b643f0eac3df4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Transforming images...', max=14993, style=ProgressStyle(descrâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612608a925cd49438ce4bddea54264b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Transforming images...', max=14993, style=ProgressStyle(descrâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86510a625c844ffaa487c09d06513d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Transforming images...', max=14993, style=ProgressStyle(descrâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0cb188aa5a74060abbb3b001bf8d52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Transform in target encoding', max=11, style=ProgressStyle(deâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in sqrt\n",
      "/home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages/numpy/lib/function_base.py:3400: RuntimeWarning: Invalid value encountered in median\n",
      "  r = func(a, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331d66cabd1f4aed8f672112907aec0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Transforming images...', max=3948, style=ProgressStyle(descriâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d7bcd3f65c429cafff140b0eb2efda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Transforming images...', max=3948, style=ProgressStyle(descriâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1abd019c24e747e7a6f321d46d5e01aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Transforming images...', max=3948, style=ProgressStyle(descriâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_f, test_f, p, f_names = apply_pipeline(make_pipeline(\n",
    "    FeatureUnion([\n",
    "        ('desc_tokenizing', make_pipeline(\n",
    "            Mapper(lambda x: list(x.Description.fillna(''))),\n",
    "            CountVectorizer(\n",
    "                analyzer='word',\n",
    "                ngram_range=(1, 3),\n",
    "                dtype=np.float32\n",
    "            ),\n",
    "            FeatureUnion([\n",
    "                ('svd', Features(TruncatedSVD(n_components=4, random_state=SEED))),\n",
    "                ('nmf', Features(NMF(n_components=4, random_state=SEED))),\n",
    "            ])\n",
    "        )),\n",
    "        ('desc_text_fs', make_pipeline(\n",
    "            Mapper(lambda x: list(x.Description.fillna(''))),\n",
    "            TextFeatures('desc_')\n",
    "        )),\n",
    "        ('name_text_fs', make_pipeline(\n",
    "            Mapper(lambda x: list(x.Name.fillna(''))),\n",
    "            TextFeatures('name_')\n",
    "        )),\n",
    "        ('num_cols', Cols(num_cols)),\n",
    "        ('cat_cols', CatCols(cat_cols, use_label_encoder=True)),\n",
    "        ('target_encoded', TargetEncoding('AdoptionSpeed', cat_cols)),\n",
    "        ('rescuer_fs', make_pipeline(\n",
    "            RescuerFeatures(num_cols=num_cols, cat_cols=[]),\n",
    "#             FeatureUnion([\n",
    "#                 ('svd', Features(TruncatedSVD(n_components=120, random_state=SEED))),\n",
    "#                 ('nmf', Features(NMF(n_components=4, random_state=SEED))),\n",
    "#             ])\n",
    "        )),\n",
    "        ('entity_sents', make_pipeline(\n",
    "            Mapper(lambda x: x.entity_sents),\n",
    "            DictVectorizer(),\n",
    "            FeatureUnion([\n",
    "                ('svd', Features(TruncatedSVD(n_components=4, random_state=SEED))),\n",
    "                ('nmf', Features(NMF(n_components=4, random_state=SEED)))\n",
    "            ])\n",
    "        )),\n",
    "        ('img_annotations_first_img', make_pipeline(\n",
    "            Mapper(lambda x: x.first_img_la),\n",
    "            DictVectorizer(),\n",
    "#             FeatureUnion([\n",
    "#                 ('svd', Features(TruncatedSVD(n_components=8, random_state=SEED))),\n",
    "#                 ('nmf', Features(NMF(n_components=8, random_state=SEED)))\n",
    "#             ])\n",
    "        )),\n",
    "        ('img_annotations_all_imgs', make_pipeline(\n",
    "            Mapper(lambda x: x.all_imgs_la),\n",
    "            DictVectorizer(),\n",
    "#             FeatureUnion([\n",
    "#                 ('svd', Features(TruncatedSVD(n_components=8, random_state=SEED))),\n",
    "#                 ('nmf', Features(NMF(n_components=8, random_state=SEED)))\n",
    "#             ])\n",
    "        )),\n",
    "        ('img1_fs', make_pipeline(\n",
    "            Mapper(lambda x: x.PetID),\n",
    "            ImgFeatures(1, PF_HOME / 'train_images', PF_HOME / 'test_images'),\n",
    "#             FeatureUnion([\n",
    "#                 ('svd', Features(TruncatedSVD(n_components=4, random_state=SEED))),\n",
    "#                 ('nmf', Features(NMF(n_components=4, random_state=SEED))),\n",
    "#             ])\n",
    "        )),\n",
    "        ('img2_fs', make_pipeline(\n",
    "            Mapper(lambda x: x.PetID),\n",
    "            ImgFeatures(2, PF_HOME / 'train_images', PF_HOME / 'test_images'),\n",
    "#             FeatureUnion([\n",
    "#                 ('svd', Features(TruncatedSVD(n_components=4, random_state=SEED))),\n",
    "#                 ('nmf', Features(NMF(n_components=4, random_state=SEED))),\n",
    "#             ])\n",
    "        )),\n",
    "        ('img3_fs', make_pipeline(\n",
    "            Mapper(lambda x: x.PetID),\n",
    "            ImgFeatures(3, PF_HOME / 'train_images', PF_HOME / 'test_images'),\n",
    "#             FeatureUnion([\n",
    "#                 ('svd', Features(TruncatedSVD(n_components=4, random_state=SEED))),\n",
    "#                 ('nmf', Features(NMF(n_components=4, random_state=SEED))),\n",
    "#             ])\n",
    "        )),\n",
    "    ])\n",
    "), train_x, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('features_more.pickle', 'wb') as handle:\n",
    "    pickle.dump(\n",
    "        (train_y, train_f, train_ids, test_f, test_ids, f_names), \n",
    "        handle\n",
    "    )\n",
    "\n",
    "# with open('features.pickle', 'rb') as handle:\n",
    "#     train_y, train_f, train_ids, test_f, test_ids, f_names = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># of examples</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fold 0: dev_y</th>\n",
       "      <td>79.96%</td>\n",
       "      <td>2.73%</td>\n",
       "      <td>20.61%</td>\n",
       "      <td>26.93%</td>\n",
       "      <td>21.74%</td>\n",
       "      <td>27.99%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold 0: val_y</th>\n",
       "      <td>20.04%</td>\n",
       "      <td>2.76%</td>\n",
       "      <td>20.60%</td>\n",
       "      <td>26.92%</td>\n",
       "      <td>21.73%</td>\n",
       "      <td>27.99%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              # of examples      0       1       2       3       4\n",
       "fold 0: dev_y        79.96%  2.73%  20.61%  26.93%  21.74%  27.99%\n",
       "fold 0: val_y        20.04%  2.76%  20.60%  26.92%  21.73%  27.99%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11988, 5889)\n",
      "(3005, 5889)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages/lightgbm/basic.py:1186: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages/lightgbm/basic.py:752: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[500]\ttraining's rmse: 0.758252\tvalid_1's rmse: 1.04586\n",
      "[1000]\ttraining's rmse: 0.624215\tvalid_1's rmse: 1.04252\n",
      "[1500]\ttraining's rmse: 0.527834\tvalid_1's rmse: 1.04295\n",
      "[2000]\ttraining's rmse: 0.447743\tvalid_1's rmse: 1.0434\n",
      "Early stopping, best iteration is:\n",
      "[1200]\ttraining's rmse: 0.583235\tvalid_1's rmse: 1.04227\n",
      "Trained coefficients are [1.645762195896331, 2.1238246007235375, 2.473764714648191, 2.79782065933623]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># of examples</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fold 1: dev_y</th>\n",
       "      <td>80.03%</td>\n",
       "      <td>2.74%</td>\n",
       "      <td>20.61%</td>\n",
       "      <td>26.93%</td>\n",
       "      <td>21.74%</td>\n",
       "      <td>27.99%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold 1: val_y</th>\n",
       "      <td>19.97%</td>\n",
       "      <td>2.71%</td>\n",
       "      <td>20.61%</td>\n",
       "      <td>26.92%</td>\n",
       "      <td>21.74%</td>\n",
       "      <td>28.02%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              # of examples      0       1       2       3       4\n",
       "fold 1: dev_y        80.03%  2.74%  20.61%  26.93%  21.74%  27.99%\n",
       "fold 1: val_y        19.97%  2.71%  20.61%  26.92%  21.74%  28.02%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11999, 5889)\n",
      "(2994, 5889)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages/lightgbm/basic.py:1186: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages/lightgbm/basic.py:752: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[500]\ttraining's rmse: 0.751148\tvalid_1's rmse: 1.0451\n",
      "[1000]\ttraining's rmse: 0.620611\tvalid_1's rmse: 1.04286\n",
      "[1500]\ttraining's rmse: 0.52076\tvalid_1's rmse: 1.04374\n",
      "Early stopping, best iteration is:\n",
      "[785]\ttraining's rmse: 0.670482\tvalid_1's rmse: 1.04255\n",
      "Trained coefficients are [1.599956189478843, 2.113088009016818, 2.4970462215288878, 2.8388433716533514]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># of examples</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fold 2: dev_y</th>\n",
       "      <td>80.03%</td>\n",
       "      <td>2.74%</td>\n",
       "      <td>20.61%</td>\n",
       "      <td>26.93%</td>\n",
       "      <td>21.74%</td>\n",
       "      <td>27.99%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold 2: val_y</th>\n",
       "      <td>19.97%</td>\n",
       "      <td>2.71%</td>\n",
       "      <td>20.61%</td>\n",
       "      <td>26.92%</td>\n",
       "      <td>21.74%</td>\n",
       "      <td>28.02%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              # of examples      0       1       2       3       4\n",
       "fold 2: dev_y        80.03%  2.74%  20.61%  26.93%  21.74%  27.99%\n",
       "fold 2: val_y        19.97%  2.71%  20.61%  26.92%  21.74%  28.02%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11999, 5889)\n",
      "(2994, 5889)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages/lightgbm/basic.py:1186: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages/lightgbm/basic.py:752: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[500]\ttraining's rmse: 0.754441\tvalid_1's rmse: 1.04392\n",
      "[1000]\ttraining's rmse: 0.631088\tvalid_1's rmse: 1.04161\n",
      "[1500]\ttraining's rmse: 0.533751\tvalid_1's rmse: 1.04279\n",
      "Early stopping, best iteration is:\n",
      "[932]\ttraining's rmse: 0.645605\tvalid_1's rmse: 1.04152\n",
      "Trained coefficients are [1.4329425767371422, 2.0359667723097226, 2.4444603850253066, 2.80554221372973]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># of examples</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fold 3: dev_y</th>\n",
       "      <td>79.96%</td>\n",
       "      <td>2.73%</td>\n",
       "      <td>20.61%</td>\n",
       "      <td>26.92%</td>\n",
       "      <td>21.74%</td>\n",
       "      <td>28.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold 3: val_y</th>\n",
       "      <td>20.04%</td>\n",
       "      <td>2.76%</td>\n",
       "      <td>20.61%</td>\n",
       "      <td>26.93%</td>\n",
       "      <td>21.74%</td>\n",
       "      <td>27.96%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              # of examples      0       1       2       3       4\n",
       "fold 3: dev_y        79.96%  2.73%  20.61%  26.92%  21.74%  28.00%\n",
       "fold 3: val_y        20.04%  2.76%  20.61%  26.93%  21.74%  27.96%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11989, 5889)\n",
      "(3004, 5889)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages/lightgbm/basic.py:1186: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages/lightgbm/basic.py:752: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[500]\ttraining's rmse: 0.75046\tvalid_1's rmse: 1.05221\n",
      "[1000]\ttraining's rmse: 0.62114\tvalid_1's rmse: 1.05088\n",
      "[1500]\ttraining's rmse: 0.524119\tvalid_1's rmse: 1.05187\n",
      "Early stopping, best iteration is:\n",
      "[743]\ttraining's rmse: 0.68262\tvalid_1's rmse: 1.05051\n",
      "Trained coefficients are [1.6499342912413941, 2.1062513766889577, 2.4339687416329117, 2.7775147050215936]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># of examples</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fold 4: dev_y</th>\n",
       "      <td>80.02%</td>\n",
       "      <td>2.73%</td>\n",
       "      <td>20.61%</td>\n",
       "      <td>26.92%</td>\n",
       "      <td>21.74%</td>\n",
       "      <td>28.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold 4: val_y</th>\n",
       "      <td>19.98%</td>\n",
       "      <td>2.74%</td>\n",
       "      <td>20.63%</td>\n",
       "      <td>26.94%</td>\n",
       "      <td>21.73%</td>\n",
       "      <td>27.97%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              # of examples      0       1       2       3       4\n",
       "fold 4: dev_y        80.02%  2.73%  20.61%  26.92%  21.74%  28.00%\n",
       "fold 4: val_y        19.98%  2.74%  20.63%  26.94%  21.73%  27.97%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11997, 5889)\n",
      "(2996, 5889)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages/lightgbm/basic.py:1186: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages/lightgbm/basic.py:752: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[500]\ttraining's rmse: 0.751779\tvalid_1's rmse: 1.03636\n",
      "[1000]\ttraining's rmse: 0.625856\tvalid_1's rmse: 1.03424\n",
      "[1500]\ttraining's rmse: 0.531968\tvalid_1's rmse: 1.03473\n",
      "Early stopping, best iteration is:\n",
      "[877]\ttraining's rmse: 0.651726\tvalid_1's rmse: 1.03346\n",
      "Trained coefficients are [1.682566753041646, 2.169933460456557, 2.5743130378861356, 2.9125955771917935]\n",
      "Average coefficients taken from models are [1.6022324  2.10981284 2.48471062 2.82646331]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dev cv rmse</th>\n",
       "      <th>val cv rmse</th>\n",
       "      <th>dev cv qwk</th>\n",
       "      <th>val cv qwk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lgbm - fold 0</th>\n",
       "      <td>0.583</td>\n",
       "      <td>1.042</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbm - fold 1</th>\n",
       "      <td>0.670</td>\n",
       "      <td>1.043</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbm - fold 2</th>\n",
       "      <td>0.646</td>\n",
       "      <td>1.042</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbm - fold 3</th>\n",
       "      <td>0.683</td>\n",
       "      <td>1.051</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbm - fold 4</th>\n",
       "      <td>0.652</td>\n",
       "      <td>1.033</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              dev cv rmse val cv rmse dev cv qwk val cv qwk\n",
       "lgbm - fold 0       0.583       1.042      0.852      0.447\n",
       "lgbm - fold 1       0.670       1.043      0.831      0.457\n",
       "lgbm - fold 2       0.646       1.042      0.847      0.450\n",
       "lgbm - fold 3       0.683       1.051      0.817      0.425\n",
       "lgbm - fold 4       0.652       1.033      0.829      0.473"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>dev cv rmse (avg)</th>\n",
       "      <th>dev cv rmse (conf int)</th>\n",
       "      <th>val cv rmse (avg)</th>\n",
       "      <th>val cv rmse (conf int)</th>\n",
       "      <th>dev cv qwk (avg)</th>\n",
       "      <th>dev cv qwk (conf int)</th>\n",
       "      <th>val cv qwk (avg)</th>\n",
       "      <th>val cv qwk (conf int)</th>\n",
       "      <th>train set qwk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lgbm</th>\n",
       "      <td></td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.067</td>\n",
       "      <td>1.042</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     params dev cv rmse (avg) dev cv rmse (conf int) val cv rmse (avg)  \\\n",
       "lgbm                    0.647                  0.067             1.042   \n",
       "\n",
       "     val cv rmse (conf int) dev cv qwk (avg) dev cv qwk (conf int)  \\\n",
       "lgbm                  0.011            0.835                 0.025   \n",
       "\n",
       "     val cv qwk (avg) val cv qwk (conf int) train set qwk  \n",
       "lgbm            0.451                 0.030         0.451  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_y</th>\n",
       "      <td>2.73%</td>\n",
       "      <td>20.61%</td>\n",
       "      <td>26.93%</td>\n",
       "      <td>21.74%</td>\n",
       "      <td>27.99%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_train_y</th>\n",
       "      <td>2.93%</td>\n",
       "      <td>20.30%</td>\n",
       "      <td>26.96%</td>\n",
       "      <td>21.63%</td>\n",
       "      <td>28.19%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_test_y</th>\n",
       "      <td>2.08%</td>\n",
       "      <td>22.85%</td>\n",
       "      <td>25.35%</td>\n",
       "      <td>21.53%</td>\n",
       "      <td>28.19%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0       1       2       3       4\n",
       "train_y       2.73%  20.61%  26.93%  21.74%  27.99%\n",
       "pred_train_y  2.93%  20.30%  26.96%  21.63%  28.19%\n",
       "pred_test_y   2.08%  22.85%  25.35%  21.53%  28.19%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to path `predictions/lgbm-2.csv`\n",
      "Saving successfull\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f0dd7065a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {\n",
    "    'application': 'regression',\n",
    "    'boosting': 'gbdt',\n",
    "    'metric': 'rmse',\n",
    "    'num_leaves': 280,\n",
    "    'max_depth': 11,\n",
    "    'learning_rate': 0.01,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'feature_fraction': 0.5,\n",
    "    'min_split_gain': 0.02,\n",
    "    'min_child_samples': 150,\n",
    "    'min_child_weight': 0.02,\n",
    "    'lambda_l2': 0.0475,\n",
    "    'verbosity': -1,\n",
    "    'data_random_seed': 17,\n",
    "    'early_stop': 1000,\n",
    "    'verbose_eval': 500,\n",
    "    'num_rounds': 20000,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "results = run_cv_model(\n",
    "    'lgbm', \n",
    "    train_y,\n",
    "#     replace_missing_with_nan(train_f), train_ids, \n",
    "#     replace_missing_with_nan(test_f), test_ids, \n",
    "    train_f, train_ids, \n",
    "    test_f, test_ids, \n",
    "    lambda: LgbmPredictor(params, f_names), lambda: DistributionQwkPredictor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_lgbm_importance(results, f_names, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show_random_example(results['pred_train_y'], actual_label=4, pred_label=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
