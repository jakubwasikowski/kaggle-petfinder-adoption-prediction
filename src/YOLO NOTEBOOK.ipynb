{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS_LOCAL = True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import json\n",
    "import random\n",
    "import colorsys\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from math import sqrt\n",
    "from itertools import chain\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "\n",
    "from IPython.display import display, Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "from sklearn.pipeline import make_pipeline, make_union, Pipeline, FeatureUnion\n",
    "from sklearn.metrics import confusion_matrix as sk_cmatrix\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder\n",
    "from ml_metrics import quadratic_weighted_kappa, rmse\n",
    "\n",
    "DATA_HOME = Path('../input')\n",
    "\n",
    "IS_LOCAL = Path('IS_LOCAL').exists()\n",
    "LABEL_NUM = 5\n",
    "\n",
    "DEFAULT = 0\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(f'IS_LOCAL = {IS_LOCAL}')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "SEED = 7\n",
    "np.random.seed(SEED)\n",
    "CATEGORY_SUFFIX = '@c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ml_metrics in /home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages (0.1.4)\n",
      "Requirement already satisfied: nltk in /home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages (3.4)\n",
      "Collecting torch==1.0.1.post2 from https://download.pytorch.org/whl/cpu/torch-1.0.1.post2-cp36-cp36m-linux_x86_64.whl\n",
      "\u001b[?25l  Downloading https://download.pytorch.org/whl/cpu/torch-1.0.1.post2-cp36-cp36m-linux_x86_64.whl (67.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 67.1MB 768kB/s ta 0:00:0111\n",
      "\u001b[?25hCollecting torchvision\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/01/03fd7e503c16b3dc262483e5555ad40974ab5da8b9879e164b56c1f4ef6f/torchvision-0.2.2.post3-py2.py3-none-any.whl (64kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 1.1MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages (from ml_metrics) (1.16.0)\n",
      "Requirement already satisfied: pandas in /home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages (from ml_metrics) (0.23.4)\n",
      "Requirement already satisfied: six in /home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in /home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages (from nltk) (3.4.0.3)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages (from torchvision) (5.4.1)\n",
      "Requirement already satisfied: pytz>=2011k in /home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages (from pandas->ml_metrics) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /home/kuba/Competitions/pet-finder-adoption-prediction/env/lib/python3.6/site-packages (from pandas->ml_metrics) (2.7.5)\n",
      "Installing collected packages: torch, torchvision\n",
      "Successfully installed torch-1.0.1.post2 torchvision-0.2.2.post3\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "TORCH=\"https://download.pytorch.org/whl/cpu/torch-1.0.1.post2-cp36-cp36m-linux_x86_64.whl\"\n",
    "!pip install ml_metrics nltk $TORCH torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_group_k_fold_stratified(X, y, groups, k=5):\n",
    "    y_counts_per_group = defaultdict(lambda: np.zeros(LABEL_NUM))\n",
    "    y_distr = Counter()\n",
    "    for label, g in zip(y, groups):\n",
    "        y_counts_per_group[g][label] += 1\n",
    "        y_distr[label] += 1\n",
    "\n",
    "    y_counts_per_fold = defaultdict(lambda: np.zeros(LABEL_NUM))\n",
    "    groups_per_fold = defaultdict(set)\n",
    "\n",
    "    def eval_y_counts_per_fold(y_counts, fold):\n",
    "        y_counts_per_fold[fold] += y_counts\n",
    "        std_per_label = []\n",
    "        for label in range(LABEL_NUM):\n",
    "            label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(k)])\n",
    "            std_per_label.append(label_std)\n",
    "        y_counts_per_fold[fold] -= y_counts\n",
    "        return np.mean(std_per_label)\n",
    "\n",
    "    for g, y_counts in sorted(y_counts_per_group.items(), key=lambda x: -np.std(x[1])):\n",
    "        best_fold = None\n",
    "        min_eval = None\n",
    "        for i in range(k):\n",
    "            fold_eval = eval_y_counts_per_fold(y_counts, i)\n",
    "            if min_eval is None or fold_eval < min_eval:\n",
    "                min_eval = fold_eval\n",
    "                best_fold = i\n",
    "        y_counts_per_fold[best_fold] += y_counts\n",
    "        groups_per_fold[best_fold].add(g)\n",
    "\n",
    "    all_groups = set(groups)\n",
    "    for i in range(k):\n",
    "        train_groups = all_groups - groups_per_fold[i]\n",
    "        test_groups = groups_per_fold[i]\n",
    "\n",
    "        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n",
    "        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n",
    "\n",
    "        yield train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def by_indices(data, indices):\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            return data.iloc[indices]\n",
    "        if isinstance(data, list):\n",
    "            return [data[i] for i in indices]\n",
    "        return data[indices]     \n",
    "\n",
    "def run_cv_model(\n",
    "    name, \n",
    "    train_y, train_rescue_ids,\n",
    "    train_x, train_ids,\n",
    "    test_x, test_ids, \n",
    "    reg_model_factory, qwk_model_factory, k=5\n",
    "):\n",
    "    def format_stats(mean: float, std: float) -> str:\n",
    "        return f'{mean:.3f} ± {std:.3f}'\n",
    "\n",
    "    def save_predictions(pred_y):\n",
    "        if IS_LOCAL:\n",
    "            preds_dir = Path('predictions')\n",
    "            preds_path = preds_dir / f'{name}.csv'\n",
    "            checks_num = test_ids\n",
    "            while preds_path.exists():\n",
    "                checks_num += 1\n",
    "                new_file_name = f'{name}-{checks_num}.csv'\n",
    "                preds_path = preds_dir / new_file_name\n",
    "            print(f'Saving to path `{preds_path}`')\n",
    "        else:\n",
    "            preds_path = 'submission.csv'\n",
    "\n",
    "        submission = pd.DataFrame({'PetID': test_ids, 'AdoptionSpeed': pred_y.astype(np.int32)})\n",
    "        submission.to_csv(preds_path, index=False)\n",
    "    \n",
    "    def get_formatted_cv_evals(dev_rmse_scores, val_rmse_scores, dev_qwk_scores, val_qwk_scores):\n",
    "        data = []\n",
    "        indices = []\n",
    "        for fold_ind, scores in enumerate(zip(\n",
    "            dev_rmse_scores, val_rmse_scores, dev_qwk_scores, val_qwk_scores\n",
    "        )):\n",
    "            data.append([f'{s:.3f}' for s in scores])\n",
    "            indices.append(f'{name} - fold {fold_ind}')\n",
    "        return pd.DataFrame(data, index=indices, columns=[\n",
    "            'dev cv rmse', 'val cv rmse', 'dev cv qwk', 'val cv qwk'])\n",
    "    \n",
    "    def get_distr(y_vals):\n",
    "        y_distr = Counter(y_vals)\n",
    "        y_vals_sum = sum(y_distr.values())\n",
    "        return [f'{y_distr[i] / y_vals_sum:.2%}' for i in range(5)]\n",
    "\n",
    "    dev_rmse_scores = []\n",
    "    dev_qwk_scores = []\n",
    "    \n",
    "    val_rmse_scores = []\n",
    "    val_qwk_scores = []\n",
    "\n",
    "    pred_train_y_reg = np.zeros(len(train_y))\n",
    "    pred_test_y_reg = 0\n",
    "    \n",
    "    models = []\n",
    "    qwk_models = []\n",
    "    \n",
    "#     splits = StratifiedKFold(n_splits=k, random_state=7, shuffle=True).split(train_x, train_y)\n",
    "#     splits = GroupKFold(n_splits=k).split(train_x, train_y, groups=train_rescue_ids)\n",
    "#     splits = group_k_fold_stratified(train_x, train_y, groups=train_rescue_ids, k=5)\n",
    "    splits = greedy_group_k_fold_stratified(train_x, train_y, groups=train_rescue_ids, k=5)\n",
    "\n",
    "    for fold, (dev_indices, val_indices) in enumerate(splits):\n",
    "        g1 = set(by_indices(train_rescue_ids, dev_indices))\n",
    "        g2 = set(by_indices(train_rescue_ids, val_indices))\n",
    "        \n",
    "        assert len(g1 & g2) == 0\n",
    "        \n",
    "        dev_x, val_x = by_indices(train_x, dev_indices), by_indices(train_x, val_indices)\n",
    "        dev_y, val_y = by_indices(train_y, dev_indices), by_indices(train_y, val_indices)\n",
    "        dev_ids, val_ids = by_indices(train_ids, dev_indices), by_indices(train_ids, val_indices)\n",
    "        \n",
    "        display(pd.DataFrame([\n",
    "            [f'{len(dev_y) / len(train_y):.2%}'] + get_distr(dev_y),\n",
    "            [f'{len(val_y) / len(train_y):.2%}'] + get_distr(val_y)\n",
    "        ], index=[f'fold {fold}: dev_y', f'fold {fold}: val_y'], \n",
    "           columns=['# of examples'] + list(range(5))\n",
    "        ))\n",
    "\n",
    "        model = reg_model_factory()\n",
    "        model.fit(dev_x, dev_ids, dev_y, val_x, val_ids, val_y)\n",
    "        \n",
    "        pred_dev_y_reg = model.predict(dev_x, dev_ids)\n",
    "        dev_rmse_scores.append(rmse(dev_y, pred_dev_y_reg))\n",
    "        \n",
    "        pred_val_y_reg = model.predict(val_x, val_ids)\n",
    "        val_rmse_scores.append(rmse(val_y, pred_val_y_reg))\n",
    "\n",
    "        qwk_model = qwk_model_factory()\n",
    "        qwk_model.fit(pred_dev_y_reg, dev_ids, dev_y, pred_val_y_reg, val_ids, val_y)\n",
    "        \n",
    "        pred_dev_y = qwk_model.predict(pred_dev_y_reg, dev_ids)\n",
    "        dev_qwk_scores.append(quadratic_weighted_kappa(dev_y, pred_dev_y))\n",
    "        \n",
    "        pred_val_y = qwk_model.predict(pred_val_y_reg, val_ids)\n",
    "        val_qwk_scores.append(quadratic_weighted_kappa(val_y, pred_val_y))\n",
    "\n",
    "        pred_train_y_reg[val_indices] = pred_val_y_reg\n",
    "        pred_test_y_reg += model.predict(test_x)\n",
    "        \n",
    "        models.append(model)\n",
    "        qwk_models.append(qwk_model)\n",
    "\n",
    "    pred_test_y_reg /= k\n",
    "\n",
    "    qwk_model = qwk_model_factory()\n",
    "    qwk_model = qwk_model.from_models(qwk_models)\n",
    "\n",
    "    pred_train_y = qwk_model.predict(pred_train_y_reg, train_ids)\n",
    "    train_qwk = quadratic_weighted_kappa(train_y, pred_train_y)\n",
    "    \n",
    "    display(get_formatted_cv_evals(dev_rmse_scores, val_rmse_scores, dev_qwk_scores, val_qwk_scores))\n",
    "\n",
    "    display(pd.DataFrame([[\n",
    "        '',\n",
    "        f'{np.mean(dev_rmse_scores):.3f}',\n",
    "        f'{np.std(dev_rmse_scores):.3f}',\n",
    "        f'{np.mean(val_rmse_scores):.3f}',\n",
    "        f'{np.std(val_rmse_scores):.3f}',\n",
    "        f'{np.mean(dev_qwk_scores):.3f}',\n",
    "        f'{np.std(dev_qwk_scores):.3f}',\n",
    "        f'{np.mean(val_qwk_scores):.3f}',\n",
    "        f'{np.std(val_qwk_scores):.3f}',\n",
    "        f'{train_qwk:.3f}'\n",
    "    ]], index=[name], columns=[\n",
    "        'params', \n",
    "        'dev cv rmse (avg)', 'dev cv rmse (std)', 'val cv rmse (avg)', 'val cv rmse (std)',\n",
    "        'dev cv qwk (avg)', 'dev cv qwk (std)', 'val cv qwk (avg)', 'val cv qwk (std)', \n",
    "        'train set qwk']))\n",
    "    \n",
    "    pred_test_y = qwk_model.predict(pred_test_y_reg, test_ids)\n",
    "    \n",
    "    display(pd.DataFrame([\n",
    "        get_distr(train_y),\n",
    "        get_distr(pred_train_y),\n",
    "        get_distr(pred_test_y),\n",
    "    ], index=['train_y', 'pred_train_y', 'pred_test_y'], columns=list(range(5))))\n",
    "    \n",
    "    save_predictions(pred_test_y)\n",
    "    \n",
    "    conf_matrix = pd.DataFrame(\n",
    "        sk_cmatrix(train_y, pred_train_y), index=list(range(5)), columns=list(range(5)))\n",
    "    ax = sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
    "    ax.set_xlabel('y_pred')\n",
    "    ax.set_ylabel('y_true')\n",
    "    display(ax)\n",
    "    \n",
    "    return OrderedDict([\n",
    "        ('models',            models),\n",
    "        ('qwk_models',        qwk_models),\n",
    "        ('pred_train_y_reg',  pred_train_y_reg),\n",
    "        ('pred_train_y',      pred_train_y),\n",
    "        ('pred_test_y_reg',   pred_test_y_reg),\n",
    "        ('pred_test_y',       pred_test_y),\n",
    "    ])\n",
    "\n",
    "\n",
    "def display_train_example(idx):\n",
    "    data_id = train_ids[idx]\n",
    "    data = train_x.iloc[idx].to_dict()\n",
    "    df = pd.DataFrame([\n",
    "        ('Type', {1: 'dog', 2: 'cat'}[data['Type']]),\n",
    "        ('Name', data['Name']),\n",
    "        ('Age', data['Age']),\n",
    "        ('Breed1', breed_labels_map.get(data['Breed1'], \"MISSING\")),\n",
    "        ('Breed2', breed_labels_map.get(data['Breed2'], \"MISSING\")),\n",
    "        ('Gender', {1: 'Male', 2: 'Female', 3: 'Mixed'}[data['Gender']]),\n",
    "        ('Color1', color_labels_map.get(data['Color1'], \"MISSING\")),\n",
    "        ('Color2', color_labels_map.get(data['Color2'], \"MISSING\")),\n",
    "        ('MaturitySize', {1: 'small', 2: 'medium', 3: 'large', 4: 'extra large', 0: 'NOT SPECIFIED'}[data['MaturitySize']]),\n",
    "        ('FurLength', {1: 'short', 2: 'medium', 3: 'Long', 0: 'NOT SPECIFIED'}[data['FurLength']]),\n",
    "        ('Vaccinated', {1: 'Yes', 2: 'No', 3: 'Not sure'}[data['Vaccinated']]),\n",
    "        ('Dewormed', {1: 'Yes', 2: 'No', 3: 'Not sure'}[data['Dewormed']]),\n",
    "        ('Sterilized', {1: 'Yes', 2: 'No', 3: 'Not sure'}[data['Sterilized']]),\n",
    "        ('Health', {1: 'Healthy', 2: 'Minor', 3: 'Serious', 0: 'NOT SPECIFIED'}[data['Health']]),\n",
    "        ('Quantity', data['Quantity']),\n",
    "        ('Fee', data['Fee']),\n",
    "        ('State', state_labels_map[data['State']]),\n",
    "        ('VideoAmt', data['VideoAmt']),\n",
    "        ('PhotoAmt', data['PhotoAmt']),\n",
    "    ], columns=['attribute', 'value'])\n",
    "    df = df.set_index('attribute')\n",
    "    display(df)\n",
    "    display(data['Description'])\n",
    "    for i in range(1, int(data['PhotoAmt']) + 1):\n",
    "        display(Image(filename=str(DATA_HOME / 'train_images' / f'{data_id}-{i}.jpg')))\n",
    "\n",
    "def show_random_example(pred_train_y, actual_label, pred_label, n=10):\n",
    "    indices = [idx for idx, (y, pred_y) in enumerate(zip(train_y, pred_train_y)) \n",
    "               if y == actual_label and pred_y == pred_label]\n",
    "    display_train_example(random.choice(indices))\n",
    "\n",
    "    \n",
    "def derive_feature_names(transformer):\n",
    "    if isinstance(transformer, Pipeline):\n",
    "        _, t = transformer.steps[-1]\n",
    "        return derive_feature_names(t)\n",
    "    elif isinstance(transformer, FeatureUnion):\n",
    "        return [f'{n}_{f}' for n, t in transformer.transformer_list for f in derive_feature_names(t)]\n",
    "    try:\n",
    "        return transformer.get_feature_names()\n",
    "    except AttributeError:\n",
    "        raise AttributeError(f'Transformer {transformer} does not have `get_feature_names` function')\n",
    "\n",
    "\n",
    "def apply_pipeline(pipeline, train_x, test_x):\n",
    "    train_f = pipeline.fit_transform(train_x)\n",
    "    test_f = pipeline.transform(test_x)\n",
    "    feature_names = derive_feature_names(pipeline)\n",
    "    return train_f, test_f, pipeline, feature_names\n",
    "\n",
    "\n",
    "def get_categorical_indices(feature_names):\n",
    "    return [i for i, name in enumerate(feature_names) if name.endswith(CATEGORY_SUFFIX)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnSelector(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self, cat_cols, num_cols):\n",
    "        self._cat_cols = cat_cols\n",
    "        self._num_cols = num_cols\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, *_):\n",
    "        res_X = X[self._cat_cols + self._num_cols]\n",
    "        res_X.loc[:, self._cat_cols] = X[self._cat_cols].astype('category')\n",
    "        return res_X\n",
    "\n",
    "\n",
    "class Mapper(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self, func):\n",
    "        self._func = func\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, *_):\n",
    "        return self._func(X)\n",
    "\n",
    "\n",
    "class Features(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self, transformer, prefix='', categorical=False, feature_names=None):\n",
    "        self._transformer = transformer\n",
    "        self._prefix = prefix\n",
    "        self._categorical_suffix = CATEGORY_SUFFIX if categorical else ''\n",
    "        if feature_names is not None:\n",
    "            self._feature_names = self._format_feature_names(feature_names)\n",
    "            self._features_num = len(feature_names)\n",
    "        else:\n",
    "            self._feature_names = None\n",
    "            self._features_num = None\n",
    "    \n",
    "    def fit(self, *args, **kwds):\n",
    "        self._transformer.fit(*args, **kwds)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, *args, **kwds):\n",
    "        result = self._transformer.transform(*args, **kwds)\n",
    "        \n",
    "        if self._features_num is None:\n",
    "            self._features_num = self._get_size(result)\n",
    "        else:\n",
    "            assert self._features_num == self._get_size(result)\n",
    "        return result\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        if not self._feature_names:\n",
    "            try:\n",
    "                fns = self._transformer.get_feature_names()\n",
    "            except AttributeError:\n",
    "                if self._features_num is None:\n",
    "                    raise ValueError('Feature names cannot be taken before calling transform')\n",
    "                fns = range(self._features_num)\n",
    "            self._feature_names = self._format_feature_names(fns)\n",
    "        return self._feature_names\n",
    "    \n",
    "    def _format_feature_names(self, feature_names):\n",
    "        return [f'{self._prefix}{i}{self._categorical_suffix}' for i in feature_names]\n",
    "    \n",
    "    def _get_size(self, x):\n",
    "        if isinstance(x, list) or isinstance(x, pd.Series):\n",
    "            return len(x[0])\n",
    "        return x.shape[1]\n",
    "\n",
    "\n",
    "def Cols(cols, categorical=False):\n",
    "    return Features(\n",
    "        Mapper(lambda x: list(zip(*[x[c] for c in cols]))),\n",
    "        categorical=categorical,\n",
    "        feature_names=cols\n",
    "    )\n",
    "\n",
    "# def CatCols(cols):\n",
    "#     return Cols(cols, True)\n",
    "\n",
    "\n",
    "class CatCols(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self, cols, use_label_encoder=False):\n",
    "        self._cols = cols\n",
    "        self._feature_names = [f'{c}{CATEGORY_SUFFIX}' for c in cols]\n",
    "        \n",
    "        self._encoders = None\n",
    "        if use_label_encoder:\n",
    "            self._encoders = defaultdict(dict)\n",
    "            \n",
    "    def fit(self, X, *args, **kwds):        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, *args, **kwds):\n",
    "        col_values = []\n",
    "        for c in self._cols:\n",
    "            vals = X[c]\n",
    "            if self._encoders is not None:\n",
    "                vals = [self._encode_value(c, v) for v in vals]\n",
    "            col_values.append(vals)\n",
    "        return list(zip(*col_values))\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self._feature_names\n",
    "    \n",
    "    def _encode_value(self, col, value):\n",
    "        try:\n",
    "            result = self._encoders[col][value]\n",
    "        except KeyError:\n",
    "            result = len(self._encoders[col])\n",
    "            self._encoders[col][value] = result\n",
    "        return result\n",
    "        \n",
    "\n",
    "class Predictor(ABC):\n",
    "    @abstractmethod\n",
    "    def fit(self, train_x, train_ids, train_y, valid_x, valid_ids, valid_y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, x, ids):\n",
    "        pass\n",
    "\n",
    "\n",
    "class QwkPredictor(Predictor):\n",
    "    @abstractmethod\n",
    "    def from_models(self, models):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LgbmPredictor(Predictor):\n",
    "    def __init__(self, params, f_names):\n",
    "        params = params.copy()\n",
    "        self.num_rounds = params.pop('num_rounds')\n",
    "        self.verbose_eval = params.pop('verbose_eval')\n",
    "        self.early_stop = params.pop('early_stop', None)\n",
    "\n",
    "        self.params = dict(params)\n",
    "        self.params['seed'] = SEED\n",
    "        \n",
    "        self.model = None\n",
    "        self._cat_indices = get_categorical_indices(f_names)\n",
    "\n",
    "    def fit(self, train_x, train_ids, train_y, valid_x, valid_ids, valid_y):\n",
    "        print(train_x.shape)\n",
    "        print(valid_x.shape)\n",
    "        d_train = lgb.Dataset(train_x, label=train_y, categorical_feature=self._cat_indices)\n",
    "        d_valid = lgb.Dataset(valid_x, label=valid_y, categorical_feature=self._cat_indices)\n",
    "\n",
    "        watch_list = [d_train, d_valid]\n",
    "\n",
    "        self.model = lgb.train(\n",
    "            self.params,\n",
    "            train_set=d_train,\n",
    "            num_boost_round=self.num_rounds,\n",
    "            valid_sets=watch_list,\n",
    "            verbose_eval=self.verbose_eval,\n",
    "            early_stopping_rounds=self.early_stop\n",
    "        )\n",
    "\n",
    "    def predict(self, x, ids):\n",
    "        return self.model.predict(x, num_iteration=self.model.best_iteration)\n",
    "\n",
    "\n",
    "class QwkOptimizer(QwkPredictor):\n",
    "    def __init__(self):\n",
    "        self.coefficient = None\n",
    "\n",
    "    def fit(self, train_x, train_ids, train_y, valid_x, valid_ids, valid_y):\n",
    "        assert len(train_x) == len(train_y)\n",
    "        assert len(valid_x) == len(valid_y)\n",
    "        loss_partial = partial(self._kappa_loss, x=train_x, y=train_y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coefficient = sp.optimize.minimize(\n",
    "            loss_partial, initial_coef, method='nelder-mead')['x']\n",
    "        print(f'Coefficients after fitting are {self.coefficient}')\n",
    "\n",
    "    def predict(self, x, ids):\n",
    "        return self._apply_coeffs(self.coefficient, x)\n",
    "\n",
    "    def from_models(self, models):\n",
    "        self.coefficient = np.mean([m.coefficient for m in models], axis=0)\n",
    "        print(f'Average coefficients taken from models are {self.coefficient}')\n",
    "        return self\n",
    "\n",
    "    def _kappa_loss(self, coeffs, x, y):\n",
    "        x_p = self._apply_coeffs(coeffs, x)\n",
    "        return -quadratic_weighted_kappa(y, x_p)\n",
    "\n",
    "    def _apply_coeffs(self, coeffs, x):\n",
    "        x_p = np.copy(x)\n",
    "        for i, pred in enumerate(x_p):\n",
    "            if pred < coeffs[0]:\n",
    "                x_p[i] = 0\n",
    "            elif coeffs[0] <= pred < coeffs[1]:\n",
    "                x_p[i] = 1\n",
    "            elif coeffs[1] <= pred < coeffs[2]:\n",
    "                x_p[i] = 2\n",
    "            elif coeffs[2] <= pred < coeffs[3]:\n",
    "                x_p[i] = 3\n",
    "            else:\n",
    "                x_p[i] = 4\n",
    "        return x_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class PetPyTorchDataset(Dataset):\n",
    "    def __init__(self, pet_ids, train, y=None, transform=None):\n",
    "        self.transform = transforms.Compose(transform or [])\n",
    "        \n",
    "        image_dir = DATA_HOME / ('train_images' if train else 'test_images')\n",
    "        self.image_paths, data_indices = self._get_image_paths(image_dir, pet_ids)\n",
    "        \n",
    "        if y is not None:\n",
    "            self.y = np.array(by_indices(y, data_indices), dtype=np.float32)\n",
    "        else: \n",
    "            self.y = np.full(len(data_indices), -1, dtype=np.float32)\n",
    "            \n",
    "        self.pet_ids = by_indices(pet_ids, data_indices)\n",
    "        \n",
    "    def _get_image_paths(self, image_dir, pet_ids, only_first_image=True):\n",
    "        def get_image_idx(p):\n",
    "            return int(str(p).rsplit('-')[1].split('.')[0])\n",
    "        def get_pet_id(p):\n",
    "            return str(p).rsplit('/', 1)[1].split('-')[0]\n",
    "        \n",
    "        image_paths_per_pet_id = defaultdict(list)\n",
    "        for p in image_dir.iterdir():\n",
    "            image_paths_per_pet_id[get_pet_id(p)].append(p)\n",
    "        \n",
    "        image_paths = []\n",
    "        data_indices = []\n",
    "        for p_index, p_id in enumerate(pet_ids):\n",
    "            pet_image_paths = sorted(image_paths_per_pet_id[p_id], key=get_image_idx)\n",
    "            if len(pet_image_paths) > 0 and only_first_image:\n",
    "                pet_image_paths = [pet_image_paths[0]]\n",
    "            image_paths.extend(pet_image_paths)\n",
    "            data_indices.extend([p_index] * len(pet_image_paths))\n",
    "        \n",
    "        return image_paths, data_indices\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        return self.transform(image), self.y[idx], self.pet_ids[idx]\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base = models.resnet34(pretrained=True)\n",
    "        last_resnet_size = self.base.fc.in_features\n",
    "        print(f'Last resnet layer is: {last_resnet_size}')\n",
    "        \n",
    "        self.fc = nn.Linear(last_resnet_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base.conv1(x)\n",
    "        x = self.base.bn1(x)\n",
    "        x = self.base.relu(x)\n",
    "        x = self.base.maxpool(x)\n",
    "\n",
    "        x = self.base.layer1(x)\n",
    "        x = self.base.layer2(x)\n",
    "        x = self.base.layer3(x)\n",
    "        x = self.base.layer4(x)\n",
    "\n",
    "        x = self.base.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        loss = torch.sqrt(self.mse(yhat,y) + self.eps)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "class PyTorchModel(Predictor):\n",
    "    def __init__(self, net, lr, epochs, verbose=True):\n",
    "        self.net = net\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.batch_size = 32\n",
    "        \n",
    "        self.transforms = [\n",
    "            transforms.Resize((128, 128)),\n",
    "#             transforms.CenterCrop(224),\n",
    "#             transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "        \n",
    "        device_type = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f'Device `{device_type}` will be used')\n",
    "        self.device = torch.device(device_type)\n",
    "        self.net.to(self.device)\n",
    "    \n",
    "    def fit(self, train_x, train_ids, train_y, valid_x, valid_ids, valid_y):\n",
    "        print('Loading train images')\n",
    "        train_ds = PetPyTorchDataset(train_ids, train=True, y=train_y, transform=self.transforms)\n",
    "        print('Loading valid images')\n",
    "        valid_ds = PetPyTorchDataset(valid_ids, train=True, y=valid_y, transform=self.transforms)\n",
    "        \n",
    "        loader = DataLoader(train_ds, shuffle=True, batch_size=self.batch_size)\n",
    "        \n",
    "        loss_func = RMSELoss()\n",
    "        optimizer = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        \n",
    "        best_valid_loss = float('inf')\n",
    "        best_net_state = None\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            for x, y, pet_ids in tqdm(loader, desc=f'Training (epoch={epoch})'):\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                y_pred = self.net(x)\n",
    "\n",
    "                loss = loss_func(y_pred, y)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                valid_y_pred, valid_y = self._interference(valid_ds)\n",
    "                valid_loss = loss_func(valid_y_pred, valid_y)\n",
    "\n",
    "                loss_val = loss.item()\n",
    "                valid_loss_val = valid_loss.item()\n",
    "\n",
    "                if verbose:\n",
    "                    print(f'Epoch={epoch}, '\n",
    "                          f'Update={updates_num}, '\n",
    "                          f'Batch loss={loss_val}, '\n",
    "                          f'Valid loss={valid_loss_val}')\n",
    "\n",
    "                if valid_loss_val < best_valid_loss:\n",
    "                    best_valid_loss = valid_loss_val\n",
    "                    best_net_state = self.net.state_dict()\n",
    "\n",
    "        self.net.load_state_dict(best_net_state)\n",
    "    \n",
    "    def _interference(self, ds):\n",
    "        with torch.no_grad():\n",
    "            x = self._urls_to_tensor(urls)\n",
    "            loader = DataLoader(ds, shuffle=False, batch_size=self.batch_size)\n",
    "\n",
    "            preds_per_batch = []\n",
    "            y = []\n",
    "            pet_ids = []\n",
    "            for batch_x, batch_y, batch_pet_ids in loader:\n",
    "                x = x.to(self.device)\n",
    "                preds = self.net(x)\n",
    "                preds_per_batch.append(preds)\n",
    "                y.extend(batch_y)\n",
    "                pet_ids.extend(batch_pet_ids)\n",
    "            return torch.cat(preds_per_batch), y, pet_ids\n",
    "\n",
    "    def predict(self, x, ids):\n",
    "        ds = PetPyTorchDataset(ids, train=False, transform=self.transforms)\n",
    "        y_pred, _, inf_ids = self._interference(ds)\n",
    "        y_pred = y_pred.squeeze(1).cpu().numpy()\n",
    "        \n",
    "        preds_per_pet_id = defaultdict(list)\n",
    "        for pred, pet_id in zip(y_pred, inf_ids):\n",
    "            preds_per_pet_id[pet_id].append(pred)\n",
    "        assert(all(map(lambda xs: len(xs) == 1, preds_per_pet_id.values())))\n",
    "        \n",
    "        return [preds_per_pet_id[pet_id] for pet_id in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_nn = run_cv_model(\n",
    "    'lgbm', \n",
    "    train_y, train_x['RescuerID'], \n",
    "    train_f, train_ids, \n",
    "    test_f, test_ids, \n",
    "    lambda: PyTorchModel(ResNet(), lr=0.01, epochs=10), lambda: QwkOptimizer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path, shuffle=False):\n",
    "    X = pd.read_csv(data_path)\n",
    "    if shuffle:\n",
    "        X = X.sample(frac=1, random_seed=SEED).reset_index(drop=True)\n",
    "    try:\n",
    "        y = list(X.AdoptionSpeed)\n",
    "    except AttributeError:\n",
    "        y = None\n",
    "    ids = list(X.PetID)\n",
    "    X.drop(['PetID', 'AdoptionSpeed'], axis=1, inplace=True, errors='ignore')\n",
    "    return X, y, ids\n",
    "\n",
    "\n",
    "def read_metadata(pet_ids_list, metadata_dir):\n",
    "    def get_idx(path):\n",
    "        return int(str(path).rsplit('-', 1)[1].split('.')[0])\n",
    "    \n",
    "    def get_sorted_photos_by_idx(paths):\n",
    "        return sorted((get_idx(p), p) for p in paths)\n",
    "    \n",
    "    def extract_label_annotations(metadata_dict):\n",
    "        def by_score(xs): return xs[1]\n",
    "        label_annotations = []\n",
    "        for annotation in metadata_dict.get('labelAnnotations', []):\n",
    "            desc = annotation['mid']\n",
    "            score = annotation['score']\n",
    "            label_annotations.append((desc, score))\n",
    "        return sorted(label_annotations, key=by_score, reverse=True)\n",
    "    \n",
    "    def extract_dominant_colors(metadata_dict):\n",
    "        def by_score(xs): return xs[3]\n",
    "        dominant_colors = []\n",
    "        colors = metadata_dict.get('imagePropertiesAnnotation', {}).get('dominantColors', {}).get('colors', [])\n",
    "        for color_obj in colors:\n",
    "            rgb_obj = color_obj['color']\n",
    "            red = rgb_obj.get('red', 0) / 255\n",
    "            green = rgb_obj.get('green', 0) / 255\n",
    "            blue = rgb_obj.get('blue', 0) / 255\n",
    "            score = color_obj['score']\n",
    "            pixel_fraction = color_obj['pixelFraction']\n",
    "            dominant_colors.append(\n",
    "                (red, green, blue, score, pixel_fraction)\n",
    "            )\n",
    "        return sorted(dominant_colors, key=by_score, reverse=True)\n",
    "    \n",
    "    metadata_dir = Path(metadata_dir)\n",
    "    for pet_id in pet_ids_list:\n",
    "        pet_metadata_paths = metadata_dir.glob(f'{pet_id}-1.json')\n",
    "        \n",
    "        pet_label_annotations = []\n",
    "        pet_dominant_colors = []\n",
    "        \n",
    "        for idx, metadata_path in get_sorted_photos_by_idx(pet_metadata_paths):\n",
    "            with metadata_path.open() as in_file:\n",
    "                metadata_dict = json.load(in_file)\n",
    "                \n",
    "                pet_label_annotations.append(extract_label_annotations(metadata_dict))\n",
    "                pet_dominant_colors.append(extract_dominant_colors(metadata_dict))\n",
    "        \n",
    "        yield pet_label_annotations, pet_dominant_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentiment_data(pet_ids_list, sentiment_dir):\n",
    "    sentiment_dir = Path(sentiment_dir)\n",
    "    missed_sentiments = 0\n",
    "    for pet_id in pet_ids_list:\n",
    "        sentiment_data_path = sentiment_dir / f'{pet_id}.json'\n",
    "        \n",
    "        if not sentiment_data_path.exists():\n",
    "            yield [], {}, DEFAULT, DEFAULT\n",
    "            missed_sentiments += 1\n",
    "        else:\n",
    "            with sentiment_data_path.open() as in_file:\n",
    "                sentiment_data = json.load(in_file)\n",
    "\n",
    "                sentence_sents = [(obj['sentiment']['magnitude'], obj['sentiment']['score']) \n",
    "                                       for obj in sentiment_data['sentences']]\n",
    "                entity_sents = {obj['name'].lower().strip(): obj['salience']\n",
    "                            for obj in sentiment_data['entities']}\n",
    "                doc_sent_magnitude = sentiment_data['documentSentiment']['magnitude']\n",
    "                doc_sent_score = sentiment_data['documentSentiment']['score']\n",
    "\n",
    "                yield sentence_sents, entity_sents, (doc_sent_magnitude, doc_sent_score)\n",
    "    print(f'Missed sentiment files for {missed_sentiments} pet ids')\n",
    "\n",
    "def pet_data_assigner(X, data_reader, **mappers):\n",
    "    extacted_data = defaultdict(list)\n",
    "    data_num = 0\n",
    "    errors_per_mapper = Counter()\n",
    "    for data in data_reader:\n",
    "        for k, (map_func, default_val) in mappers.items():\n",
    "            try:\n",
    "                map_result = map_func(*data)\n",
    "            except:\n",
    "                map_result = default_val\n",
    "                errors_per_mapper[k] += 1\n",
    "            extacted_data[k].append(map_result)\n",
    "        data_num += 1 \n",
    "    \n",
    "    for k, errors_num in errors_per_mapper.items():\n",
    "        if errors_num > 0: \n",
    "            print(f'There were {errors_num} errors ({int(errors_num * 100 / data_num)}%) for mapper {k}')\n",
    "    \n",
    "    for k, data in extacted_data.items():\n",
    "        X[k] = data\n",
    "\n",
    "        \n",
    "def train_test_data_assigner(train_data_reader, test_data_reader, **mapper_info):\n",
    "    mappers = {}\n",
    "    col_assignments = []\n",
    "    for k, (mapper_func, default, add_to) in mapper_info.items():\n",
    "        mappers[k] = (mapper_func, default)\n",
    "        if add_to is not None:\n",
    "            col_assignments.append((add_to, k))\n",
    "    \n",
    "    pet_data_assigner(\n",
    "        train_x, train_data_reader, **mappers\n",
    "    )\n",
    "    pet_data_assigner(\n",
    "        test_x, test_data_reader, **mappers\n",
    "    )\n",
    "    \n",
    "    for cols_list, col in col_assignments:\n",
    "        if col not in cols_list:\n",
    "            cols_list.append(col)\n",
    "        \n",
    "def m(mapper_func, *, default, add_to=None):\n",
    "    return mapper_func, default, add_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14993, 22)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(3948, 22)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RescuerID']\n",
      "['Name', 'Description']\n",
      "['Age', 'Quantity', 'Fee', 'VideoAmt', 'PhotoAmt', 'MaturitySize', 'FurLength', 'Health']\n",
      "['Breed2', 'Sterilized', 'Color1', 'State', 'Dewormed', 'Vaccinated', 'Color3', 'Breed1', 'Type', 'Color2', 'Gender']\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, train_ids = read_data(DATA_HOME / 'train' / 'train.csv')\n",
    "test_x, test_y, test_ids = read_data(DATA_HOME / 'test' / 'test.csv')\n",
    "\n",
    "display(train_x.shape)\n",
    "display(test_x.shape)\n",
    "\n",
    "special_cols = ['RescuerID']\n",
    "text_cols = ['Name', 'Description']\n",
    "num_cols = ['Age', 'Quantity', 'Fee', 'VideoAmt', 'PhotoAmt', 'MaturitySize', 'FurLength', 'Health']\n",
    "cat_cols = list(set(train_x.columns) - set(text_cols) - set(num_cols) - set(special_cols))\n",
    "\n",
    "breed_labels_map = {row.BreedID: row.BreedName \n",
    "                   for _, row in pd.read_csv(DATA_HOME / 'breed_labels.csv').iterrows()}\n",
    "\n",
    "state_labels_map = {row.StateID: row.StateName \n",
    "                   for _, row in pd.read_csv(DATA_HOME / 'state_labels.csv').iterrows()}\n",
    "\n",
    "color_labels_map = {row.ColorID: row.ColorName \n",
    "                   for _, row in pd.read_csv(DATA_HOME / 'color_labels.csv').iterrows()}\n",
    "\n",
    "# breed_rating = json.loads((DATA_HOME / 'breed_rating.json').read_text())\n",
    "\n",
    "display(len(breed_labels_map))\n",
    "\n",
    "print(special_cols)\n",
    "print(text_cols)\n",
    "print(num_cols)\n",
    "print(cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missed sentiment files for 551 pet ids\n",
      "There were 5074 errors (33%) for mapper third_sentence_magnitude\n",
      "There were 5074 errors (33%) for mapper third_sentence_score\n",
      "There were 3161 errors (21%) for mapper second_sentence_magnitude\n",
      "There were 3161 errors (21%) for mapper second_sentence_score\n",
      "There were 551 errors (3%) for mapper first_sentence_magnitude\n",
      "There were 551 errors (3%) for mapper first_sentence_score\n",
      "There were 551 errors (3%) for mapper mean_sentence_magnitude\n",
      "There were 551 errors (3%) for mapper std_sentence_magnitude\n",
      "There were 551 errors (3%) for mapper sum_sentence_magnitude\n",
      "There were 551 errors (3%) for mapper mean_sentence_score\n",
      "There were 551 errors (3%) for mapper std_sentence_score\n",
      "There were 551 errors (3%) for mapper sum_sentence_score\n",
      "There were 551 errors (3%) for mapper sentence_count\n",
      "There were 551 errors (3%) for mapper entity_sents\n",
      "There were 551 errors (3%) for mapper doc_sent_magnitude\n",
      "There were 551 errors (3%) for mapper doc_sent_score\n",
      "Missed sentiment files for 133 pet ids\n",
      "There were 1233 errors (31%) for mapper third_sentence_magnitude\n",
      "There were 1233 errors (31%) for mapper third_sentence_score\n",
      "There were 793 errors (20%) for mapper second_sentence_magnitude\n",
      "There were 793 errors (20%) for mapper second_sentence_score\n",
      "There were 133 errors (3%) for mapper first_sentence_magnitude\n",
      "There were 133 errors (3%) for mapper first_sentence_score\n",
      "There were 133 errors (3%) for mapper mean_sentence_magnitude\n",
      "There were 133 errors (3%) for mapper std_sentence_magnitude\n",
      "There were 133 errors (3%) for mapper sum_sentence_magnitude\n",
      "There were 133 errors (3%) for mapper mean_sentence_score\n",
      "There were 133 errors (3%) for mapper std_sentence_score\n",
      "There were 133 errors (3%) for mapper sum_sentence_score\n",
      "There were 133 errors (3%) for mapper sentence_count\n",
      "There were 133 errors (3%) for mapper entity_sents\n",
      "There were 133 errors (3%) for mapper doc_sent_magnitude\n",
      "There were 133 errors (3%) for mapper doc_sent_score\n"
     ]
    }
   ],
   "source": [
    "train_test_data_assigner(\n",
    "    read_sentiment_data(train_ids, DATA_HOME / 'train_sentiment'),\n",
    "    read_sentiment_data(test_ids, DATA_HOME / 'test_sentiment'),\n",
    "    first_sentence_magnitude=     m(lambda ss, es, ds: ss[0][0], default=DEFAULT, add_to=num_cols),\n",
    "    first_sentence_score=         m(lambda ss, es, ds: ss[0][1], default=DEFAULT, add_to=num_cols),\n",
    "    second_sentence_magnitude=    m(lambda ss, es, ds: ss[1][0], default=DEFAULT, add_to=num_cols),\n",
    "    second_sentence_score=        m(lambda ss, es, ds: ss[1][1], default=DEFAULT, add_to=num_cols),\n",
    "    third_sentence_magnitude=     m(lambda ss, es, ds: ss[2][0], default=DEFAULT, add_to=num_cols),\n",
    "    third_sentence_score=         m(lambda ss, es, ds: ss[2][1], default=DEFAULT, add_to=num_cols),\n",
    "    mean_sentence_magnitude=      m(lambda ss, es, ds: np.mean([m for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    std_sentence_magnitude=       m(lambda ss, es, ds: np.std([m for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    sum_sentence_magnitude=       m(lambda ss, es, ds: np.sum([m for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    mean_sentence_score=          m(lambda ss, es, ds: np.mean([s for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    std_sentence_score=           m(lambda ss, es, ds: np.std([s for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    sum_sentence_score=           m(lambda ss, es, ds: np.sum([s for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    sentence_count=               m(lambda ss, es, ds: len(ss), default=DEFAULT, add_to=num_cols),\n",
    "    entity_sents=                 m(lambda ss, es, ds: es, default={}),\n",
    "    doc_sent_magnitude=           m(lambda ss, es, ds: ds[0], default=DEFAULT, add_to=num_cols),\n",
    "    doc_sent_score=               m(lambda ss, es, ds: ds[1], default=DEFAULT, add_to=num_cols),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_data_assigner(\n",
    "#     read_metadata(train_ids, DATA_HOME / 'train_metadata'),\n",
    "#     read_metadata(test_ids, DATA_HOME / 'test_metadata'),\n",
    "#     first_image_label_annotations=     m(lambda la, dm: dict(la[0]), default={}),\n",
    "#     second_image_label_annotations=     m(lambda la, dm: dict(la[1]), default={}),\n",
    "#     third_image_label_annotations=     m(lambda la, dm: dict(la[2]), default={}),\n",
    "#     first_image_color_annotations=     m(lambda la, dm: dict(la[0]), default={}),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFeatures(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self, prefix=''):\n",
    "        self._prefix = prefix\n",
    "    \n",
    "    def transform(self, texts, *_):\n",
    "        return [self._text_features(t) for t in texts]\n",
    "    \n",
    "    def _text_features(self, text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        token_sizes = list(map(len, tokens))\n",
    "        \n",
    "        return [\n",
    "            len(text),\n",
    "            len(tokens),\n",
    "            min(token_sizes) if token_sizes else DEFAULT,\n",
    "            max(token_sizes) if token_sizes else DEFAULT,\n",
    "            sum(token_sizes) if token_sizes else DEFAULT,\n",
    "            np.mean(token_sizes) if token_sizes else DEFAULT,\n",
    "            np.std(token_sizes) if token_sizes else DEFAULT,\n",
    "        ]\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return [f'{self._prefix}{n}' for n in [\n",
    "            'text_len', 'tokens_num', 'min_token_size', 'max_token_size', 'sum_token_size', \n",
    "            'mean_token_size', 'std_token_size']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "\n",
    "# LANG = 'english'\n",
    "# stemmer = nltk.stem.SnowballStemmer(LANG)\n",
    "# stopwords = stopwords.words('english')\n",
    "\n",
    "# def tokenize(text, language=LANG):\n",
    "#     return [w for w in word_tokenize(text.lower(), language=language) if w.isalnum()]\n",
    "\n",
    "# def stemmed_words(words):\n",
    "#     return [stemmer.stem(w) for w in words]\n",
    "\n",
    "# def ngrams(tokens, min_n, max_n):\n",
    "#     if max_n != 1:\n",
    "#         original_tokens = tokens\n",
    "#         if min_n == 1:\n",
    "#             # no need to do any slicing for unigrams\n",
    "#             # just iterate through the original tokens\n",
    "#             tokens = list(original_tokens)\n",
    "#             min_n += 1\n",
    "#         else:\n",
    "#             tokens = []\n",
    "\n",
    "#         n_original_tokens = len(original_tokens)\n",
    "\n",
    "#         # bind method outside of loop to reduce overhead\n",
    "#         tokens_append = tokens.append\n",
    "#         space_join = \" \".join\n",
    "\n",
    "#         for n in range(min_n, min(max_n + 1, n_original_tokens + 1)):\n",
    "#             for i in range(n_original_tokens - n + 1):\n",
    "#                 tokens_append(space_join(original_tokens[i: i + n]))\n",
    "#     return tokens\n",
    "\n",
    "\n",
    "# def nltk_analyzer(text, min_n, max_n):\n",
    "#     tokens = [t for t in tokenize(text) if t not in stopwords]\n",
    "#     return ngrams(stemmed_words(text), min_n, max_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode(df, col, fillna=None):\n",
    "    ret = None\n",
    "    \n",
    "    col_name = col if isinstance(col, str) else '_'.join(col)\n",
    "    \n",
    "    new_cols = pd.DataFrame(({f'{col_name}_{k}': v for k, v in d.items()} \n",
    "                             for idx, d in df[col].iteritems()))\n",
    "    if fillna is not None:\n",
    "        new_cols = new_cols.fillna(fillna)\n",
    "    new_cols.index = df.index\n",
    "    \n",
    "    ret = pd.concat([df, new_cols], axis=1)\n",
    "    del ret[col]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RescuerFeatures(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self, num_cols, cat_cols, rescued_id_col='RescuerID'):\n",
    "        self._num_cols = num_cols\n",
    "        self._cat_cols = cat_cols\n",
    "        self._rescued_id_col = rescued_id_col\n",
    "        \n",
    "        self.columns = None\n",
    "    \n",
    "    def transform(self, df, *_):\n",
    "        result = self._extract_rescuer_features(df)\n",
    "        for c in self.columns:\n",
    "            if c not in result.columns:\n",
    "                result[c] = np.full(df.shape[0], DEFAULT)\n",
    "        return result[self.columns]\n",
    "    \n",
    "    def fit(self, df, *_):\n",
    "        df = self._extract_rescuer_features(df)\n",
    "        display(df)\n",
    "        self.columns = df.columns\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, df, *_):\n",
    "        df = self._extract_rescuer_features(df)\n",
    "        self.columns = df.columns\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        if self.columns is None:\n",
    "            raise ValueError('RescuerFeatures was not fitted')\n",
    "        return self.columns\n",
    "    \n",
    "    def _extract_rescuer_features(self, df):\n",
    "        def num_col_features(values):\n",
    "            return dict(\n",
    "                min=min(values) if len(values) > 0 else DEFAULT,\n",
    "                max=max(values) if len(values) > 0 else DEFAULT,\n",
    "                sum=sum(values) if len(values) > 0 else DEFAULT,\n",
    "                mean=np.mean(values) if len(values) > 0 else DEFAULT,\n",
    "                std=np.std(values) if len(values) > 0 else DEFAULT,\n",
    "            )\n",
    "        \n",
    "        def cat_col_features(values):\n",
    "            return Counter(values)\n",
    "        \n",
    "        agg_operations = defaultdict(list)\n",
    "        for c in self._num_cols:\n",
    "            agg_operations[c].append(num_col_features)\n",
    "        for c in self._cat_cols:\n",
    "            agg_operations[c].append(cat_col_features)\n",
    "        agg_operations[self._rescued_id_col].append('size')\n",
    "            \n",
    "        by_rescuer_id = df.groupby(self._rescued_id_col).agg(agg_operations)\n",
    "        for outer_c, inner_c in by_rescuer_id.columns:\n",
    "            if inner_c in {'num_col_features', 'cat_col_features'}:\n",
    "                by_rescuer_id = explode(by_rescuer_id, (outer_c, inner_c), DEFAULT)\n",
    "        by_rescuer_id.columns = [c if isinstance(c, str) else '_'.join(c) \n",
    "                                 for c in by_rescuer_id.columns]\n",
    "        \n",
    "        result = df[[self._rescued_id_col]].merge(\n",
    "            by_rescuer_id, how='outer', left_on=self._rescued_id_col, right_index=True\n",
    "        ).sort_index()\n",
    "        result.drop(self._rescued_id_col, axis=1, inplace=True)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_f, test_f, p, f_names = apply_pipeline(make_pipeline(\n",
    "    FeatureUnion([\n",
    "#         ('desc_tokenizing', make_pipeline(\n",
    "#             Mapper(lambda x: list(x.Description.fillna(''))),\n",
    "#             CountVectorizer(\n",
    "#                 analyzer='word',\n",
    "#                 ngram_range=(1, 3),\n",
    "#                 dtype=np.float32\n",
    "#             ),\n",
    "#             FeatureUnion([\n",
    "#                 ('svd', Features(TruncatedSVD(n_components=128, random_state=SEED))),\n",
    "#                 ('nmf', Features(NMF(n_components=128, random_state=SEED)))\n",
    "#             ])\n",
    "#         )),\n",
    "        ('desc_text_fs', make_pipeline(\n",
    "            Mapper(lambda x: list(x.Description.fillna(''))),\n",
    "            TextFeatures('desc_')\n",
    "        )),\n",
    "#         ('name_text_fs', make_pipeline(\n",
    "#             Mapper(lambda x: list(x.Name.fillna(''))),\n",
    "#             TextFeatures('name_')\n",
    "#         )),\n",
    "#         ('num_cols', Cols(num_cols)),\n",
    "#         ('cat_cols', CatCols(cat_cols, use_label_encoder=True)),\n",
    "#         ('rescuer_fs', make_pipeline(\n",
    "#             RescuerFeatures(num_cols=num_cols, cat_cols=cat_cols),\n",
    "# #             FeatureUnion([\n",
    "# #                 Features(TruncatedSVD(n_components=16, random_state=SEED)),\n",
    "# #                 Features(NMF(n_components=16, random_state=SEED))\n",
    "# #             ])\n",
    "#         )),\n",
    "# #         make_pipeline(\n",
    "# #             Mapper(lambda x: x.first_image_label_annotations),\n",
    "# #             DictVectorizer()\n",
    "# #         ),\n",
    "#         ('entity_sents', make_pipeline(\n",
    "#             Mapper(lambda x: x.entity_sents),\n",
    "#             DictVectorizer(),\n",
    "#             FeatureUnion([\n",
    "#                 ('svd', Features(TruncatedSVD(n_components=8, random_state=SEED))),\n",
    "#                 ('nmf', Features(NMF(n_components=8, random_state=SEED)))\n",
    "#             ])\n",
    "#         )),\n",
    "#         Features(Mapper(lambda x: list(x.first_image_color_annotations)))\n",
    "    ])\n",
    "), train_x, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_f, columns=f_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'max_bin': 512,\n",
    "          'num_leaves': 30,\n",
    "          'max_depth': 6,\n",
    "          'min_data_in_leaf': 10,\n",
    "          'learning_rate': 0.001,\n",
    "          'bagging_fraction': 0.85,\n",
    "#           'feature_fraction': 0.8,\n",
    "#           'min_split_gain': 0.02,\n",
    "#           'min_child_weight': 0.02,\n",
    "          'lambda_l2': 0.25,\n",
    "          'verbosity': 1,\n",
    "          'early_stop': 2000,\n",
    "          'verbose_eval': 100,\n",
    "          'num_rounds': 20000,\n",
    "         }\n",
    "\n",
    "results = run_cv_model(\n",
    "    'lgbm', \n",
    "    train_y, train_x['RescuerID'], \n",
    "    train_f, train_ids, \n",
    "    test_f, test_ids, \n",
    "    lambda: LgbmPredictor(params, f_names), lambda: QwkOptimizer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attribute</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <td>Snoopy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breed1</th>\n",
       "      <td>Mixed Breed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breed2</th>\n",
       "      <td>MISSING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Color1</th>\n",
       "      <td>Golden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Color2</th>\n",
       "      <td>MISSING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MaturitySize</th>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FurLength</th>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vaccinated</th>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dewormed</th>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sterilized</th>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health</th>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quantity</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fee</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <td>Johor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VideoAmt</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PhotoAmt</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    value\n",
       "attribute                \n",
       "Type                  dog\n",
       "Name               Snoopy\n",
       "Age                    10\n",
       "Breed1        Mixed Breed\n",
       "Breed2            MISSING\n",
       "Gender               Male\n",
       "Color1             Golden\n",
       "Color2            MISSING\n",
       "MaturitySize       medium\n",
       "FurLength          medium\n",
       "Vaccinated            Yes\n",
       "Dewormed               No\n",
       "Sterilized            Yes\n",
       "Health            Healthy\n",
       "Quantity                1\n",
       "Fee                     0\n",
       "State               Johor\n",
       "VideoAmt                0\n",
       "PhotoAmt                1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Snoopy very cute. He looks better than in the picture. No caging for long hours except for precautionary purposes. Interested adopters pls call Ms.Ooi at.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_example(results['pred_train_y'], actual_label=1, pred_label=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat_cols_Breed1@c</td>\n",
       "      <td>515179.702916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>num_cols_Age</td>\n",
       "      <td>391941.525415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cat_cols_Sterilized@c</td>\n",
       "      <td>109767.612425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rescuer_fs_Type_cat_col_features_2</td>\n",
       "      <td>52059.698224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat_cols_Breed2@c</td>\n",
       "      <td>51125.398274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rescuer_fs_PhotoAmt_num_col_features_sum</td>\n",
       "      <td>47838.754436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cat_cols_State@c</td>\n",
       "      <td>45254.057627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rescuer_fs_Quantity_num_col_features_mean</td>\n",
       "      <td>40575.521587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rescuer_fs_Age_num_col_features_mean</td>\n",
       "      <td>39706.839125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rescuer_fs_PhotoAmt_num_col_features_mean</td>\n",
       "      <td>39523.141807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rescuer_fs_Sterilized_cat_col_features_2</td>\n",
       "      <td>35889.190979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rescuer_fs_PhotoAmt_num_col_features_std</td>\n",
       "      <td>33918.591371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rescuer_fs_doc_sent_magnitude_num_col_features...</td>\n",
       "      <td>32278.510279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>num_cols_Quantity</td>\n",
       "      <td>29303.131022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rescuer_fs_Vaccinated_cat_col_features_3</td>\n",
       "      <td>22260.593412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>num_cols_PhotoAmt</td>\n",
       "      <td>21714.770802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rescuer_fs_first_sentence_magnitude_num_col_fe...</td>\n",
       "      <td>20927.974398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rescuer_fs_Quantity_num_col_features_min</td>\n",
       "      <td>18410.154257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rescuer_fs_FurLength_num_col_features_sum</td>\n",
       "      <td>16474.175291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rescuer_fs_PhotoAmt_num_col_features_max</td>\n",
       "      <td>15226.563411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rescuer_fs_FurLength_num_col_features_std</td>\n",
       "      <td>14954.099413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rescuer_fs_PhotoAmt_num_col_features_min</td>\n",
       "      <td>14375.019211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>desc_tokenizing_svd_29</td>\n",
       "      <td>13388.705388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rescuer_fs_Gender_cat_col_features_1</td>\n",
       "      <td>13251.795547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>desc_tokenizing_nmf_99</td>\n",
       "      <td>12945.627378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rescuer_fs_first_sentence_magnitude_num_col_fe...</td>\n",
       "      <td>12670.196321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>rescuer_fs_Breed1_cat_col_features_307</td>\n",
       "      <td>12266.682116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>cat_cols_Gender@c</td>\n",
       "      <td>11025.462009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>cat_cols_Color1@c</td>\n",
       "      <td>10307.823086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>rescuer_fs_mean_sentence_magnitude_num_col_fea...</td>\n",
       "      <td>10051.890508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>rescuer_fs_Color3_cat_col_features_4</td>\n",
       "      <td>10042.918868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>desc_tokenizing_svd_8</td>\n",
       "      <td>10012.902095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>rescuer_fs_first_sentence_score_num_col_featur...</td>\n",
       "      <td>9895.345179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>rescuer_fs_third_sentence_score_num_col_featur...</td>\n",
       "      <td>9521.162695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>desc_tokenizing_svd_54</td>\n",
       "      <td>9507.313196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>num_cols_MaturitySize</td>\n",
       "      <td>9415.810806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>desc_tokenizing_svd_79</td>\n",
       "      <td>9266.227711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>rescuer_fs_Gender_cat_col_features_3</td>\n",
       "      <td>9114.219080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>desc_tokenizing_svd_17</td>\n",
       "      <td>8940.216492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>rescuer_fs_Fee_num_col_features_mean</td>\n",
       "      <td>8414.058311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>desc_text_fs_desc_mean_token_size</td>\n",
       "      <td>8362.782991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>desc_tokenizing_nmf_76</td>\n",
       "      <td>8284.501591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>rescuer_fs_Sterilized_cat_col_features_1</td>\n",
       "      <td>8262.080286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>rescuer_fs_MaturitySize_num_col_features_std</td>\n",
       "      <td>7917.184302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>cat_cols_Dewormed@c</td>\n",
       "      <td>7826.565899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>rescuer_fs_sentence_count_num_col_features_sum</td>\n",
       "      <td>7669.893396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>desc_tokenizing_svd_124</td>\n",
       "      <td>7655.269908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>rescuer_fs_mean_sentence_score_num_col_feature...</td>\n",
       "      <td>7157.144203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>desc_tokenizing_nmf_98</td>\n",
       "      <td>7016.793711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>rescuer_fs_Breed2_cat_col_features_266</td>\n",
       "      <td>7003.441095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0              1\n",
       "0                                   cat_cols_Breed1@c  515179.702916\n",
       "1                                        num_cols_Age  391941.525415\n",
       "2                               cat_cols_Sterilized@c  109767.612425\n",
       "3                  rescuer_fs_Type_cat_col_features_2   52059.698224\n",
       "4                                   cat_cols_Breed2@c   51125.398274\n",
       "5            rescuer_fs_PhotoAmt_num_col_features_sum   47838.754436\n",
       "6                                    cat_cols_State@c   45254.057627\n",
       "7           rescuer_fs_Quantity_num_col_features_mean   40575.521587\n",
       "8                rescuer_fs_Age_num_col_features_mean   39706.839125\n",
       "9           rescuer_fs_PhotoAmt_num_col_features_mean   39523.141807\n",
       "10           rescuer_fs_Sterilized_cat_col_features_2   35889.190979\n",
       "11           rescuer_fs_PhotoAmt_num_col_features_std   33918.591371\n",
       "12  rescuer_fs_doc_sent_magnitude_num_col_features...   32278.510279\n",
       "13                                  num_cols_Quantity   29303.131022\n",
       "14           rescuer_fs_Vaccinated_cat_col_features_3   22260.593412\n",
       "15                                  num_cols_PhotoAmt   21714.770802\n",
       "16  rescuer_fs_first_sentence_magnitude_num_col_fe...   20927.974398\n",
       "17           rescuer_fs_Quantity_num_col_features_min   18410.154257\n",
       "18          rescuer_fs_FurLength_num_col_features_sum   16474.175291\n",
       "19           rescuer_fs_PhotoAmt_num_col_features_max   15226.563411\n",
       "20          rescuer_fs_FurLength_num_col_features_std   14954.099413\n",
       "21           rescuer_fs_PhotoAmt_num_col_features_min   14375.019211\n",
       "22                             desc_tokenizing_svd_29   13388.705388\n",
       "23               rescuer_fs_Gender_cat_col_features_1   13251.795547\n",
       "24                             desc_tokenizing_nmf_99   12945.627378\n",
       "25  rescuer_fs_first_sentence_magnitude_num_col_fe...   12670.196321\n",
       "26             rescuer_fs_Breed1_cat_col_features_307   12266.682116\n",
       "27                                  cat_cols_Gender@c   11025.462009\n",
       "28                                  cat_cols_Color1@c   10307.823086\n",
       "29  rescuer_fs_mean_sentence_magnitude_num_col_fea...   10051.890508\n",
       "30               rescuer_fs_Color3_cat_col_features_4   10042.918868\n",
       "31                              desc_tokenizing_svd_8   10012.902095\n",
       "32  rescuer_fs_first_sentence_score_num_col_featur...    9895.345179\n",
       "33  rescuer_fs_third_sentence_score_num_col_featur...    9521.162695\n",
       "34                             desc_tokenizing_svd_54    9507.313196\n",
       "35                              num_cols_MaturitySize    9415.810806\n",
       "36                             desc_tokenizing_svd_79    9266.227711\n",
       "37               rescuer_fs_Gender_cat_col_features_3    9114.219080\n",
       "38                             desc_tokenizing_svd_17    8940.216492\n",
       "39               rescuer_fs_Fee_num_col_features_mean    8414.058311\n",
       "40                  desc_text_fs_desc_mean_token_size    8362.782991\n",
       "41                             desc_tokenizing_nmf_76    8284.501591\n",
       "42           rescuer_fs_Sterilized_cat_col_features_1    8262.080286\n",
       "43       rescuer_fs_MaturitySize_num_col_features_std    7917.184302\n",
       "44                                cat_cols_Dewormed@c    7826.565899\n",
       "45     rescuer_fs_sentence_count_num_col_features_sum    7669.893396\n",
       "46                            desc_tokenizing_svd_124    7655.269908\n",
       "47  rescuer_fs_mean_sentence_score_num_col_feature...    7157.144203\n",
       "48                             desc_tokenizing_nmf_98    7016.793711\n",
       "49             rescuer_fs_Breed2_cat_col_features_266    7003.441095"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(\n",
    "    Counter(dict(zip(f_names, results['models'][0].model.feature_importance('gain')))).most_common(50)\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
