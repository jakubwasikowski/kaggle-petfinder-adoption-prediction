{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "0a7e025d428361b9e1c7b0fda4cd762df32a5009"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS_LOCAL = False\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import json\n",
    "import random\n",
    "import colorsys\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from math import sqrt\n",
    "from itertools import chain\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "\n",
    "from IPython.display import display, Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "from sklearn.pipeline import make_pipeline, make_union, Pipeline, FeatureUnion\n",
    "from sklearn.metrics import confusion_matrix as sk_cmatrix\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder\n",
    "from ml_metrics import quadratic_weighted_kappa, rmse\n",
    "\n",
    "DATA_HOME = Path('../input')\n",
    "PF_HOME = DATA_HOME / 'petfinder-adoption-prediction'\n",
    "\n",
    "IS_LOCAL = Path('IS_LOCAL').exists()\n",
    "LABEL_NUM = 5\n",
    "\n",
    "DEFAULT = 0\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(f'IS_LOCAL = {IS_LOCAL}')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "SEED = 7\n",
    "np.random.seed(SEED)\n",
    "CATEGORY_SUFFIX = '@c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "76a0fcf653021b0b5440c41d47590a787861d928"
   },
   "outputs": [],
   "source": [
    "def greedy_group_k_fold_stratified(X, y, groups, k=5):\n",
    "    y_counts_per_group = defaultdict(lambda: np.zeros(LABEL_NUM))\n",
    "    y_distr = Counter()\n",
    "    for label, g in zip(y, groups):\n",
    "        y_counts_per_group[g][label] += 1\n",
    "        y_distr[label] += 1\n",
    "\n",
    "    y_counts_per_fold = defaultdict(lambda: np.zeros(LABEL_NUM))\n",
    "    groups_per_fold = defaultdict(set)\n",
    "\n",
    "    def eval_y_counts_per_fold(y_counts, fold):\n",
    "        y_counts_per_fold[fold] += y_counts\n",
    "        std_per_label = []\n",
    "        for label in range(LABEL_NUM):\n",
    "            label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(k)])\n",
    "            std_per_label.append(label_std)\n",
    "        y_counts_per_fold[fold] -= y_counts\n",
    "        return np.mean(std_per_label)\n",
    "\n",
    "    for g, y_counts in sorted(y_counts_per_group.items(), key=lambda x: -np.std(x[1])):\n",
    "        best_fold = None\n",
    "        min_eval = None\n",
    "        for i in range(k):\n",
    "            fold_eval = eval_y_counts_per_fold(y_counts, i)\n",
    "            if min_eval is None or fold_eval < min_eval:\n",
    "                min_eval = fold_eval\n",
    "                best_fold = i\n",
    "        y_counts_per_fold[best_fold] += y_counts\n",
    "        groups_per_fold[best_fold].add(g)\n",
    "\n",
    "    all_groups = set(groups)\n",
    "    for i in range(k):\n",
    "        train_groups = all_groups - groups_per_fold[i]\n",
    "        test_groups = groups_per_fold[i]\n",
    "\n",
    "        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n",
    "        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n",
    "\n",
    "        yield train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "29bc8ed7b95e70e591a85150d8a147eb07cf15bd"
   },
   "outputs": [],
   "source": [
    "def by_indices(data, indices):\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            return data.iloc[indices]\n",
    "        if isinstance(data, list):\n",
    "            return [data[i] for i in indices]\n",
    "        return data[indices]     \n",
    "\n",
    "def run_cv_model(\n",
    "    name, \n",
    "    train_y, train_rescue_ids,\n",
    "    train_x, train_ids,\n",
    "    test_x, test_ids, \n",
    "    reg_model_factory, qwk_model_factory, k=5\n",
    "):\n",
    "    def format_stats(mean: float, std: float) -> str:\n",
    "        return f'{mean:.3f} Â± {std:.3f}'\n",
    "\n",
    "    def save_predictions(pred_y):\n",
    "        if IS_LOCAL:\n",
    "            preds_dir = Path('predictions')\n",
    "            preds_path = preds_dir / f'{name}.csv'\n",
    "            checks_num = test_ids\n",
    "            while preds_path.exists():\n",
    "                checks_num += 1\n",
    "                new_file_name = f'{name}-{checks_num}.csv'\n",
    "                preds_path = preds_dir / new_file_name\n",
    "            print(f'Saving to path `{preds_path}`')\n",
    "        else:\n",
    "            preds_path = 'submission.csv'\n",
    "\n",
    "        submission = pd.DataFrame({'PetID': test_ids, 'AdoptionSpeed': pred_y.astype(np.int32)})\n",
    "        submission.to_csv(preds_path, index=False)\n",
    "    \n",
    "    def get_formatted_cv_evals(dev_rmse_scores, val_rmse_scores, dev_qwk_scores, val_qwk_scores):\n",
    "        data = []\n",
    "        indices = []\n",
    "        for fold_ind, scores in enumerate(zip(\n",
    "            dev_rmse_scores, val_rmse_scores, dev_qwk_scores, val_qwk_scores\n",
    "        )):\n",
    "            data.append([f'{s:.3f}' for s in scores])\n",
    "            indices.append(f'{name} - fold {fold_ind}')\n",
    "        return pd.DataFrame(data, index=indices, columns=[\n",
    "            'dev cv rmse', 'val cv rmse', 'dev cv qwk', 'val cv qwk'])\n",
    "    \n",
    "    def get_distr(y_vals):\n",
    "        y_distr = Counter(y_vals)\n",
    "        y_vals_sum = sum(y_distr.values())\n",
    "        return [f'{y_distr[i] / y_vals_sum:.2%}' for i in range(5)]\n",
    "\n",
    "    dev_rmse_scores = []\n",
    "    dev_qwk_scores = []\n",
    "    \n",
    "    val_rmse_scores = []\n",
    "    val_qwk_scores = []\n",
    "\n",
    "    pred_train_y_reg = np.zeros(len(train_y))\n",
    "    pred_test_y_reg = 0\n",
    "    \n",
    "    models = []\n",
    "    qwk_models = []\n",
    "    \n",
    "#     splits = StratifiedKFold(n_splits=k, random_state=7, shuffle=True).split(train_x, train_y)\n",
    "#     splits = GroupKFold(n_splits=k).split(train_x, train_y, groups=train_rescue_ids)\n",
    "#     splits = group_k_fold_stratified(train_x, train_y, groups=train_rescue_ids, k=5)\n",
    "    splits = greedy_group_k_fold_stratified(train_x, train_y, groups=train_rescue_ids, k=5)\n",
    "\n",
    "    for fold, (dev_indices, val_indices) in enumerate(splits):\n",
    "        g1 = set(by_indices(train_rescue_ids, dev_indices))\n",
    "        g2 = set(by_indices(train_rescue_ids, val_indices))\n",
    "        \n",
    "        assert len(g1 & g2) == 0\n",
    "        \n",
    "        dev_x, val_x = by_indices(train_x, dev_indices), by_indices(train_x, val_indices)\n",
    "        dev_y, val_y = by_indices(train_y, dev_indices), by_indices(train_y, val_indices)\n",
    "        dev_ids, val_ids = by_indices(train_ids, dev_indices), by_indices(train_ids, val_indices)\n",
    "        \n",
    "        display(pd.DataFrame([\n",
    "            [f'{len(dev_y) / len(train_y):.2%}'] + get_distr(dev_y),\n",
    "            [f'{len(val_y) / len(train_y):.2%}'] + get_distr(val_y)\n",
    "        ], index=[f'fold {fold}: dev_y', f'fold {fold}: val_y'], \n",
    "           columns=['# of examples'] + list(range(5))\n",
    "        ))\n",
    "\n",
    "        model = reg_model_factory()\n",
    "        model.fit(dev_x, dev_ids, dev_y, val_x, val_ids, val_y)\n",
    "        \n",
    "        pred_dev_y_reg = model.predict(dev_x, dev_ids, is_train=True)\n",
    "        dev_rmse_scores.append(rmse(dev_y, pred_dev_y_reg))\n",
    "        \n",
    "        pred_val_y_reg = model.predict(val_x, val_ids, is_train=True)\n",
    "        val_rmse_scores.append(rmse(val_y, pred_val_y_reg))\n",
    "\n",
    "        qwk_model = qwk_model_factory()\n",
    "        qwk_model.fit(pred_dev_y_reg, dev_ids, dev_y, pred_val_y_reg, val_ids, val_y)\n",
    "        \n",
    "        pred_dev_y = qwk_model.predict(pred_dev_y_reg, dev_ids, is_train=True)\n",
    "        dev_qwk_scores.append(quadratic_weighted_kappa(dev_y, pred_dev_y))\n",
    "        \n",
    "        pred_val_y = qwk_model.predict(pred_val_y_reg, val_ids, is_train=True)\n",
    "        val_qwk_scores.append(quadratic_weighted_kappa(val_y, pred_val_y))\n",
    "\n",
    "        pred_train_y_reg[val_indices] = pred_val_y_reg\n",
    "        pred_test_y_reg += model.predict(test_x, test_ids, is_train=False)\n",
    "        \n",
    "        models.append(model)\n",
    "        qwk_models.append(qwk_model)\n",
    "\n",
    "    pred_test_y_reg /= k\n",
    "\n",
    "    qwk_model = qwk_model_factory()\n",
    "    qwk_model = qwk_model.from_models(qwk_models)\n",
    "\n",
    "    pred_train_y = qwk_model.predict(pred_train_y_reg, train_ids, is_train=True)\n",
    "    train_qwk = quadratic_weighted_kappa(train_y, pred_train_y)\n",
    "    \n",
    "    display(get_formatted_cv_evals(dev_rmse_scores, val_rmse_scores, dev_qwk_scores, val_qwk_scores))\n",
    "\n",
    "    display(pd.DataFrame([[\n",
    "        '',\n",
    "        f'{np.mean(dev_rmse_scores):.3f}',\n",
    "        f'{np.std(dev_rmse_scores):.3f}',\n",
    "        f'{np.mean(val_rmse_scores):.3f}',\n",
    "        f'{np.std(val_rmse_scores):.3f}',\n",
    "        f'{np.mean(dev_qwk_scores):.3f}',\n",
    "        f'{np.std(dev_qwk_scores):.3f}',\n",
    "        f'{np.mean(val_qwk_scores):.3f}',\n",
    "        f'{np.std(val_qwk_scores):.3f}',\n",
    "        f'{train_qwk:.3f}'\n",
    "    ]], index=[name], columns=[\n",
    "        'params', \n",
    "        'dev cv rmse (avg)', 'dev cv rmse (std)', 'val cv rmse (avg)', 'val cv rmse (std)',\n",
    "        'dev cv qwk (avg)', 'dev cv qwk (std)', 'val cv qwk (avg)', 'val cv qwk (std)', \n",
    "        'train set qwk']))\n",
    "    \n",
    "    pred_test_y = qwk_model.predict(pred_test_y_reg, test_ids, is_train=False)\n",
    "    \n",
    "    display(pd.DataFrame([\n",
    "        get_distr(train_y),\n",
    "        get_distr(pred_train_y),\n",
    "        get_distr(pred_test_y),\n",
    "    ], index=['train_y', 'pred_train_y', 'pred_test_y'], columns=list(range(5))))\n",
    "    \n",
    "    save_predictions(pred_test_y)\n",
    "    \n",
    "    conf_matrix = pd.DataFrame(\n",
    "        sk_cmatrix(train_y, pred_train_y), index=list(range(5)), columns=list(range(5)))\n",
    "    ax = sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
    "    ax.set_xlabel('y_pred')\n",
    "    ax.set_ylabel('y_true')\n",
    "    display(ax)\n",
    "    \n",
    "    return OrderedDict([\n",
    "        ('models',            models),\n",
    "        ('qwk_models',        qwk_models),\n",
    "        ('pred_train_y_reg',  pred_train_y_reg),\n",
    "        ('pred_train_y',      pred_train_y),\n",
    "        ('pred_test_y_reg',   pred_test_y_reg),\n",
    "        ('pred_test_y',       pred_test_y),\n",
    "    ])\n",
    "\n",
    "\n",
    "def display_train_example(idx):\n",
    "    data_id = train_ids[idx]\n",
    "    data = train_x.iloc[idx].to_dict()\n",
    "    df = pd.DataFrame([\n",
    "        ('Type', {1: 'dog', 2: 'cat'}[data['Type']]),\n",
    "        ('Name', data['Name']),\n",
    "        ('Age', data['Age']),\n",
    "        ('Breed1', breed_labels_map.get(data['Breed1'], \"MISSING\")),\n",
    "        ('Breed2', breed_labels_map.get(data['Breed2'], \"MISSING\")),\n",
    "        ('Gender', {1: 'Male', 2: 'Female', 3: 'Mixed'}[data['Gender']]),\n",
    "        ('Color1', color_labels_map.get(data['Color1'], \"MISSING\")),\n",
    "        ('Color2', color_labels_map.get(data['Color2'], \"MISSING\")),\n",
    "        ('MaturitySize', {1: 'small', 2: 'medium', 3: 'large', 4: 'extra large', 0: 'NOT SPECIFIED'}[data['MaturitySize']]),\n",
    "        ('FurLength', {1: 'short', 2: 'medium', 3: 'Long', 0: 'NOT SPECIFIED'}[data['FurLength']]),\n",
    "        ('Vaccinated', {1: 'Yes', 2: 'No', 3: 'Not sure'}[data['Vaccinated']]),\n",
    "        ('Dewormed', {1: 'Yes', 2: 'No', 3: 'Not sure'}[data['Dewormed']]),\n",
    "        ('Sterilized', {1: 'Yes', 2: 'No', 3: 'Not sure'}[data['Sterilized']]),\n",
    "        ('Health', {1: 'Healthy', 2: 'Minor', 3: 'Serious', 0: 'NOT SPECIFIED'}[data['Health']]),\n",
    "        ('Quantity', data['Quantity']),\n",
    "        ('Fee', data['Fee']),\n",
    "        ('State', state_labels_map[data['State']]),\n",
    "        ('VideoAmt', data['VideoAmt']),\n",
    "        ('PhotoAmt', data['PhotoAmt']),\n",
    "    ], columns=['attribute', 'value'])\n",
    "    df = df.set_index('attribute')\n",
    "    display(df)\n",
    "    display(data['Description'])\n",
    "    for i in range(1, int(data['PhotoAmt']) + 1):\n",
    "        display(Image(filename=str(PF_HOME / 'train_images' / f'{data_id}-{i}.jpg')))\n",
    "\n",
    "def show_random_example(pred_train_y, actual_label, pred_label, n=10):\n",
    "    indices = [idx for idx, (y, pred_y) in enumerate(zip(train_y, pred_train_y)) \n",
    "               if y == actual_label and pred_y == pred_label]\n",
    "    display_train_example(random.choice(indices))\n",
    "\n",
    "    \n",
    "def derive_feature_names(transformer):\n",
    "    if isinstance(transformer, Pipeline):\n",
    "        _, t = transformer.steps[-1]\n",
    "        return derive_feature_names(t)\n",
    "    elif isinstance(transformer, FeatureUnion):\n",
    "        return [f'{n}_{f}' for n, t in transformer.transformer_list for f in derive_feature_names(t)]\n",
    "    try:\n",
    "        return transformer.get_feature_names()\n",
    "    except AttributeError:\n",
    "        raise AttributeError(f'Transformer {transformer} does not have `get_feature_names` function')\n",
    "\n",
    "\n",
    "def apply_pipeline(pipeline, train_x, test_x):\n",
    "    train_f = pipeline.fit_transform(train_x)\n",
    "    test_f = pipeline.transform(test_x)\n",
    "    feature_names = derive_feature_names(pipeline)\n",
    "    return train_f, test_f, pipeline, feature_names\n",
    "\n",
    "\n",
    "def get_categorical_indices(feature_names):\n",
    "    return [i for i, name in enumerate(feature_names) if name.endswith(CATEGORY_SUFFIX)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "91a899437403e586b54b7652a4d6471c5e1fc0e4"
   },
   "outputs": [],
   "source": [
    "class ColumnSelector(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self, cat_cols, num_cols):\n",
    "        self._cat_cols = cat_cols\n",
    "        self._num_cols = num_cols\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, *_):\n",
    "        res_X = X[self._cat_cols + self._num_cols]\n",
    "        res_X.loc[:, self._cat_cols] = X[self._cat_cols].astype('category')\n",
    "        return res_X\n",
    "\n",
    "\n",
    "class Mapper(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self, func):\n",
    "        self._func = func\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, *_):\n",
    "        return self._func(X)\n",
    "\n",
    "\n",
    "class Features(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self, transformer, prefix='', categorical=False, feature_names=None):\n",
    "        self._transformer = transformer\n",
    "        self._prefix = prefix\n",
    "        self._categorical_suffix = CATEGORY_SUFFIX if categorical else ''\n",
    "        if feature_names is not None:\n",
    "            self._feature_names = self._format_feature_names(feature_names)\n",
    "            self._features_num = len(feature_names)\n",
    "        else:\n",
    "            self._feature_names = None\n",
    "            self._features_num = None\n",
    "    \n",
    "    def fit(self, *args, **kwds):\n",
    "        self._transformer.fit(*args, **kwds)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, *args, **kwds):\n",
    "        result = self._transformer.transform(*args, **kwds)\n",
    "        \n",
    "        if self._features_num is None:\n",
    "            self._features_num = self._get_size(result)\n",
    "        else:\n",
    "            assert self._features_num == self._get_size(result)\n",
    "        return result\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        if not self._feature_names:\n",
    "            try:\n",
    "                fns = self._transformer.get_feature_names()\n",
    "            except AttributeError:\n",
    "                if self._features_num is None:\n",
    "                    raise ValueError('Feature names cannot be taken before calling transform')\n",
    "                fns = range(self._features_num)\n",
    "            self._feature_names = self._format_feature_names(fns)\n",
    "        return self._feature_names\n",
    "    \n",
    "    def _format_feature_names(self, feature_names):\n",
    "        return [f'{self._prefix}{i}{self._categorical_suffix}' for i in feature_names]\n",
    "    \n",
    "    def _get_size(self, x):\n",
    "        if isinstance(x, list) or isinstance(x, pd.Series):\n",
    "            return len(x[0])\n",
    "        return x.shape[1]\n",
    "\n",
    "\n",
    "def Cols(cols, categorical=False):\n",
    "    return Features(\n",
    "        Mapper(lambda x: list(zip(*[x[c] for c in cols]))),\n",
    "        categorical=categorical,\n",
    "        feature_names=cols\n",
    "    )\n",
    "\n",
    "# def CatCols(cols):\n",
    "#     return Cols(cols, True)\n",
    "\n",
    "\n",
    "class CatCols(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self, cols, use_label_encoder=False):\n",
    "        self._cols = cols\n",
    "        self._feature_names = [f'{c}{CATEGORY_SUFFIX}' for c in cols]\n",
    "        \n",
    "        self._encoders = None\n",
    "        if use_label_encoder:\n",
    "            self._encoders = defaultdict(dict)\n",
    "            \n",
    "    def fit(self, X, *args, **kwds):        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, *args, **kwds):\n",
    "        col_values = []\n",
    "        for c in self._cols:\n",
    "            vals = X[c]\n",
    "            if self._encoders is not None:\n",
    "                vals = [self._encode_value(c, v) for v in vals]\n",
    "            col_values.append(vals)\n",
    "        return list(zip(*col_values))\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self._feature_names\n",
    "    \n",
    "    def _encode_value(self, col, value):\n",
    "        try:\n",
    "            result = self._encoders[col][value]\n",
    "        except KeyError:\n",
    "            result = len(self._encoders[col])\n",
    "            self._encoders[col][value] = result\n",
    "        return result\n",
    "        \n",
    "\n",
    "class Predictor(ABC):\n",
    "    @abstractmethod\n",
    "    def fit(self, train_x, train_ids, train_y, valid_x, valid_ids, valid_y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, x, ids, is_train):\n",
    "        pass\n",
    "\n",
    "\n",
    "class QwkPredictor(Predictor):\n",
    "    @abstractmethod\n",
    "    def from_models(self, models):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "3ca917ef29448f56f1fee2b9f58d282ef3a4ed52"
   },
   "outputs": [],
   "source": [
    "class LgbmPredictor(Predictor):\n",
    "    def __init__(self, params, f_names):\n",
    "        params = params.copy()\n",
    "        self.num_rounds = params.pop('num_rounds')\n",
    "        self.verbose_eval = params.pop('verbose_eval')\n",
    "        self.early_stop = params.pop('early_stop', None)\n",
    "\n",
    "        self.params = dict(params)\n",
    "        self.params['seed'] = SEED\n",
    "        \n",
    "        self.model = None\n",
    "        self._cat_indices = get_categorical_indices(f_names)\n",
    "\n",
    "    def fit(self, train_x, train_ids, train_y, valid_x, valid_ids, valid_y):\n",
    "        print(train_x.shape)\n",
    "        print(valid_x.shape)\n",
    "        d_train = lgb.Dataset(train_x, label=train_y, categorical_feature=self._cat_indices)\n",
    "        d_valid = lgb.Dataset(valid_x, label=valid_y, categorical_feature=self._cat_indices)\n",
    "\n",
    "        watch_list = [d_train, d_valid]\n",
    "\n",
    "        self.model = lgb.train(\n",
    "            self.params,\n",
    "            train_set=d_train,\n",
    "            num_boost_round=self.num_rounds,\n",
    "            valid_sets=watch_list,\n",
    "            verbose_eval=self.verbose_eval,\n",
    "            early_stopping_rounds=self.early_stop\n",
    "        )\n",
    "\n",
    "    def predict(self, x, ids, is_train):\n",
    "        return self.model.predict(x, num_iteration=self.model.best_iteration)\n",
    "\n",
    "\n",
    "class QwkOptimizer(QwkPredictor):\n",
    "    def __init__(self):\n",
    "        self.coefficient = None\n",
    "\n",
    "    def fit(self, train_x, train_ids, train_y, valid_x, valid_ids, valid_y):\n",
    "        assert len(train_x) == len(train_y)\n",
    "        assert len(valid_x) == len(valid_y)\n",
    "        loss_partial = partial(self._kappa_loss, x=train_x, y=train_y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coefficient = sp.optimize.minimize(\n",
    "            loss_partial, initial_coef, method='nelder-mead')['x']\n",
    "        print(f'Coefficients after fitting are {self.coefficient}')\n",
    "\n",
    "    def predict(self, x, ids, is_train):\n",
    "        return self._apply_coeffs(self.coefficient, x)\n",
    "\n",
    "    def from_models(self, models):\n",
    "        self.coefficient = np.mean([m.coefficient for m in models], axis=0)\n",
    "        print(f'Average coefficients taken from models are {self.coefficient}')\n",
    "        return self\n",
    "\n",
    "    def _kappa_loss(self, coeffs, x, y):\n",
    "        x_p = self._apply_coeffs(coeffs, x)\n",
    "        return -quadratic_weighted_kappa(y, x_p)\n",
    "\n",
    "    def _apply_coeffs(self, coeffs, x):\n",
    "        x_p = np.copy(x)\n",
    "        for i, pred in enumerate(x_p):\n",
    "            if pred < coeffs[0]:\n",
    "                x_p[i] = 0\n",
    "            elif coeffs[0] <= pred < coeffs[1]:\n",
    "                x_p[i] = 1\n",
    "            elif coeffs[1] <= pred < coeffs[2]:\n",
    "                x_p[i] = 2\n",
    "            elif coeffs[2] <= pred < coeffs[3]:\n",
    "                x_p[i] = 3\n",
    "            else:\n",
    "                x_p[i] = 4\n",
    "        return x_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "f5dfd5d1a821501738b3e53d473665d34f6cfb69"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class PetPyTorchDataset(Dataset):\n",
    "    def __init__(self, pet_ids, train, y=None, transform=None):\n",
    "        self.transform = transforms.Compose(transform or [])\n",
    "        \n",
    "        image_dir = PF_HOME / ('train_images' if train else 'test_images')\n",
    "        self.image_paths, data_indices = self._get_image_paths(image_dir, pet_ids)\n",
    "        \n",
    "        if y is not None:\n",
    "            self.y = np.array(by_indices(y, data_indices), dtype=np.float32)\n",
    "        else: \n",
    "            self.y = np.full(len(data_indices), -1, dtype=np.float32)\n",
    "            \n",
    "        self.pet_ids = by_indices(pet_ids, data_indices)\n",
    "        \n",
    "    def _get_image_paths(self, image_dir, pet_ids, only_first_image=True):\n",
    "        def get_image_idx(p):\n",
    "            return int(str(p).rsplit('-', 1)[1].split('.')[0])\n",
    "        def get_pet_id(p):\n",
    "            return str(p).rsplit('/', 1)[1].split('-')[0]\n",
    "        \n",
    "        image_paths_per_pet_id = defaultdict(list)\n",
    "        for p in image_dir.iterdir():\n",
    "            image_paths_per_pet_id[get_pet_id(p)].append(p)\n",
    "        \n",
    "        image_paths = []\n",
    "        data_indices = []\n",
    "        for p_index, p_id in enumerate(pet_ids):\n",
    "            pet_image_paths = sorted(image_paths_per_pet_id[p_id], key=get_image_idx)\n",
    "            if len(pet_image_paths) > 0 and only_first_image:\n",
    "                pet_image_paths = [pet_image_paths[0]]\n",
    "            image_paths.extend(pet_image_paths)\n",
    "            data_indices.extend([p_index] * len(pet_image_paths))\n",
    "        \n",
    "        return image_paths, data_indices\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        return self.transform(image), torch.Tensor([self.y[idx]]), self.pet_ids[idx]\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base = models.resnet34(pretrained=False)\n",
    "        self.base.load_state_dict(self._load_net_state())\n",
    "        \n",
    "        last_resnet_size = self.base.fc.in_features\n",
    "        print(f'Last resnet layer is: {last_resnet_size}')\n",
    "        \n",
    "        self.fc = nn.Linear(last_resnet_size * 4, 1)\n",
    "    \n",
    "    def _load_net_state(self):\n",
    "        net_state_path = DATA_HOME / 'resnet34pytorch/resnet34-333f7ec4.pth'\n",
    "        with net_state_path.open('rb') as in_file:\n",
    "            return torch.load(in_file)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base.conv1(x)\n",
    "        x = self.base.bn1(x)\n",
    "        x = self.base.relu(x)\n",
    "        x = self.base.maxpool(x)\n",
    "\n",
    "        x = self.base.layer1(x)\n",
    "        x = self.base.layer2(x)\n",
    "        x = self.base.layer3(x)\n",
    "        x = self.base.layer4(x)\n",
    "\n",
    "        x = self.base.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        loss = torch.sqrt(self.mse(yhat,y) + self.eps)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "class PyTorchModel(Predictor):\n",
    "    def __init__(self, net, lr, epochs, batch_size, verbose=True):\n",
    "        self.net = net\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.transforms = [\n",
    "            transforms.Resize((256, 256)),\n",
    "#             transforms.CenterCrop(224),\n",
    "#             transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "        \n",
    "        device_type = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f'Device `{device_type}` will be used')\n",
    "        self.device = torch.device(device_type)\n",
    "        self.net.to(self.device)\n",
    "    \n",
    "    def fit(self, train_x, train_ids, train_y, valid_x, valid_ids, valid_y):\n",
    "        print(f'Loading train images for {len(train_ids)} pet ids')\n",
    "        train_ds = PetPyTorchDataset(train_ids, train=True, y=train_y, transform=self.transforms)\n",
    "        train_loader = DataLoader(train_ds, shuffle=True, batch_size=self.batch_size)\n",
    "        \n",
    "        print(f'Loading valid images for {len(valid_ids)} pet ids')\n",
    "        valid_ds = PetPyTorchDataset(valid_ids, train=True, y=valid_y, transform=self.transforms)\n",
    "        valid_loader = DataLoader(valid_ds, shuffle=True, batch_size=self.batch_size)\n",
    "        \n",
    "        loss_func = RMSELoss()\n",
    "        optimizer = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        \n",
    "        best_valid_loss = float('inf')\n",
    "        best_net_state = None\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            for x, y, pet_ids in tqdm(train_loader, desc=f'Training (epoch={epoch})', leave=False):\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                y_pred = self.net(x)\n",
    "\n",
    "                loss = loss_func(y_pred, y)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                valid_y_pred, valid_y, _ = self._interference(valid_loader)\n",
    "                valid_loss = loss_func(valid_y_pred, valid_y)\n",
    "\n",
    "                loss_val = loss.item()\n",
    "                valid_loss_val = valid_loss.item()\n",
    "\n",
    "                if self.verbose:\n",
    "                    print(f'Epoch={epoch}, '\n",
    "                          f'Batch loss={loss_val}, '\n",
    "                          f'Valid loss={valid_loss_val}')\n",
    "\n",
    "                if valid_loss_val < best_valid_loss:\n",
    "                    best_valid_loss = valid_loss_val\n",
    "                    best_net_state = self.net.state_dict()\n",
    "\n",
    "        self.net.load_state_dict(best_net_state)\n",
    "    \n",
    "    def _interference(self, loader):\n",
    "        with torch.no_grad():\n",
    "            preds_per_batch = []\n",
    "            y_per_batch = []\n",
    "            pet_ids = []\n",
    "            for batch_x, batch_y, batch_pet_ids in tqdm(loader, desc=f'Interference...', leave=False):\n",
    "                batch_x = batch_x.to(self.device)\n",
    "                batch_y = batch_y.to(self.device)\n",
    "                preds = self.net(batch_x)\n",
    "                preds_per_batch.append(preds)\n",
    "                y_per_batch.append(batch_y)\n",
    "                pet_ids.extend(batch_pet_ids)\n",
    "            return torch.cat(preds_per_batch), torch.cat(y_per_batch), pet_ids\n",
    "\n",
    "    def predict(self, x, ids, is_train):\n",
    "        ds = PetPyTorchDataset(ids, train=is_train, transform=self.transforms)\n",
    "        loader = DataLoader(ds, shuffle=False, batch_size=self.batch_size)\n",
    "        y_pred, _, inf_ids = self._interference(loader)\n",
    "        y_pred = y_pred.squeeze(1).cpu().numpy()\n",
    "        \n",
    "        preds_per_pet_id = {}\n",
    "        for pred, pet_id in zip(y_pred, inf_ids):\n",
    "            assert pet_id not in preds_per_pet_id\n",
    "            preds_per_pet_id[pet_id] = pred\n",
    "        \n",
    "        def random_class():\n",
    "            return float(np.random.randint(0, LABEL_NUM, dtype='int'))\n",
    "        \n",
    "        return np.array([preds_per_pet_id.get(pet_id, random_class()) for pet_id in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "a1729b04b73f84570380a94aaec5e2b9e6cb4df5"
   },
   "outputs": [],
   "source": [
    "def read_data(data_path, shuffle=False):\n",
    "    X = pd.read_csv(data_path)\n",
    "    if shuffle:\n",
    "        X = X.sample(frac=1, random_seed=SEED).reset_index(drop=True)\n",
    "    try:\n",
    "        y = list(X.AdoptionSpeed)\n",
    "    except AttributeError:\n",
    "        y = None\n",
    "    ids = list(X.PetID)\n",
    "    X.drop(['PetID', 'AdoptionSpeed'], axis=1, inplace=True, errors='ignore')\n",
    "    return X, y, ids\n",
    "\n",
    "\n",
    "def read_metadata(pet_ids_list, metadata_dir):\n",
    "    def get_idx(path):\n",
    "        return int(str(path).rsplit('-', 1)[1].split('.')[0])\n",
    "    \n",
    "    def get_sorted_photos_by_idx(paths):\n",
    "        return sorted((get_idx(p), p) for p in paths)\n",
    "    \n",
    "    def extract_label_annotations(metadata_dict):\n",
    "        def by_score(xs): return xs[1]\n",
    "        label_annotations = []\n",
    "        for annotation in metadata_dict.get('labelAnnotations', []):\n",
    "            desc = annotation['mid']\n",
    "            score = annotation['score']\n",
    "            label_annotations.append((desc, score))\n",
    "        return sorted(label_annotations, key=by_score, reverse=True)\n",
    "    \n",
    "    def extract_dominant_colors(metadata_dict):\n",
    "        def by_score(xs): return xs[3]\n",
    "        dominant_colors = []\n",
    "        colors = metadata_dict.get('imagePropertiesAnnotation', {}).get('dominantColors', {}).get('colors', [])\n",
    "        for color_obj in colors:\n",
    "            rgb_obj = color_obj['color']\n",
    "            red = rgb_obj.get('red', 0) / 255\n",
    "            green = rgb_obj.get('green', 0) / 255\n",
    "            blue = rgb_obj.get('blue', 0) / 255\n",
    "            score = color_obj['score']\n",
    "            pixel_fraction = color_obj['pixelFraction']\n",
    "            dominant_colors.append(\n",
    "                (red, green, blue, score, pixel_fraction)\n",
    "            )\n",
    "        return sorted(dominant_colors, key=by_score, reverse=True)\n",
    "    \n",
    "    metadata_dir = Path(metadata_dir)\n",
    "    for pet_id in pet_ids_list:\n",
    "        pet_metadata_paths = metadata_dir.glob(f'{pet_id}-1.json')\n",
    "        \n",
    "        pet_label_annotations = []\n",
    "        pet_dominant_colors = []\n",
    "        \n",
    "        for idx, metadata_path in get_sorted_photos_by_idx(pet_metadata_paths):\n",
    "            with metadata_path.open() as in_file:\n",
    "                metadata_dict = json.load(in_file)\n",
    "                \n",
    "                pet_label_annotations.append(extract_label_annotations(metadata_dict))\n",
    "                pet_dominant_colors.append(extract_dominant_colors(metadata_dict))\n",
    "        \n",
    "        yield pet_label_annotations, pet_dominant_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "557cc2919af6c5a6ba3e8ef6e8f3b0a452ca4add"
   },
   "outputs": [],
   "source": [
    "def read_sentiment_data(pet_ids_list, sentiment_dir):\n",
    "    sentiment_dir = Path(sentiment_dir)\n",
    "    missed_sentiments = 0\n",
    "    for pet_id in pet_ids_list:\n",
    "        sentiment_data_path = sentiment_dir / f'{pet_id}.json'\n",
    "        \n",
    "        if not sentiment_data_path.exists():\n",
    "            yield [], {}, DEFAULT, DEFAULT\n",
    "            missed_sentiments += 1\n",
    "        else:\n",
    "            with sentiment_data_path.open() as in_file:\n",
    "                sentiment_data = json.load(in_file)\n",
    "\n",
    "                sentence_sents = [(obj['sentiment']['magnitude'], obj['sentiment']['score']) \n",
    "                                       for obj in sentiment_data['sentences']]\n",
    "                entity_sents = {obj['name'].lower().strip(): obj['salience']\n",
    "                            for obj in sentiment_data['entities']}\n",
    "                doc_sent_magnitude = sentiment_data['documentSentiment']['magnitude']\n",
    "                doc_sent_score = sentiment_data['documentSentiment']['score']\n",
    "\n",
    "                yield sentence_sents, entity_sents, (doc_sent_magnitude, doc_sent_score)\n",
    "    print(f'Missed sentiment files for {missed_sentiments} pet ids')\n",
    "\n",
    "def pet_data_assigner(X, data_reader, **mappers):\n",
    "    extacted_data = defaultdict(list)\n",
    "    data_num = 0\n",
    "    errors_per_mapper = Counter()\n",
    "    for data in data_reader:\n",
    "        for k, (map_func, default_val) in mappers.items():\n",
    "            try:\n",
    "                map_result = map_func(*data)\n",
    "            except:\n",
    "                map_result = default_val\n",
    "                errors_per_mapper[k] += 1\n",
    "            extacted_data[k].append(map_result)\n",
    "        data_num += 1 \n",
    "    \n",
    "    for k, errors_num in errors_per_mapper.items():\n",
    "        if errors_num > 0: \n",
    "            print(f'There were {errors_num} errors ({int(errors_num * 100 / data_num)}%) for mapper {k}')\n",
    "    \n",
    "    for k, data in extacted_data.items():\n",
    "        X[k] = data\n",
    "\n",
    "        \n",
    "def train_test_data_assigner(train_data_reader, test_data_reader, **mapper_info):\n",
    "    mappers = {}\n",
    "    col_assignments = []\n",
    "    for k, (mapper_func, default, add_to) in mapper_info.items():\n",
    "        mappers[k] = (mapper_func, default)\n",
    "        if add_to is not None:\n",
    "            col_assignments.append((add_to, k))\n",
    "    \n",
    "    pet_data_assigner(\n",
    "        train_x, train_data_reader, **mappers\n",
    "    )\n",
    "    pet_data_assigner(\n",
    "        test_x, test_data_reader, **mappers\n",
    "    )\n",
    "    \n",
    "    for cols_list, col in col_assignments:\n",
    "        if col not in cols_list:\n",
    "            cols_list.append(col)\n",
    "        \n",
    "def m(mapper_func, *, default, add_to=None):\n",
    "    return mapper_func, default, add_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "c30ce82221a96a93bfcdf43982f6ea6967d6ab48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14993, 22)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(3948, 22)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RescuerID']\n",
      "['Name', 'Description']\n",
      "['Age', 'Quantity', 'Fee', 'VideoAmt', 'PhotoAmt', 'MaturitySize', 'FurLength', 'Health']\n",
      "['Color2', 'Color3', 'Vaccinated', 'Breed2', 'State', 'Sterilized', 'Breed1', 'Gender', 'Dewormed', 'Color1', 'Type']\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, train_ids = read_data(PF_HOME / 'train' / 'train.csv')\n",
    "test_x, test_y, test_ids = read_data(PF_HOME / 'test' / 'test.csv')\n",
    "\n",
    "display(train_x.shape)\n",
    "display(test_x.shape)\n",
    "\n",
    "special_cols = ['RescuerID']\n",
    "text_cols = ['Name', 'Description']\n",
    "num_cols = ['Age', 'Quantity', 'Fee', 'VideoAmt', 'PhotoAmt', 'MaturitySize', 'FurLength', 'Health']\n",
    "cat_cols = list(set(train_x.columns) - set(text_cols) - set(num_cols) - set(special_cols))\n",
    "\n",
    "breed_labels_map = {row.BreedID: row.BreedName \n",
    "                   for _, row in pd.read_csv(PF_HOME / 'breed_labels.csv').iterrows()}\n",
    "\n",
    "state_labels_map = {row.StateID: row.StateName \n",
    "                   for _, row in pd.read_csv(PF_HOME / 'state_labels.csv').iterrows()}\n",
    "\n",
    "color_labels_map = {row.ColorID: row.ColorName \n",
    "                   for _, row in pd.read_csv(PF_HOME / 'color_labels.csv').iterrows()}\n",
    "\n",
    "# breed_rating = json.loads((PF_HOME / 'breed_rating.json').read_text())\n",
    "\n",
    "display(len(breed_labels_map))\n",
    "\n",
    "print(special_cols)\n",
    "print(text_cols)\n",
    "print(num_cols)\n",
    "print(cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cf85b8c1c77eddef6abd99481741845f6a713c44"
   },
   "outputs": [],
   "source": [
    "train_test_data_assigner(\n",
    "    read_sentiment_data(train_ids, PF_HOME / 'train_sentiment'),\n",
    "    read_sentiment_data(test_ids, PF_HOME / 'test_sentiment'),\n",
    "    first_sentence_magnitude=     m(lambda ss, es, ds: ss[0][0], default=DEFAULT, add_to=num_cols),\n",
    "    first_sentence_score=         m(lambda ss, es, ds: ss[0][1], default=DEFAULT, add_to=num_cols),\n",
    "    second_sentence_magnitude=    m(lambda ss, es, ds: ss[1][0], default=DEFAULT, add_to=num_cols),\n",
    "    second_sentence_score=        m(lambda ss, es, ds: ss[1][1], default=DEFAULT, add_to=num_cols),\n",
    "    third_sentence_magnitude=     m(lambda ss, es, ds: ss[2][0], default=DEFAULT, add_to=num_cols),\n",
    "    third_sentence_score=         m(lambda ss, es, ds: ss[2][1], default=DEFAULT, add_to=num_cols),\n",
    "    mean_sentence_magnitude=      m(lambda ss, es, ds: np.mean([m for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    std_sentence_magnitude=       m(lambda ss, es, ds: np.std([m for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    sum_sentence_magnitude=       m(lambda ss, es, ds: np.sum([m for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    mean_sentence_score=          m(lambda ss, es, ds: np.mean([s for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    std_sentence_score=           m(lambda ss, es, ds: np.std([s for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    sum_sentence_score=           m(lambda ss, es, ds: np.sum([s for m, s in ss]), default=DEFAULT, add_to=num_cols),\n",
    "    sentence_count=               m(lambda ss, es, ds: len(ss), default=DEFAULT, add_to=num_cols),\n",
    "    entity_sents=                 m(lambda ss, es, ds: es, default={}),\n",
    "    doc_sent_magnitude=           m(lambda ss, es, ds: ds[0], default=DEFAULT, add_to=num_cols),\n",
    "    doc_sent_score=               m(lambda ss, es, ds: ds[1], default=DEFAULT, add_to=num_cols),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2915cf92ebfaf1bddb29b8c439df39d4ac912f0a"
   },
   "outputs": [],
   "source": [
    "# train_test_data_assigner(\n",
    "#     read_metadata(train_ids, PF_HOME / 'train_metadata'),\n",
    "#     read_metadata(test_ids, PF_HOME / 'test_metadata'),\n",
    "#     first_image_label_annotations=     m(lambda la, dm: dict(la[0]), default={}),\n",
    "#     second_image_label_annotations=     m(lambda la, dm: dict(la[1]), default={}),\n",
    "#     third_image_label_annotations=     m(lambda la, dm: dict(la[2]), default={}),\n",
    "#     first_image_color_annotations=     m(lambda la, dm: dict(la[0]), default={}),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "ecbdc6dd2ce620495b0034cc8bc491caece90c96"
   },
   "outputs": [],
   "source": [
    "class TextFeatures(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self, prefix=''):\n",
    "        self._prefix = prefix\n",
    "    \n",
    "    def transform(self, texts, *_):\n",
    "        return [self._text_features(t) for t in texts]\n",
    "    \n",
    "    def _text_features(self, text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        token_sizes = list(map(len, tokens))\n",
    "        \n",
    "        return [\n",
    "            len(text),\n",
    "            len(tokens),\n",
    "            min(token_sizes) if token_sizes else DEFAULT,\n",
    "            max(token_sizes) if token_sizes else DEFAULT,\n",
    "            sum(token_sizes) if token_sizes else DEFAULT,\n",
    "            np.mean(token_sizes) if token_sizes else DEFAULT,\n",
    "            np.std(token_sizes) if token_sizes else DEFAULT,\n",
    "        ]\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return [f'{self._prefix}{n}' for n in [\n",
    "            'text_len', 'tokens_num', 'min_token_size', 'max_token_size', 'sum_token_size', \n",
    "            'mean_token_size', 'std_token_size']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d0ab5b16bedc8b895ad5395493b17da2dd7ce2fe"
   },
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "\n",
    "# LANG = 'english'\n",
    "# stemmer = nltk.stem.SnowballStemmer(LANG)\n",
    "# stopwords = stopwords.words('english')\n",
    "\n",
    "# def tokenize(text, language=LANG):\n",
    "#     return [w for w in word_tokenize(text.lower(), language=language) if w.isalnum()]\n",
    "\n",
    "# def stemmed_words(words):\n",
    "#     return [stemmer.stem(w) for w in words]\n",
    "\n",
    "# def ngrams(tokens, min_n, max_n):\n",
    "#     if max_n != 1:\n",
    "#         original_tokens = tokens\n",
    "#         if min_n == 1:\n",
    "#             # no need to do any slicing for unigrams\n",
    "#             # just iterate through the original tokens\n",
    "#             tokens = list(original_tokens)\n",
    "#             min_n += 1\n",
    "#         else:\n",
    "#             tokens = []\n",
    "\n",
    "#         n_original_tokens = len(original_tokens)\n",
    "\n",
    "#         # bind method outside of loop to reduce overhead\n",
    "#         tokens_append = tokens.append\n",
    "#         space_join = \" \".join\n",
    "\n",
    "#         for n in range(min_n, min(max_n + 1, n_original_tokens + 1)):\n",
    "#             for i in range(n_original_tokens - n + 1):\n",
    "#                 tokens_append(space_join(original_tokens[i: i + n]))\n",
    "#     return tokens\n",
    "\n",
    "\n",
    "# def nltk_analyzer(text, min_n, max_n):\n",
    "#     tokens = [t for t in tokenize(text) if t not in stopwords]\n",
    "#     return ngrams(stemmed_words(text), min_n, max_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b5afd42cddcd90fe23e4d6c7e1c333112e6d51d8"
   },
   "outputs": [],
   "source": [
    "def explode(df, col, fillna=None):\n",
    "    ret = None\n",
    "    \n",
    "    col_name = col if isinstance(col, str) else '_'.join(col)\n",
    "    \n",
    "    new_cols = pd.DataFrame(({f'{col_name}_{k}': v for k, v in d.items()} \n",
    "                             for idx, d in df[col].iteritems()))\n",
    "    if fillna is not None:\n",
    "        new_cols = new_cols.fillna(fillna)\n",
    "    new_cols.index = df.index\n",
    "    \n",
    "    ret = pd.concat([df, new_cols], axis=1)\n",
    "    del ret[col]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f66049672bb340e3e08f857e683670824005a245"
   },
   "outputs": [],
   "source": [
    "class RescuerFeatures(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self, num_cols, cat_cols, rescued_id_col='RescuerID'):\n",
    "        self._num_cols = num_cols\n",
    "        self._cat_cols = cat_cols\n",
    "        self._rescued_id_col = rescued_id_col\n",
    "        \n",
    "        self.columns = None\n",
    "    \n",
    "    def transform(self, df, *_):\n",
    "        result = self._extract_rescuer_features(df)\n",
    "        for c in self.columns:\n",
    "            if c not in result.columns:\n",
    "                result[c] = np.full(df.shape[0], DEFAULT)\n",
    "        return result[self.columns]\n",
    "    \n",
    "    def fit(self, df, *_):\n",
    "        df = self._extract_rescuer_features(df)\n",
    "        display(df)\n",
    "        self.columns = df.columns\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, df, *_):\n",
    "        df = self._extract_rescuer_features(df)\n",
    "        self.columns = df.columns\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        if self.columns is None:\n",
    "            raise ValueError('RescuerFeatures was not fitted')\n",
    "        return self.columns\n",
    "    \n",
    "    def _extract_rescuer_features(self, df):\n",
    "        def num_col_features(values):\n",
    "            return dict(\n",
    "                min=min(values) if len(values) > 0 else DEFAULT,\n",
    "                max=max(values) if len(values) > 0 else DEFAULT,\n",
    "                sum=sum(values) if len(values) > 0 else DEFAULT,\n",
    "                mean=np.mean(values) if len(values) > 0 else DEFAULT,\n",
    "                std=np.std(values) if len(values) > 0 else DEFAULT,\n",
    "            )\n",
    "        \n",
    "        def cat_col_features(values):\n",
    "            return Counter(values)\n",
    "        \n",
    "        agg_operations = defaultdict(list)\n",
    "        for c in self._num_cols:\n",
    "            agg_operations[c].append(num_col_features)\n",
    "        for c in self._cat_cols:\n",
    "            agg_operations[c].append(cat_col_features)\n",
    "        agg_operations[self._rescued_id_col].append('size')\n",
    "            \n",
    "        by_rescuer_id = df.groupby(self._rescued_id_col).agg(agg_operations)\n",
    "        for outer_c, inner_c in by_rescuer_id.columns:\n",
    "            if inner_c in {'num_col_features', 'cat_col_features'}:\n",
    "                by_rescuer_id = explode(by_rescuer_id, (outer_c, inner_c), DEFAULT)\n",
    "        by_rescuer_id.columns = [c if isinstance(c, str) else '_'.join(c) \n",
    "                                 for c in by_rescuer_id.columns]\n",
    "        \n",
    "        result = df[[self._rescued_id_col]].merge(\n",
    "            by_rescuer_id, how='outer', left_on=self._rescued_id_col, right_index=True\n",
    "        ).sort_index()\n",
    "        result.drop(self._rescued_id_col, axis=1, inplace=True)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "8d8a59337b52eda2608556d1b43440235cdb20ca",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_f, test_f, p, f_names = apply_pipeline(make_pipeline(\n",
    "    FeatureUnion([\n",
    "#         ('desc_tokenizing', make_pipeline(\n",
    "#             Mapper(lambda x: list(x.Description.fillna(''))),\n",
    "#             CountVectorizer(\n",
    "#                 analyzer='word',\n",
    "#                 ngram_range=(1, 3),\n",
    "#                 dtype=np.float32\n",
    "#             ),\n",
    "#             FeatureUnion([\n",
    "#                 ('svd', Features(TruncatedSVD(n_components=128, random_state=SEED))),\n",
    "#                 ('nmf', Features(NMF(n_components=128, random_state=SEED)))\n",
    "#             ])\n",
    "#         )),\n",
    "        ('desc_text_fs', make_pipeline(\n",
    "            Mapper(lambda x: list(x.Description.fillna(''))),\n",
    "            TextFeatures('desc_')\n",
    "        )),\n",
    "#         ('name_text_fs', make_pipeline(\n",
    "#             Mapper(lambda x: list(x.Name.fillna(''))),\n",
    "#             TextFeatures('name_')\n",
    "#         )),\n",
    "#         ('num_cols', Cols(num_cols)),\n",
    "#         ('cat_cols', CatCols(cat_cols, use_label_encoder=True)),\n",
    "#         ('rescuer_fs', make_pipeline(\n",
    "#             RescuerFeatures(num_cols=num_cols, cat_cols=cat_cols),\n",
    "# #             FeatureUnion([\n",
    "# #                 Features(TruncatedSVD(n_components=16, random_state=SEED)),\n",
    "# #                 Features(NMF(n_components=16, random_state=SEED))\n",
    "# #             ])\n",
    "#         )),\n",
    "# #         make_pipeline(\n",
    "# #             Mapper(lambda x: x.first_image_label_annotations),\n",
    "# #             DictVectorizer()\n",
    "# #         ),\n",
    "#         ('entity_sents', make_pipeline(\n",
    "#             Mapper(lambda x: x.entity_sents),\n",
    "#             DictVectorizer(),\n",
    "#             FeatureUnion([\n",
    "#                 ('svd', Features(TruncatedSVD(n_components=8, random_state=SEED))),\n",
    "#                 ('nmf', Features(NMF(n_components=8, random_state=SEED)))\n",
    "#             ])\n",
    "#         )),\n",
    "#         Features(Mapper(lambda x: list(x.first_image_color_annotations)))\n",
    "    ])\n",
    "), train_x, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0e589bc4d954b777e17e7e4f834bc6fc3c039e7d"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(train_f, columns=f_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e29e174661fdc6215bcffdbf78942bc183c35da4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'max_bin': 512,\n",
    "          'num_leaves': 30,\n",
    "          'max_depth': 6,\n",
    "          'min_data_in_leaf': 10,\n",
    "          'learning_rate': 0.001,\n",
    "          'bagging_fraction': 0.85,\n",
    "#           'feature_fraction': 0.8,\n",
    "#           'min_split_gain': 0.02,\n",
    "#           'min_child_weight': 0.02,\n",
    "          'lambda_l2': 0.25,\n",
    "          'verbosity': 1,\n",
    "          'early_stop': 2000,\n",
    "          'verbose_eval': 100,\n",
    "          'num_rounds': 20000,\n",
    "         }\n",
    "\n",
    "results = run_cv_model(\n",
    "    'lgbm', \n",
    "    train_y, train_x['RescuerID'], \n",
    "    train_f, train_ids, \n",
    "    test_f, test_ids, \n",
    "    lambda: LgbmPredictor(params, f_names), lambda: QwkOptimizer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bde427c4c26e6948bfb42d68a2b4d009328b99c2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_nn = run_cv_model(\n",
    "    'lgbm', \n",
    "    train_y, train_x['RescuerID'], \n",
    "    train_f, train_ids, \n",
    "    test_f, test_ids, \n",
    "    lambda: PyTorchModel(ResNet(), lr=0.01, epochs=10, batch_size=128), lambda: QwkOptimizer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0b2980f7f747fdee6e91e10f1809f127ca95eb4a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_random_example(results['pred_train_y'], actual_label=1, pred_label=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f49620f5774800765ac94fdc4ae25602fd98c0b5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(pd.DataFrame(\n",
    "    Counter(dict(zip(f_names, results['models'][0].model.feature_importance('gain')))).most_common(50)\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
